# 本节重点总结：

- syncLoopIteration 是syncLoop处理的主循环，从各种通道读取数据，并将POD分派给给定的处理程序。

> 5类事件循环 7个chan

- configCh：将配置更改的POD分派到相应的事件类型的处理程序回调处理
- plegCh: pleg产生的pod状态变化的event
- syncCh： 处理等待sync的pod
- housekeepingCh：触发清理pod的
- health manager: 处理健康检测失败的pod，包括下面3个

  - 慢启动探针 startupManager
  - 就绪探针 readinessManager
  - 存活探针 livenessManager
- configCh 主要监听来自apiserver、file和http的变更

  - 其中和apiserver的通信就是使用client-go 包创建的cache
  - 监听所有ns的pods资源：这很好理解因为调度到这个节点上的pods来自多个namespace
  - 同时会传入过滤器 spec.nodeName=这个节点的pods
    - 意思是只关注调度到我这个节点的pods，其他的node不关注
      - 因为集群中的node资源太多了，比如1k个，全部拉下来本地的没有必要
- configCh中监听来自apiserver的数据作为生产者，通过updates这个chan发往syncLoop消费即可，通信就是依赖informer机制
  ![informer01.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/908/1635057774000/3dc19350030a4e4fbc082480428da5cd.png)

# kubelet的syncLoop5大循环总结

- syncLoopIteration 是syncLoop处理的主循环
- syncLoopIteration从各种通道读取数据，并将POD分派给给定的处理程序。

## 5类事件循环 7个chan

- configCh：将配置更改的POD分派到相应的事件类型的处理程序回调处理
- plegCh: pleg产生的pod状态变化的event
- syncCh： 处理等待sync的pod
- housekeepingCh：触发清理pod的
- health manager: 处理健康检测失败的pod，包括下面3个
  - 慢启动探针 startupManager
  - 就绪探针 readinessManager
  - 存活探针 livenessManager

## 主分支代码

- 位置 D:\go_path\src\github.com\kubernetes\kubernetes\pkg\kubelet\kubelet.go

```go
func (kl *Kubelet) syncLoopIteration(configCh <-chan kubetypes.PodUpdate, handler SyncHandler,
	syncCh <-chan time.Time, housekeepingCh <-chan time.Time, plegCh <-chan *pleg.PodLifecycleEvent) bool {
	select {
	    case u, open := <-configCh:
        case e := <-plegCh:
        case <-syncCh:
        case update := <-kl.livenessManager.Updates():
        case update := <-kl.readinessManager.Updates():
        case update := <-kl.startupManager.Updates():
        case <-housekeepingCh:
	}
```

# 事件循环01 configCh 分析

- 从case处理的代码来看就是根据 PodUpdate的 PodOperation 调用指定的回调处理

```go
		switch u.Op {
		case kubetypes.ADD:
			klog.V(2).InfoS("SyncLoop ADD", "source", u.Source, "pods", format.Pods(u.Pods))
			// After restarting, kubelet will get all existing pods through
			// ADD as if they are new pods. These pods will then go through the
			// admission process and *may* be rejected. This can be resolved
			// once we have checkpointing.
			handler.HandlePodAdditions(u.Pods)
		case kubetypes.UPDATE:
			klog.V(2).InfoS("SyncLoop UPDATE", "source", u.Source, "pods", format.Pods(u.Pods))
			handler.HandlePodUpdates(u.Pods)
		case kubetypes.REMOVE:
			klog.V(2).InfoS("SyncLoop REMOVE", "source", u.Source, "pods", format.Pods(u.Pods))
			handler.HandlePodRemoves(u.Pods)
		case kubetypes.RECONCILE:
			klog.V(4).InfoS("SyncLoop RECONCILE", "source", u.Source, "pods", format.Pods(u.Pods))
			handler.HandlePodReconcile(u.Pods)
		case kubetypes.DELETE:
			klog.V(2).InfoS("SyncLoop DELETE", "source", u.Source, "pods", format.Pods(u.Pods))
			// DELETE is treated as a UPDATE because of graceful deletion.
			handler.HandlePodUpdates(u.Pods)
		case kubetypes.SET:
			// TODO: Do we want to support this?
			klog.ErrorS(nil, "Kubelet does not support snapshot update")
		default:
			klog.ErrorS(nil, "Invalid operation type received", "operation", u.Op)
		}

```

- 以update动作为例 ,底层就是调用 dispatchWork分发

```go
// HandlePodUpdates is the callback in the SyncHandler interface for pods
// being updated from a config source.
func (kl *Kubelet) HandlePodUpdates(pods []*v1.Pod) {
	start := kl.clock.Now()
	for _, pod := range pods {
		kl.podManager.UpdatePod(pod)
		if kubetypes.IsMirrorPod(pod) {
			kl.handleMirrorPod(pod, start)
			continue
		}
		mirrorPod, _ := kl.podManager.GetMirrorPodByPod(pod)
		kl.dispatchWork(pod, kubetypes.SyncPodUpdate, mirrorPod, start)
	}
}

```

## configCh的PodUpdate 来源

- 一路追查可以发现 是在kubelet的run的启动传入的update chan
- 这种做法很常见，相当于make 一个chan ，同时传给生成者和消费者，那么二者就可以通过chan 交流数据
- 位置 D:\go_path\src\github.com\kubernetes\kubernetes\cmd\kubelet\app\server.go
-

```go
	// start the kubelet
	go k.Run(podCfg.Updates())

```

- 追查Updates方法发现是返回PodConfig的updates字段

```go
// Updates returns a channel of updates to the configuration, properly denormalized.
func (c *PodConfig) Updates() <-chan kubetypes.PodUpdate {
	return c.updates
}
```

- 追查updates字段可以看到在NewPodConfig中创建的chan，同时也传给了newPodStorage

```go
// 位置 D:\go_path\src\github.com\kubernetes\kubernetes\pkg\kubelet\config\config.go
// NewPodConfig creates an object that can merge many configuration sources into a stream
// of normalized updates to a pod configuration.
func NewPodConfig(mode PodConfigNotificationMode, recorder record.EventRecorder) *PodConfig {
	updates := make(chan kubetypes.PodUpdate, 50)
	storage := newPodStorage(updates, mode, recorder)
	podConfig := &PodConfig{
		pods:    storage,
		mux:     config.NewMux(storage),
		updates: updates,
		sources: sets.String{},
	}
	return podConfig
}
```

- 追踪podStorage的updates字段调用发现在podStorage的Merge方法中，位置D:\go_path\src\github.com\kubernetes\kubernetes\pkg\kubelet\config\config.go

### podStorage的Merge解析

- 首先可以看到调用merge按照类型合并变更

```go
adds, updates, deletes, removes, reconciles := s.merge(source, change)
```

- 首先可以看到是通过判断 mode，代表不同的同步模式

```go
	switch s.mode {
	case PodConfigNotificationIncremental:
    case PodConfigNotificationSnapshotAndUpdates:
    case PodConfigNotificationSnapshot:
    case PodConfigNotificationUnknown:
}
```

- 追溯NewPodConfig可以看到 传入的通知模式是PodConfigNotificationIncremental 增量通知

```go
	// source of all configuration
	cfg := config.NewPodConfig(config.PodConfigNotificationIncremental, kubeDeps.Recorder)

```

> PodConfigNotificationIncremental 代表增量通知

- 可以看到会根据上面merge得到 几种类型的变更，然后传入 s.updates chan中

```go
	switch s.mode {
	case PodConfigNotificationIncremental:
		if len(removes.Pods) > 0 {
			s.updates <- *removes
		}
		if len(adds.Pods) > 0 {
			s.updates <- *adds
		}
		if len(updates.Pods) > 0 {
			s.updates <- *updates
		}
		if len(deletes.Pods) > 0 {
			s.updates <- *deletes
		}
		if firstSet && len(adds.Pods) == 0 && len(updates.Pods) == 0 && len(deletes.Pods) == 0 {
			// Send an empty update when first seeing the source and there are
			// no ADD or UPDATE or DELETE pods from the source. This signals kubelet that
			// the source is ready.
			s.updates <- *adds
		}
		// Only add reconcile support here, because kubelet doesn't support Snapshot update now.
		if len(reconciles.Pods) > 0 {
			s.updates <- *reconciles
		}
```

### podStorage的Merge 调用

- 追溯可以发现由 Mux的listen调用，遍历listenChannel中的update调用Merge
- 而上面的Channel 方法会周期执行listen

```go
func (m *Mux) Channel(source string) chan interface{} {
	if len(source) == 0 {
		panic("Channel given an empty name")
	}
	m.sourceLock.Lock()
	defer m.sourceLock.Unlock()
	channel, exists := m.sources[source]
	if exists {
		return channel
	}
	newChannel := make(chan interface{})
	m.sources[source] = newChannel
	go wait.Until(func() { m.listen(source, newChannel) }, 0, wait.NeverStop)
	return newChannel
}

func (m *Mux) listen(source string, listenChannel <-chan interface{}) {
	for update := range listenChannel {
		m.merger.Merge(source, update)
	}
}

```

- 再向上追查Channel，发现 三种channel（file，apiserver和http）分别代表了kubelet创建pod的3个途径
- 函数makePodSourceConfig，位置 D:\go_path\src\github.com\kubernetes\kubernetes\pkg\kubelet\kubelet.go

```go
	// define file config source
	if kubeCfg.StaticPodPath != "" {
		klog.InfoS("Adding static pod path", "path", kubeCfg.StaticPodPath)
		config.NewSourceFile(kubeCfg.StaticPodPath, nodeName, kubeCfg.FileCheckFrequency.Duration, cfg.Channel(kubetypes.FileSource))
	}

	// define url config source
	if kubeCfg.StaticPodURL != "" {
		klog.InfoS("Adding pod URL with HTTP header", "URL", kubeCfg.StaticPodURL, "header", manifestURLHeader)
		config.NewSourceURL(kubeCfg.StaticPodURL, manifestURLHeader, nodeName, kubeCfg.HTTPCheckFrequency.Duration, cfg.Channel(kubetypes.HTTPSource))
	}

	if kubeDeps.KubeClient != nil {
		klog.InfoS("Adding apiserver pod source")
		config.NewSourceApiserver(kubeDeps.KubeClient, nodeName, nodeHasSynced, cfg.Channel(kubetypes.ApiserverSource))
	}
```

### 来自apiserver的pod 变更

- 上面看到的apiserver的 chanel

```go
	if kubeDeps.KubeClient != nil {
		klog.InfoS("Adding apiserver pod source")
		config.NewSourceApiserver(kubeDeps.KubeClient, nodeName, nodeHasSynced, cfg.Channel(kubetypes.ApiserverSource))
	}
```

### 监听apiserver的变更 解析

#### 首先创建了一个 listandWatch的client

- 这里的 NewListWatchFromClient就是调用的client-go的sdk ，创建了一个和apiserver进行通信的本地cache

```go
	lw := cache.NewListWatchFromClient(c.CoreV1().RESTClient(), "pods", metav1.NamespaceAll, fields.OneTermEqualSelector("spec.nodeName", string(nodeName)))

```

- 同时指定了resource=pods 代表关注的是pods资源
- namespace=metav1.NamespaceAll 代表关注监听所有ns的pods资源
  - 这很好理解因为调度到这个节点上的pods来自多个namespace
- 同时会传入过滤器 spec.nodeName=这个节点的pods
  - 意思是只关注调度到我这个节点的pods，其他的node不关注
    - 因为集群中的node资源太多了，比如1k个，全部拉下来本地的没有必要

#### 等待nodeHasSynced 结束再启动 Reflector

- 从这里打印的info日志能看出来，先会等待 node sync 结束，再监听 apiserver的pods
- 日志打印完毕后就是启动一个goroutine，先不断等待nodeHasSynced的结果，为true后再 执行 newSourceApiserverFromLW

```go
	klog.InfoS("Waiting for node sync before watching apiserver pods")
	go func() {
		for {
			if nodeHasSynced() {
				klog.V(4).InfoS("node sync completed")
				break
			}
			time.Sleep(WaitForAPIServerSyncPeriod)
			klog.V(4).InfoS("node sync has not completed yet")
		}
		klog.InfoS("Watching apiserver")
		newSourceApiserverFromLW(lw, updates)
	}()
```

- 从这里我们可以猜到这个nodeHasSynced应该是某种依赖或者先决条件

> 追踪 nodeHasSynced

- 可以发现 nodeHasSynced 的类型为cache.InformerSynced

```go
var nodeHasSynced cache.InformerSynced
```

- 继续追踪这个cache.InformerSynced ，可以发现这么一段注释

```go
// InformerSynced is a function that can be used to determine if an informer has synced.  This is useful for determining if caches have synced.

```

- 意思就是cache.InformerSynced 可以用来判定informer是否已经同步过了。因为informer内置了缓存，再缓存重启之后需要先进行list获取最新的全量数据填充
- 同时可以看到在kubelet中nodeHasSynced 的定义为 对应的nodes的informer是否已经sync

```go
		nodeHasSynced = func() bool {
			return kubeInformers.Core().V1().Nodes().Informer().HasSynced()
		}
```

##### 然后使用创建的lw对象创建NewReflector，并启动

- 同时传入和消费者通信的 updates队列 ，用来构造传递消息的send方法
- 遍历后面由informer传入send方法中的objs 对象，断言成pod类型，然后构造kubetypes.PodUpdate对象传入updates chan中

```go
// newSourceApiserverFromLW holds creates a config source that watches and pulls from the apiserver.
func newSourceApiserverFromLW(lw cache.ListerWatcher, updates chan<- interface{}) {
	send := func(objs []interface{}) {
		var pods []*v1.Pod
		for _, o := range objs {
			pods = append(pods, o.(*v1.Pod))
		}
		updates <- kubetypes.PodUpdate{Pods: pods, Op: kubetypes.SET, Source: kubetypes.ApiserverSource}
	}
}
```

- 然后NewReflector初始化 Reflector，启动就可以了

```go
	r := cache.NewReflector(lw, &v1.Pod{}, cache.NewUndeltaStore(send, cache.MetaNamespaceKeyFunc), 0)
	go r.Run(wait.NeverStop)
```

# 本节重点总结：

- syncLoopIteration 是syncLoop处理的主循环，从各种通道读取数据，并将POD分派给给定的处理程序。

> 5类事件循环 7个chan

- configCh：将配置更改的POD分派到相应的事件类型的处理程序回调处理
- plegCh: pleg产生的pod状态变化的event
- syncCh： 处理等待sync的pod
- housekeepingCh：触发清理pod的
- health manager: 处理健康检测失败的pod，包括下面3个

  - 慢启动探针 startupManager
  - 就绪探针 readinessManager
  - 存活探针 livenessManager
- configCh 主要监听来自apiserver、file和http的变更

  - 其中和apiserver的通信就是使用client-go 包创建的cache
  - 监听所有ns的pods资源：这很好理解因为调度到这个节点上的pods来自多个namespace
  - 同时会传入过滤器 spec.nodeName=这个节点的pods
    - 意思是只关注调度到我这个节点的pods，其他的node不关注
      - 因为集群中的node资源太多了，比如1k个，全部拉下来本地的没有必要
- configCh中监听来自apiserver的数据作为生产者，通过updates这个chan发往syncLoop消费即可，通信就是依赖informer机制
  ![informer01.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/908/1635057774000/f452048a29354f6485af1c014d1be696.png)