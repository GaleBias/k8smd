# 本节重点总结 :

- numa和cpu计算密集型任务的关系
- cgroup的cpuset绑核技术
- 编写go代码体会cpuset，go中 GOMAXPROCS和绑核的关系

# 为什么要有NUMA

- 在NUMA架构出现前，CPU欢快的朝着频率越来越高的方向发展。受到物理极限的挑战，又转为核数越来越多的方向发展
- 如果每个core的工作性质都是share-nothing（类似于map-reduce的node节点的作业属性），那么也许就不会有NUMA
- 由于所有CPU Core都是通过共享一个北桥来读取内存，随着核数如何的发展，北桥在响应时间上的性能瓶颈越来越明显
- 于是，聪明的硬件设计师们，先到了把内存控制器（原本北桥中读取内存的部分）也做个拆分，平分到了每个die上。于是NUMA就出现了！

## NUMA是什么

![numa.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/908/1634434376000/bfca057217d94a44be520ebdff54003e.png)

- NUMA中，虽然内存直接attach在CPU上，但是由于内存被平均分配在了各个die上
- 只有当CPU访问自身直接attach内存对应的物理地址时，才会有较短的响应时间（后称Local Access）
- 而如果需要访问其他CPU attach的内存的数据时，就需要通过inter-connect通道访问，响应时间就相比之前变慢了（后称Remote Access）
- 所以NUMA（Non-Uniform Memory Access）就此得名

# CPU 密集型任务要减少在多个核心中切换

- 有了上面numa的知识我们知道，既然CPU只有在Local-Access时响应时间才能有保障，那么我们就尽量把该CPU所要的数据集中在他local的内存中
- 对于某些 CPU 密集型的程序来说，不仅需要获取更多的 CPU 使用时间，还要减少工作负载在节流时引起的上下文切换
- 现在的多核系统中每个核心都有自己的缓存，如果频繁的调度进程在不同的核心上执行势必会带来缓存失效等开销

# 绑定核的技术 cgroup cpu set

- 我们可以利用cgroup 的cpu set能力将指定的进程绑定到指定的核上面运行

## 什么任务需要cpuset

- 对CPU节流效应敏感。
- 对上下文切换敏感。
- 对处理器缓存未命中敏感。
- 共享处理器资源（例如，数据和指令缓存）的好处。
- 对跨套接字内存流量敏感。
- 敏感或需要来自同一物理CPU内核的超线程。

## 使用cgroup实验 cpuset绑定核心

> 创建cpuset cgroup  my_cpu_set

```shell
mkdir /sys/fs/cgroup/cpuset/my_cpu_set

```

- 和cpu subsystem类似，cpuset也会生成相关文件

```shell
[root@k8s-node01 ~]# ll /sys/fs/cgroup/cpuset/my_cpu_set/
total 0
-rw-r--r-- 1 root root 0 Sep 27 16:14 cgroup.clone_children
--w--w--w- 1 root root 0 Sep 27 16:14 cgroup.event_control
-rw-r--r-- 1 root root 0 Sep 27 16:14 cgroup.procs
-rw-r--r-- 1 root root 0 Sep 27 16:14 cpuset.cpu_exclusive
-rw-r--r-- 1 root root 0 Sep 27 16:16 cpuset.cpus
-r--r--r-- 1 root root 0 Sep 27 16:14 cpuset.effective_cpus
-r--r--r-- 1 root root 0 Sep 27 16:14 cpuset.effective_mems
-rw-r--r-- 1 root root 0 Sep 27 16:14 cpuset.mem_exclusive
-rw-r--r-- 1 root root 0 Sep 27 16:14 cpuset.mem_hardwall
-rw-r--r-- 1 root root 0 Sep 27 16:14 cpuset.memory_migrate
-r--r--r-- 1 root root 0 Sep 27 16:14 cpuset.memory_pressure
-rw-r--r-- 1 root root 0 Sep 27 16:14 cpuset.memory_spread_page
-rw-r--r-- 1 root root 0 Sep 27 16:14 cpuset.memory_spread_slab
-rw-r--r-- 1 root root 0 Sep 27 16:16 cpuset.mems
-rw-r--r-- 1 root root 0 Sep 27 16:14 cpuset.sched_load_balance
-rw-r--r-- 1 root root 0 Sep 27 16:14 cpuset.sched_relax_domain_level
-rw-r--r-- 1 root root 0 Sep 27 16:14 notify_on_release
-rw-r--r-- 1 root root 0 Sep 27 16:17 tasks
```

- 其中tasks表示需要加入的进程ID和线程ID
- cpuset.mems代表 绑定的内存节点
- cpuset.cpus代表绑定的核心id

> 安装 numactl 确定 逻辑核和内存节点的关系

```shell
yum -y install numactl
```

- 每个内存节点和 NUMA 节点一一对应。如果进程的内存需求量较大，可以把所有的 NUMA  节点都配置进去。这里就用到了 NUMA 的概念
- 出于性能的考虑，配置的逻辑核和内存节点一般属于同一个 NUMA 节点，可用 numactl --hardware 命令获知它们的映射关系。

```shell
[root@k8s-node01 ~]#  numactl --hardware
available: 1 nodes (0)
node 0 cpus: 0 1 2 3 4 5 6 7 8 9
node 0 size: 16383 MB
node 0 free: 2172 MB
node distances:
node   0 
  0:  10 
```

- 我的机器有10个核，将我们的测试程序绑定到id=9的核心上

```shell
[root@k8s-node01 ~]# lscpu 
Architecture:          x86_64
CPU op-mode(s):        32-bit, 64-bit
Byte Order:            Little Endian
CPU(s):                10
On-line CPU(s) list:   0-9
```

- 很显然，我的主机是虚拟机，没有采用 NUMA 架构，只需将 cpuset.mems设为节点 0 就好了

```shell
echo "9" > /sys/fs/cgroup/cpuset/my_cpu_set/cpuset.cpus
echo "0" > /sys/fs/cgroup/cpuset/my_cpu_set/cpuset.mems

```

> 编写go的cpu密集型程序

- 我们这里使用md5计算hash，并循环计算，可以轻松打满一个核心

```shell
cat <<EOF >cpu_set.go
package main

import (
	"crypto/md5"
	"encoding/hex"
	"fmt"
)

func getHash() {
	i := 0
	for {

		h := md5.New()
		h.Write([]byte(fmt.Sprintf("md5_hash_%d", i)))
		hex.EncodeToString(h.Sum(nil))
		i++
	}
}

func main() {
	getHash()
}


EOF
```

- 编译运行

```shell
go build cpu_set.go 
./cpu_set
```

- 在未使用cpu set绑核前top观察
  - 可以看到 cpu_set使用了 107.0的cpu代表就是1个核心
  - 但是每个核心并的us并没有跑满，证明cpu_set在多个核心汇总切换，使用的是cfs调度
- top截屏

```shell
top - 16:38:27 up 38 days,  5:34,  2 users,  load average: 1.39, 1.83, 2.11
Tasks: 348 total,   1 running, 347 sleeping,   0 stopped,   0 zombie
%Cpu0  : 12.3 us,  1.3 sy,  0.0 ni, 85.0 id,  0.0 wa,  0.0 hi,  0.3 si,  1.0 st
%Cpu1  : 11.0 us,  1.7 sy,  0.0 ni, 86.6 id,  0.0 wa,  0.0 hi,  0.3 si,  0.3 st
%Cpu2  : 11.8 us,  1.0 sy,  0.0 ni, 86.5 id,  0.0 wa,  0.0 hi,  0.0 si,  0.7 st
%Cpu3  : 14.6 us,  1.0 sy,  0.0 ni, 83.1 id,  0.0 wa,  0.0 hi,  0.0 si,  1.3 st
%Cpu4  : 14.7 us,  1.3 sy,  0.0 ni, 82.6 id,  0.0 wa,  0.0 hi,  0.0 si,  1.3 st
%Cpu5  : 10.7 us,  1.0 sy,  0.0 ni, 87.3 id,  0.0 wa,  0.0 hi,  0.0 si,  1.0 st
%Cpu6  : 12.5 us,  2.4 sy,  0.0 ni, 84.5 id,  0.0 wa,  0.0 hi,  0.0 si,  0.7 st
%Cpu7  :  9.1 us,  1.7 sy,  0.0 ni, 88.9 id,  0.0 wa,  0.0 hi,  0.0 si,  0.3 st
%Cpu8  :  7.6 us,  1.0 sy,  0.0 ni, 89.7 id,  0.0 wa,  0.0 hi,  0.0 si,  1.7 st
%Cpu9  : 16.4 us,  1.7 sy,  0.0 ni, 81.6 id,  0.0 wa,  0.0 hi,  0.0 si,  0.3 st
KiB Mem : 16265484 total,  2231800 free,  5530572 used,  8503112 buff/cache
KiB Swap:        0 total,        0 free,        0 used.  9423760 avail Mem 

  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND                                                                                                                                                           
27891 root      20   0  705932   7648    896 S 107.0  0.0   0:32.51 cpu_set                                                                                                                                                           
 6049 nfsnobo+  20   0 1847756 399580  43732 S   9.6  2.5 849:23.71 prometheus                                                                                                                                                        
14736 root      20   0   14.3g   1.3g  28852 S   6.6  8.1 589:45.52 m3dbnode                                                                                                                                                          
29147 root      20   0 3985248 142520  38548 S   6.0  0.9   4405:41 kubelet                                                                                                                                                           
         
```

> 将cpu_set的进程pid写入cpuset的tasks中观察

```shell
ps -ef |grep cpu_set   
root     25292 15952 92 16:48 pts/0    00:00:04 ./cpu_set
root     25454  5220  0 16:48 pts/1    00:00:00 grep --color=auto cpu_set
echo 25292 > /sys/fs/cgroup/cpuset/my_cpu_set/tasks    

```

- 发现top命令中cpu9的使用并未达到100%，但cpu_set还是达到了100，说明我们的go程序绑核失败了

```shell
top - 16:49:02 up 38 days,  5:45,  2 users,  load average: 1.55, 2.04, 2.10
Tasks: 350 total,   1 running, 349 sleeping,   0 stopped,   0 zombie
%Cpu0  : 13.2 us,  2.7 sy,  0.3 ni, 82.8 id,  0.0 wa,  0.0 hi,  0.0 si,  1.0 st
%Cpu1  : 15.0 us,  3.0 sy,  0.3 ni, 80.3 id,  0.0 wa,  0.0 hi,  0.0 si,  1.3 st
%Cpu2  : 16.4 us,  2.3 sy,  0.0 ni, 80.3 id,  0.0 wa,  0.0 hi,  0.0 si,  1.0 st
%Cpu3  : 18.8 us,  3.3 sy,  0.0 ni, 76.6 id,  0.0 wa,  0.0 hi,  0.0 si,  1.3 st
%Cpu4  : 15.0 us,  3.3 sy,  0.0 ni, 79.3 id,  0.3 wa,  0.0 hi,  0.0 si,  2.0 st
%Cpu5  : 12.7 us,  5.7 sy,  0.7 ni, 79.9 id,  0.0 wa,  0.0 hi,  0.0 si,  1.0 st
%Cpu6  : 12.2 us,  2.6 sy,  0.0 ni, 82.9 id,  0.0 wa,  0.0 hi,  0.0 si,  2.3 st
%Cpu7  :  8.3 us,  5.6 sy,  0.3 ni, 83.8 id,  0.0 wa,  0.0 hi,  0.0 si,  2.0 st
%Cpu8  : 12.7 us,  4.3 sy,  0.3 ni, 81.6 id,  0.0 wa,  0.0 hi,  0.0 si,  1.0 st
%Cpu9  : 22.4 us,  4.7 sy,  0.3 ni, 71.2 id,  0.0 wa,  0.0 hi,  0.3 si,  1.0 st
KiB Mem : 16265484 total,  2192988 free,  5547708 used,  8524788 buff/cache
KiB Swap:        0 total,        0 free,        0 used.  9407060 avail Mem 

  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND                                                                                                                                                           
25292 root      20   0  705932   8304    884 S 107.3  0.1   0:16.54 cpu_set                                                                                                                                                           
 3925 centos    20   0 1664644 413520  58280 S  18.3  2.5   3037:29 prometheus                                                                                                                                                        
29147 root      20   0 3985248 141552  38548 S  15.6  0.9   4407:02 kubelet                                                                                                                                                           
14736 root      20   0   14.3g   1.3g  28852 S  12.0  8.1 590:45.08 m3dbnode                                                                                                                                                          

```

> 原因是go的runtime GMP模型会调度多个核心的p

- 我们将go代码修改为只启动一个cpu

```go
package main

import (
	"crypto/md5"
	"encoding/hex"
	"fmt"
	"runtime"
)

func getHash() {
	i := 0
	for {

		h := md5.New()
		h.Write([]byte(fmt.Sprintf("md5_hash_%d", i)))
		hex.EncodeToString(h.Sum(nil))
		i++
	}
}

func main() {
	runtime.GOMAXPROCS(1)
	getHash()
}
```

- 再次top观察 可以看到cpu9使用达到了将近100%，和cpu_set一致

```shell
top - 16:46:10 up 38 days,  5:42,  2 users,  load average: 2.40, 2.12, 2.12
Tasks: 350 total,   2 running, 348 sleeping,   0 stopped,   0 zombie
%Cpu0  :  3.4 us,  1.3 sy,  0.0 ni, 94.3 id,  0.0 wa,  0.0 hi,  0.0 si,  1.0 st
%Cpu1  :  1.3 us,  0.7 sy,  0.0 ni, 95.7 id,  0.0 wa,  0.0 hi,  0.0 si,  2.3 st
%Cpu2  :  2.3 us,  1.3 sy,  0.0 ni, 94.7 id,  0.0 wa,  0.0 hi,  0.0 si,  1.7 st
%Cpu3  :  2.3 us,  2.0 sy,  0.0 ni, 94.4 id,  0.0 wa,  0.0 hi,  0.0 si,  1.3 st
%Cpu4  :  6.3 us,  2.0 sy,  0.0 ni, 90.0 id,  0.0 wa,  0.0 hi,  0.3 si,  1.3 st
%Cpu5  :  3.6 us,  2.0 sy,  0.0 ni, 92.4 id,  0.0 wa,  0.0 hi,  0.3 si,  1.7 st
%Cpu6  :  2.6 us,  2.3 sy,  0.0 ni, 93.4 id,  0.0 wa,  0.0 hi,  0.0 si,  1.7 st
%Cpu7  :  3.4 us,  3.4 sy,  0.0 ni, 92.3 id,  0.0 wa,  0.0 hi,  0.0 si,  1.0 st
%Cpu8  :  3.7 us,  2.0 sy,  0.0 ni, 93.0 id,  0.0 wa,  0.0 hi,  0.0 si,  1.3 st
%Cpu9  : 99.3 us,  0.3 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi,  0.3 si,  0.0 st
KiB Mem : 16265484 total,  2259720 free,  5486980 used,  8518784 buff/cache
KiB Swap:        0 total,        0 free,        0 used.  9467772 avail Mem 

  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND                                                                                                                                                           
16157 root      20   0  705420   8548    840 R 100.7  0.1   0:44.28 cpu_set                                                                                                                                                           
29147 root      20   0 3985248 142368  38548 S  12.3  0.9   4406:40 kubelet                                                                                                                                                           
 9088 root      20   0  745500  58512  13852 S   8.3  0.4 676:20.79 k8s-mon                                                                                                                                                           
    
```

# 本节重点总结 :

- numa和cpu计算密集型任务的关系
- cgroup的cpuset绑核技术
- 编写go代码体会cpuset，go中 GOMAXPROCS和绑核的关系