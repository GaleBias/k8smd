# 本节重点总结

- housekeeping 是syncLoop中事件管道，做 Pod 清理工作；
- HandlePodCleanups执行一系列清理工作

  - 包括终止pod workers
  - 杀死不需要的pod
  - 并删除孤立卷/pod目录
  - 使用此方法时，不会向pod workers发送配置更改，这意味着不会有新的pod创建
  - 注意：此函数由主同步循环执行，因此不应包含任何阻塞调用。
- 孤儿进程的定义为正在运行的与缓存中的差异
- 清理的项目的细则

  - 清理probeManager中的pod
  - 从runtime中清理不需要的pod
  - pod的volume目录
  - pod的目录和子目录
  - 清理孤儿pod的cgroup
  - 清理backoff 重试map中的数据

# 接上回

- 我们分析了syncLoop中的第一大事件管道，configCh，下面来分析一下其他的事件管道

# housekeepingCh的作用

- housekeeping 是syncLoop中事件管道，做 Pod 清理工作；

# 调用的入口

- 在syncLoop中会初始化housekeepingCh，位置 D:\go_path\src\github.com\kubernetes\kubernetes\pkg\kubelet\kubelet.go

```go
	housekeepingTicker := time.NewTicker(housekeepingPeriod)
	defer housekeepingTicker.Stop()
```

- 并传入syncLoopIteration中做监听处理

```go
	if !kl.syncLoopIteration(updates, handler, syncTicker.C, housekeepingTicker.C, plegCh) {
			break
		}
```

## syncLoopIteration中的流程

- 首先判断对应的资源管理器如果没有ready就先跳过清理

```go
		if !kl.sourcesReady.AllReady() {
			// If the sources aren't ready or volume manager has not yet synced the states,
			// skip housekeeping, as we may accidentally delete pods from unready sources.
			klog.V(4).InfoS("SyncLoop (housekeeping, skipped): sources aren't ready yet")
		}
```

- 后面逻辑也比较简单，调用handler.HandlePodCleanups 执行清理操作，计算耗时如果超过15秒就打印个日志

```go
{
			start := time.Now()
			klog.V(4).InfoS("SyncLoop (housekeeping)")
			if err := handler.HandlePodCleanups(); err != nil {
				klog.ErrorS(err, "Failed cleaning pods")
			}
			duration := time.Since(start)
			if duration > housekeepingWarningDuration {
				klog.ErrorS(fmt.Errorf("housekeeping took too long"), "Housekeeping took longer than 15s", "seconds", duration.Seconds())
			}
			klog.V(4).InfoS("SyncLoop (housekeeping) end")
		}
```

- 所以我们的重点在handler.HandlePodCleanups的分析上，首先来看下这个handler是什么，可以看到在syncLoop传入的地方就是kl对应就是kubelet对象

```go
kl.syncLoop(updates, kl)
```

## HandlePodCleanups

### 前言

- 首先我们从方法的注释上可以看到这个方法主要目的

```go
// HandlePodCleanups performs a series of cleanup work, including terminating
// pod workers, killing unwanted pods, and removing orphaned volumes/pod
// directories. No config changes are sent to pod workers while this method
// is executing which means no new pods can appear.
// NOTE: This function is executed by the main sync loop, so it
// should not contain any blocking calls.
```

- HandlePodCleanups执行一系列清理工作
  - 包括终止pod workers
  - 杀死不需要的pod
  - 并删除孤立卷/pod目录
  - 使用此方法时，不会向pod workers发送配置更改，这意味着不会有新的pod创建
  - 注意：此函数由主同步循环执行，因此不应包含任何阻塞调用。
- 好了，那么我们就可以带着注释中的内容翻看源码了

### 源码查看

- 首先通过pcm PodContainerManager 获取pod对应的cgroup信息

```go
	var (
		cgroupPods map[types.UID]cm.CgroupName
		err        error
	)
	if kl.cgroupsPerQOS {
		pcm := kl.containerManager.NewPodContainerManager()
		cgroupPods, err = pcm.GetAllPodsFromCgroups()
		if err != nil {
			return fmt.Errorf("failed to get list of pods that still exist on cgroup mounts: %v", err)
		}
	}
```

> 然后就是获取常规pod和mirrorpod

```go
allPods, mirrorPods := kl.podManager.GetPodsAndMirrorPods()
```

- 底层通过遍历podManager 的 podByUID 和mirrorPodByUID两张map得到，map的key是podUid,value是对应的pod

```go
func (pm *basicManager) GetPodsAndMirrorPods() ([]*v1.Pod, []*v1.Pod) {
	pm.lock.RLock()
	defer pm.lock.RUnlock()
	pods := podsMapToPods(pm.podByUID)
	mirrorPods := mirrorPodsMapToMirrorPods(pm.mirrorPodByUID)
	return pods, mirrorPods
}

type basicManager struct {
	// Protects all internal maps.
	lock sync.RWMutex

	// Regular pods indexed by UID.
	podByUID map[kubetypes.ResolvedPodUID]*v1.Pod
	// Mirror pods indexed by UID.
	mirrorPodByUID map[kubetypes.MirrorPodUID]*v1.Pod


```

> 通过podworker的SyncKnownPods获取pod 的对应状态map

```go
	workingPods := kl.podWorkers.SyncKnownPods(allPods)

```

- SyncKnownPods的逻辑如下
  - 遍历podSyncStatuses代表pod的状态追踪器
  - 如果pod不在传入的desiredPods中或者status是 restartRequested 就从podworker中清理掉
  - 然后就是通过pod status 做判断，填充workers map返回

```go
func (p *podWorkers) SyncKnownPods(desiredPods []*v1.Pod) map[types.UID]PodWorkerState {
	workers := make(map[types.UID]PodWorkerState)
	known := make(map[types.UID]struct{})
	for _, pod := range desiredPods {
		known[pod.UID] = struct{}{}
	}

	p.podLock.Lock()
	defer p.podLock.Unlock()

	p.podsSynced = true
	for uid, status := range p.podSyncStatuses {
		if _, exists := known[uid]; !exists || status.restartRequested {
			p.removeTerminatedWorker(uid)
		}
		switch {
		case !status.terminatedAt.IsZero():
			if status.restartRequested {
				workers[uid] = TerminatedAndRecreatedPod
			} else {
				workers[uid] = TerminatedPod
			}
		case !status.terminatingAt.IsZero():
			workers[uid] = TerminatingPod
		default:
			workers[uid] = SyncPod
		}
	}
	return workers
}
```

#### 然后根据pod的status做分类

- 分类的依据如下
  - 如果状态是SyncPod，那么代表可以是runningPods也可以是possiblyRunningPods
  - 如果状态是TerminatingPod，那么代表可以是possiblyRunningPods
  - 如果状态是TerminatedAndRecreatedPod，那么代表可以是restartablePods

```go
	runningPods := make(map[types.UID]sets.Empty)
	possiblyRunningPods := make(map[types.UID]sets.Empty)
	restartablePods := make(map[types.UID]sets.Empty)
	for uid, sync := range workingPods {
		switch sync {
		case SyncPod:
			runningPods[uid] = struct{}{}
			possiblyRunningPods[uid] = struct{}{}
		case TerminatingPod:
			possiblyRunningPods[uid] = struct{}{}
		case TerminatedAndRecreatedPod:
			restartablePods[uid] = struct{}{}
		}
	}
```

#### 清理probeManager中的pod

- 把runningPods作为desiredPods传入，意思是这些不能被清理

```go
	// Stop probing pods that are not running
	klog.V(3).InfoS("Clean up probes for terminating and terminated pods")
	kl.probeManager.CleanupPods(runningPods)

```

- CleanupPods逻辑也比较清晰，就是遍历workers，如果poduid不在desiredPods中就worker.stop

```go
func (m *manager) CleanupPods(desiredPods map[types.UID]sets.Empty) {
	m.workerLock.RLock()
	defer m.workerLock.RUnlock()

	for key, worker := range m.workers {
		if _, ok := desiredPods[key.podUID]; !ok {
			worker.stop()
		}
	}
}

```

- worker.stop的逻辑为向 stopCh chan中发送信号

```go
func (w *worker) stop() {
	select {
	case w.stopCh <- struct{}{}:
	default: // Non-blocking.
	}
}
```

- 追踪接收stopCh信号的地方，可以看到就是在run中的goto语句，如果接收到信号将break，也就是doProbe不会再执行了

```go
// run periodically probes the container.
func (w *worker) run() {
	probeTickerPeriod := time.Duration(w.spec.PeriodSeconds) * time.Second

	// If kubelet restarted the probes could be started in rapid succession.
	// Let the worker wait for a random portion of tickerPeriod before probing.
	// Do it only if the kubelet has started recently.
	if probeTickerPeriod > time.Since(w.probeManager.start) {
		time.Sleep(time.Duration(rand.Float64() * float64(probeTickerPeriod)))
	}

	probeTicker := time.NewTicker(probeTickerPeriod)

	defer func() {
		// Clean up.
		probeTicker.Stop()
		if !w.containerID.IsEmpty() {
			w.resultsManager.Remove(w.containerID)
		}

		w.probeManager.removeWorker(w.pod.UID, w.container.Name, w.probeType)
		ProberResults.Delete(w.proberResultsSuccessfulMetricLabels)
		ProberResults.Delete(w.proberResultsFailedMetricLabels)
		ProberResults.Delete(w.proberResultsUnknownMetricLabels)
	}()

probeLoop:
	for w.doProbe() {
		// Wait for next probe tick.
		select {
		case <-w.stopCh:
			break probeLoop
		case <-probeTicker.C:
		case <-w.manualTriggerCh:
			// continue
		}
	}
}

```

#### 清理runtime中的容器

- 首先通过runtimeCache获取在runtime中运行的pods

```go
	runningRuntimePods, err := kl.runtimeCache.GetPods()
	if err != nil {
		klog.ErrorS(err, "Error listing containers")
		return err
	}
```

- 遍历runningRuntimePods 在workingPods查看他们的状态，如果状态是SyncPod或者TerminatingPod就不清理

```go
	for _, runningPod := range runningRuntimePods {
		switch workerState, ok := workingPods[runningPod.ID]; {
		case ok && workerState == SyncPod, ok && workerState == TerminatingPod:
			// if the pod worker is already in charge of this pod, we don't need to do anything
			continue
```

- 对于其他的状态就应该调用podWorkers.UpdatePod发送kubetypes.SyncPodKill，底层会调用runtime的killpod终止

```go
		default:
			// If the pod isn't in the set that should be running and isn't already terminating, terminate
			// now. This termination is aggressive because all known pods should already be in a known state
			// (i.e. a removed static pod should already be terminating), so these are pods that were
			// orphaned due to kubelet restart or bugs. Since housekeeping blocks other config changes, we
			// know that another pod wasn't started in the background so we are safe to terminate the
			// unknown pods.
			if _, ok := allPodsByUID[runningPod.ID]; !ok {
				klog.V(3).InfoS("Clean up orphaned pod containers", "podUID", runningPod.ID)
				one := int64(1)
				kl.podWorkers.UpdatePod(UpdatePodOptions{
					UpdateType: kubetypes.SyncPodKill,
					RunningPod: runningPod,
					KillPodOptions: &KillPodOptions{
						PodTerminationGracePeriodSecondsOverride: &one,
					},
				})
			}
		}
```

#### 清理孤儿pod状态

- 整体的逻辑也比较清晰，遍历podStatuses 中的pod如果pod不在 podManager中那么在podStatuses清理掉

```go
	// Remove orphaned pod statuses not in the total list of known config pods
	klog.V(3).InfoS("Clean up orphaned pod statuses")
	kl.removeOrphanedPodStatuses(allPods, mirrorPods)

// removeOrphanedPodStatuses removes obsolete entries in podStatus where
// the pod is no longer considered bound to this node.
func (kl *Kubelet) removeOrphanedPodStatuses(pods []*v1.Pod, mirrorPods []*v1.Pod) {
	podUIDs := make(map[types.UID]bool)
	for _, pod := range pods {
		podUIDs[pod.UID] = true
	}
	for _, pod := range mirrorPods {
		podUIDs[pod.UID] = true
	}
	kl.statusManager.RemoveOrphanedStatuses(podUIDs)
}

// TODO(filipg): It'd be cleaner if we can do this without signal from user.
func (m *manager) RemoveOrphanedStatuses(podUIDs map[types.UID]bool) {
	m.podStatusesLock.Lock()
	defer m.podStatusesLock.Unlock()
	for key := range m.podStatuses {
		if _, ok := podUIDs[key]; !ok {
			klog.V(5).InfoS("Removing pod from status map.", "podUID", key)
			delete(m.podStatuses, key)
		}
	}
}
```

#### 清理孤儿pod的目录

- 首先从containerRuntime获取pod，参数为false代表只获取running的pod

```go
	runningRuntimePods, err = kl.containerRuntime.GetPods(false)
	if err != nil {
		klog.ErrorS(err, "Error listing containers")
		return err
	}
```

- 然后就是清理孤儿pod的目录

```go
	klog.V(3).InfoS("Clean up orphaned pod directories")
	err = kl.cleanupOrphanedPodDirs(allPods, runningRuntimePods)
	if err != nil {
		// We want all cleanup tasks to be run even if one of them failed. So
		// we just log an error here and continue other cleanup tasks.
		// This also applies to the other clean up tasks.
		klog.ErrorS(err, "Failed cleaning up orphaned pod directories")
	}

```

> cleanupOrphanedPodDirs逻辑

- 首先把传入的allPods和runningRuntimePods做并集，然后从节点磁盘上获取pod

```go
	allPods := sets.NewString()
	for _, pod := range pods {
		allPods.Insert(string(pod.UID))
	}
	for _, pod := range runningPods {
		allPods.Insert(string(pod.ID))
	}

	found, err := kl.listPodsFromDisk()
	if err != nil {
		return err
	}

```

- 然后遍历本地磁盘获取的pod found，如果pod在running中就忽略

```go
	for _, uid := range found {
		if allPods.Has(string(uid)) {
			continue
		}
```

- 如果pod的volume还没有卸载或者解绑，为了避免数据错乱就忽略

```go
		// If volumes have not been unmounted/detached, do not delete directory.
		// Doing so may result in corruption of data.
		// TODO: getMountedVolumePathListFromDisk() call may be redundant with
		// kl.getPodVolumePathListFromDisk(). Can this be cleaned up?
		if podVolumesExist := kl.podVolumesExist(uid); podVolumesExist {
			klog.V(3).InfoS("Orphaned pod found, but volumes are not cleaned up", "podUID", uid)
			continue
		}

```

- 调用removeOrphanedPodVolumeDirs 尝试删除 目录

```go
		// Attempt to remove the pod volumes directory and its subdirs
		podVolumeErrors := kl.removeOrphanedPodVolumeDirs(uid)
		if len(podVolumeErrors) > 0 {
			orphanVolumeErrors = append(orphanVolumeErrors, podVolumeErrors...)
			// Not all volumes were removed, so don't clean up the pod directory yet. It is likely
			// that there are still mountpoints or files left which could cause removal of the pod
			// directory to fail below.
			// Errors for all removal operations have already been recorded, so don't add another
			// one here.
			continue
		}
```

> 删除pod目录和pod的子目录

- 获取podDir和podSubdirs

```go
		// Call RemoveAllOneFilesystem for remaining subdirs under the pod directory
		podDir := kl.getPodDir(uid)
		podSubdirs, err := ioutil.ReadDir(podDir)
		if err != nil {
			klog.ErrorS(err, "Could not read directory", "path", podDir)
			orphanRemovalErrors = append(orphanRemovalErrors, fmt.Errorf("orphaned pod %q found, but error %v occurred during reading the pod dir from disk", uid, err))
			continue
		}
```

- 遍历podSubdirs删除，删除podDir

```go

		for _, podSubdir := range podSubdirs {
			podSubdirName := podSubdir.Name()
			podSubdirPath := filepath.Join(podDir, podSubdirName)
			// Never attempt RemoveAllOneFilesystem on the volumes directory,
			// as this could lead to data loss in some situations. The volumes
			// directory should have been removed by removeOrphanedPodVolumeDirs.
			if podSubdirName == "volumes" {
				err := fmt.Errorf("volumes subdir was found after it was removed")
				klog.ErrorS(err, "Orphaned pod found, but failed to remove volumes subdir", "podUID", uid, "path", podSubdirPath)
				continue
			}
			if err := removeall.RemoveAllOneFilesystem(kl.mounter, podSubdirPath); err != nil {
				klog.ErrorS(err, "Failed to remove orphaned pod subdir", "podUID", uid, "path", podSubdirPath)
				orphanRemovalErrors = append(orphanRemovalErrors, fmt.Errorf("orphaned pod %q found, but error %v occurred when trying to remove subdir %q", uid, err, podSubdirPath))
			}
		}

		// Rmdir the pod dir, which should be empty if everything above was successful
		klog.V(3).InfoS("Orphaned pod found, removing", "podUID", uid)
		if err := syscall.Rmdir(podDir); err != nil {
			klog.ErrorS(err, "Failed to remove orphaned pod dir", "podUID", uid)
			orphanRemovalErrors = append(orphanRemovalErrors, fmt.Errorf("orphaned pod %q found, but error %v occurred when trying to remove the pod directory", uid, err))
		}
```

#### 清理孤儿MirrorPods

```go
	// Remove any orphaned mirror pods (mirror pods are tracked by name via the
	// pod worker)
	klog.V(3).InfoS("Clean up orphaned mirror pods")
	kl.deleteOrphanedMirrorPods()

```

#### 清理孤儿pod的cgroups

```go
	// Remove any cgroups in the hierarchy for pods that are definitely no longer
	// running (not in the container runtime).
	if kl.cgroupsPerQOS {
		pcm := kl.containerManager.NewPodContainerManager()
		klog.V(3).InfoS("Clean up orphaned pod cgroups")
		kl.cleanupOrphanedPodCgroups(pcm, cgroupPods, possiblyRunningPods)
	}
```

- cleanupOrphanedPodCgroups的逻辑为如果 pod在possiblyRunningPods中则忽略，如果还有volume挂载就忽略

```go
// cleanupOrphanedPodCgroups removes cgroups that should no longer exist.
// it reconciles the cached state of cgroupPods with the specified list of runningPods
func (kl *Kubelet) cleanupOrphanedPodCgroups(pcm cm.PodContainerManager, cgroupPods map[types.UID]cm.CgroupName, possiblyRunningPods map[types.UID]sets.Empty) {
	// Iterate over all the found pods to verify if they should be running
	for uid, val := range cgroupPods {
		// if the pod is in the running set, its not a candidate for cleanup
		if _, ok := possiblyRunningPods[uid]; ok {
			continue
		}

		// If volumes have not been unmounted/detached, do not delete the cgroup
		// so any memory backed volumes don't have their charges propagated to the
		// parent croup.  If the volumes still exist, reduce the cpu shares for any
		// process in the cgroup to the minimum value while we wait.  if the kubelet
		// is configured to keep terminated volumes, we will delete the cgroup and not block.
		if podVolumesExist := kl.podVolumesExist(uid); podVolumesExist && !kl.keepTerminatedPodVolumes {
			klog.V(3).InfoS("Orphaned pod found, but volumes not yet removed.  Reducing cpu to minimum", "podUID", uid)
			if err := pcm.ReduceCPULimits(val); err != nil {
				klog.InfoS("Failed to reduce cpu time for pod pending volume cleanup", "podUID", uid, "err", err)
			}
			continue
		}
		klog.V(3).InfoS("Orphaned pod found, removing pod cgroups", "podUID", uid)
		// Destroy all cgroups of pod that should not be running,
		// by first killing all the attached processes to these cgroups.
		// We ignore errors thrown by the method, as the housekeeping loop would
		// again try to delete these unwanted pod cgroups
		go pcm.Destroy(val)
	}
}
```

#### 清理backoff 重试map中的数据

```go
kl.backOff.GC()
func (p *Backoff) GC() {
	p.Lock()
	defer p.Unlock()
	now := p.Clock.Now()
	for id, entry := range p.perItemBackoff {
		if now.Sub(entry.lastUpdate) > p.maxDuration*2 {
			// GC when entry has not been updated for 2*maxDuration
			delete(p.perItemBackoff, id)
		}
	}
}
```

# 本节重点总结

- housekeeping 是syncLoop中事件管道，做 Pod 清理工作；
- HandlePodCleanups执行一系列清理工作

  - 包括终止pod workers
  - 杀死不需要的pod
  - 并删除孤立卷/pod目录
  - 使用此方法时，不会向pod workers发送配置更改，这意味着不会有新的pod创建
  - 注意：此函数由主同步循环执行，因此不应包含任何阻塞调用。
- 孤儿进程的定义为正在运行的与缓存中的差异
- 清理的项目的细则

  - 清理probeManager中的pod
  - 从runtime中清理不需要的pod
  - pod的volume目录
  - pod的目录和子目录
  - 清理孤儿pod的cgroup
  - 清理backoff 重试map中的数据