# 本节重点总结

- CronJobControllerV1 虽然也使用了控制器模式，但是它的实现与其他的控制器不太一样：
- 他没有从 Informer 中接受其他消息变动的通知，而是直接访问 apiserver 中的数据：
  - 每隔 10s 会从 apiserver 中取出job和cronjob资源并进行检查是否应该触发调度创建新的资源![cronjob.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/908/1636075752000/46c2791c9bcc45128289de96781894bd.png)
- CronJobControllerV2 使用了延迟队列和 informer 缓存
- 可以看到cronjobv2控制器跟job控制器一样的两步走，先初始化控制器对象，在运行Run方法
  - 初始化控制器中可以看到初始化了job和cronjob为一对的诸多对象
    - 和v1的显著不同就是有了workqueue
    - JobLister/cronJobLister 代表job/cronjob的存储
    - JobControl/cronJobControl类，代表更新job/CronJob的接口
    - 同时添加了job/cronJob的事件回调
  - Run 启动workers个数个 worker 执行同步的工作
    - 同步任务就是获取cronJob和关联的job进行协调同步

# cronjob 历史

- CronJob 顾名思义就是定时/周期性任务
- CronJob 从 Kubernetes v1.4 开始引入，到 v1.8 时进入到 Beta 阶段
- 事实上在 2021 年 2 月份的时候，CronJobV2 controller 已经成为了它默认的控制器版本
- 也就是说当你在 Kubernetes v1.21 版本中使用 CronJob 时，如果不想使用 CronJobV2 的控制器，而想要换回原始的控制器时，那你需要显式的将它禁用掉，比如：

```shell
--feature-gates="CronJobControllerV2=false"
```

## cronJobV2

- 但还是建议使用 CronJobV2 controller ，这个版本用了延迟队列和 informer 缓存
- 原始版本的控制器简陋了些，也会带来一些问题，比如当镜像/服务不可用时，会产生无限的 Pod 泄漏的问题。

# cronJob 源码入口

- 在kube-controller-manager中的batch中，位置 D:\go_path\src\github.com\kubernetes\kubernetes\cmd\kube-controller-manager\app\batch.go

```go
func startCronJobController(ctx ControllerContext) (controller.Interface, bool, error) {
	if utilfeature.DefaultFeatureGate.Enabled(kubefeatures.CronJobControllerV2) {
		cj2c, err := cronjob.NewControllerV2(ctx.InformerFactory.Batch().V1().Jobs(),
			ctx.InformerFactory.Batch().V1().CronJobs(),
			ctx.ClientBuilder.ClientOrDie("cronjob-controller"),
		)
		if err != nil {
			return nil, true, fmt.Errorf("error creating CronJob controller V2: %v", err)
		}
		go cj2c.Run(int(ctx.ComponentConfig.CronJobController.ConcurrentCronJobSyncs), ctx.Stop)
		return nil, true, nil
	}
	cjc, err := cronjob.NewController(
		ctx.ClientBuilder.ClientOrDie("cronjob-controller"),
	)
	if err != nil {
		return nil, true, fmt.Errorf("error creating CronJob controller: %v", err)
	}
	go cjc.Run(ctx.Stop)
	return nil, true, nil
}

```

- 在上面的startCronJobController中可以明确看到根据是否启用 CronJobControllerV2特性决定用哪个控制器

## cronJobV1 架构简介

- 如果没有开启CronJobControllerV2特性，那么可以看到使用的控制器为 cronjob.NewController ，我们把它称为CronJobControllerV1

```go
	cjc, err := cronjob.NewController(
		ctx.ClientBuilder.ClientOrDie("cronjob-controller"),
	)
	if err != nil {
		return nil, true, fmt.Errorf("error creating CronJob controller: %v", err)
	}
	go cjc.Run(ctx.Stop)
```

- CronJobControllerV1 虽然也使用了控制器模式，但是它的实现与其他的控制器不太一样：
- 他没有从 Informer 中接受其他消息变动的通知，而是直接访问 apiserver 中的数据：
  ![cronjob.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/908/1636075752000/443ac054738f4309bc531069320f79f8.png)
- 追踪Controllerv1的定义也可以发现没有workqueue字段，再次印证了上面的问题

```go
type Controller struct {
	kubeClient clientset.Interface
	jobControl jobControlInterface
	cjControl  cjControlInterface
	podControl podControlInterface
	recorder   record.EventRecorder
}

```

> 观察Controllerv1的Run方法

- 可以看到每10秒执行一次syncAll方法

```go
// Run starts the main goroutine responsible for watching and syncing jobs.
func (jm *Controller) Run(stopCh <-chan struct{}) {
	defer utilruntime.HandleCrash()
	klog.Infof("Starting CronJob Manager")
	// Check things every 10 second.
	go wait.Until(jm.syncAll, 10*time.Second, stopCh)
	<-stopCh
	klog.Infof("Shutting down CronJob Manager")
}
```

> syncAll

- syncAll我们大概看一下流程就是每隔 10s 会从 apiserver 中取出job和cronjob资源并进行检查是否应该触发调度创建新的资源
- 由于k8s新版本默认的cronjob控制器已经是 Controllerv2了，我们这里就不过多的分析v1的逻辑了

```go
func (jm *Controller) syncAll() {
	jobListFunc := func(opts metav1.ListOptions) (runtime.Object, error) {
		return jm.kubeClient.BatchV1().Jobs(metav1.NamespaceAll).List(context.TODO(), opts)
	}
    ....
	err = pager.New(func(ctx context.Context, opts metav1.ListOptions) (runtime.Object, error) {
		return jm.kubeClient.BatchV1().CronJobs(metav1.NamespaceAll).List(ctx, opts)
	}).EachListItem(context.Background(), metav1.ListOptions{}, func(object runtime.Object) error {
		cj, ok := object.(*batchv1.CronJob)
		if !ok {
			return fmt.Errorf("expected type *batchv1.CronJob, got type %T", cj)
		}
		syncOne(cj, jobsByCj[cj.UID], time.Now(), jm.jobControl, jm.cjControl, jm.recorder)
		cleanupFinishedJobs(cj, jobsByCj[cj.UID], jm.jobControl, jm.cjControl, jm.recorder)
		return nil
	})
```

## cronJobV2 代码解读

### 调用入口在startCronJobController中

- 可以看到跟job控制器一样的两步走，先初始化控制器对象，在运行Run方法

```go
	if utilfeature.DefaultFeatureGate.Enabled(kubefeatures.CronJobControllerV2) {
		cj2c, err := cronjob.NewControllerV2(ctx.InformerFactory.Batch().V1().Jobs(),
			ctx.InformerFactory.Batch().V1().CronJobs(),
			ctx.ClientBuilder.ClientOrDie("cronjob-controller"),
		)
		if err != nil {
			return nil, true, fmt.Errorf("error creating CronJob controller V2: %v", err)
		}
		go cj2c.Run(int(ctx.ComponentConfig.CronJobController.ConcurrentCronJobSyncs), ctx.Stop)
		return nil, true, nil
	}
```

### 01 初始化控制器

#### 传参分析

- 参数为3个
- jobInformer代表获取job对象的informer
- cronJobsInformer代表获取cronJob对象的informer
- kubeClient代表和apiserver通信的client

#### NewControllerV2代码和之前分析的 job非常像

- 首先初始化event并设置Broadcaster

```go
	eventBroadcaster := record.NewBroadcaster()
	eventBroadcaster.StartStructuredLogging(0)
	eventBroadcaster.StartRecordingToSink(&covev1client.EventSinkImpl{Interface: kubeClient.CoreV1().Events("")})

```

> 设置kubeclient请求限速相关的metrics

```go
	if kubeClient != nil && kubeClient.CoreV1().RESTClient().GetRateLimiter() != nil {
		if err := ratelimiter.RegisterMetricAndTrackRateLimiterUsage("cronjob_controller", kubeClient.CoreV1().RESTClient().GetRateLimiter()); err != nil {
			return nil, err
		}
	}
```

##### 构造控制器 对象jm

- 这里看到和v1的显著不同就是有了workqueue

```go
	jm := &ControllerV2{
		queue:    workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), "cronjob"),
		recorder: eventBroadcaster.NewRecorder(scheme.Scheme, corev1.EventSource{Component: "cronjob-controller"}),

		jobControl:     realJobControl{KubeClient: kubeClient},
		cronJobControl: &realCJControl{KubeClient: kubeClient},

		jobLister:     jobInformer.Lister(),
		cronJobLister: cronJobsInformer.Lister(),

		jobListerSynced:     jobInformer.Informer().HasSynced,
		cronJobListerSynced: cronJobsInformer.Informer().HasSynced,
		now:                 time.Now,
	}

```

- jobControl 代表 操作job增删的接口，它的方法如下

```go
type jobControlInterface interface {
	// GetJob retrieves a Job.
	GetJob(namespace, name string) (*batchv1.Job, error)
	// CreateJob creates new Jobs according to the spec.
	CreateJob(namespace string, job *batchv1.Job) (*batchv1.Job, error)
	// UpdateJob updates a Job.
	UpdateJob(namespace string, job *batchv1.Job) (*batchv1.Job, error)
	// PatchJob patches a Job.
	PatchJob(namespace string, name string, pt types.PatchType, data []byte, subresources ...string) (*batchv1.Job, error)
	// DeleteJob deletes the Job identified by name.
	// TODO: delete by UID?
	DeleteJob(namespace string, name string) error
}

```

- cronJobControl类似，代表更新CronJob的接口

```go
// cjControlInterface is an interface that knows how to update CronJob status
// created as an interface to allow testing.
type cjControlInterface interface {
	UpdateStatus(cj *batchv1.CronJob) (*batchv1.CronJob, error)
	// GetCronJob retrieves a CronJob.
	GetCronJob(namespace, name string) (*batchv1.CronJob, error)
}
```

- jobLister 代表job的存储

```go
// JobLister helps list Jobs.
// All objects returned here must be treated as read-only.
type JobLister interface {
	// List lists all Jobs in the indexer.
	// Objects returned here must be treated as read-only.
	List(selector labels.Selector) (ret []*v1.Job, err error)
	// Jobs returns an object that can list and get Jobs.
	Jobs(namespace string) JobNamespaceLister
	JobListerExpansion
}

```

- 类似的还有cronJobLister 代表cronjob的存储

```go
// CronJobLister helps list CronJobs.
// All objects returned here must be treated as read-only.
type CronJobLister interface {
	// List lists all CronJobs in the indexer.
	// Objects returned here must be treated as read-only.
	List(selector labels.Selector) (ret []*v1.CronJob, err error)
	// CronJobs returns an object that can list and get CronJobs.
	CronJobs(namespace string) CronJobNamespaceLister
	CronJobListerExpansion
}
```

- 下面这一对 InformerSynced代表判断job和cronjob对象是否已经list一次了

```go
	jobListerSynced     cache.InformerSynced
	cronJobListerSynced cache.InformerSynced

```

##### 给控制器添加回调

###### job的回调

```go
	jobInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
		AddFunc:    jm.addJob,
		UpdateFunc: jm.updateJob,
		DeleteFunc: jm.deleteJob,
	})

```

> AddFunc对应的 jm.addJob

- 有了job中的源码阅读经历，那么这里就很好理解了
  - 如果job.DeletionTimestamp 存在就删除job
  - 然后job应该能获取到属主信息，即controllerRef不为空，然后enqueueController推入队列中
- addJob源码如下

```go
// When a job is created, enqueue the controller that manages it and update it's expectations.
func (jm *ControllerV2) addJob(obj interface{}) {
	job := obj.(*batchv1.Job)
	if job.DeletionTimestamp != nil {
		// on a restart of the controller, it's possible a new job shows up in a state that
		// is already pending deletion. Prevent the job from being a creation observation.
		jm.deleteJob(job)
		return
	}

	// If it has a ControllerRef, that's all that matters.
	if controllerRef := metav1.GetControllerOf(job); controllerRef != nil {
		cronJob := jm.resolveControllerRef(job.Namespace, controllerRef)
		if cronJob == nil {
			return
		}
		jm.enqueueController(cronJob)
		return
	}
}
```

> UpdateFunc对应的 jm.updateJob

- 获取新旧job对象根据ResourceVersion判断是否发生变化
- 获取新旧job的属主信息也就是cronjob信息对比，如果cronjob发生变化，那么新旧job都需要同步

```go
func (jm *ControllerV2) updateJob(old, cur interface{}) {
	curJob := cur.(*batchv1.Job)
	oldJob := old.(*batchv1.Job)
	if curJob.ResourceVersion == oldJob.ResourceVersion {
		// Periodic resync will send update events for all known jobs.
		// Two different versions of the same jobs will always have different RVs.
		return
	}

	curControllerRef := metav1.GetControllerOf(curJob)
	oldControllerRef := metav1.GetControllerOf(oldJob)
	controllerRefChanged := !reflect.DeepEqual(curControllerRef, oldControllerRef)
	if controllerRefChanged && oldControllerRef != nil {
		// The ControllerRef was changed. Sync the old controller, if any.
		if cronJob := jm.resolveControllerRef(oldJob.Namespace, oldControllerRef); cronJob != nil {
			jm.enqueueController(cronJob)
		}
	}

	// If it has a ControllerRef, that's all that matters.
	if curControllerRef != nil {
		cronJob := jm.resolveControllerRef(curJob.Namespace, curControllerRef)
		if cronJob == nil {
			return
		}
		jm.enqueueController(cronJob)
		return
	}
}
```

> DeleteFunc对应的deleteJob

- 也是根据解析属主信息推入队列

```go
func (jm *ControllerV2) deleteJob(obj interface{}) {
	job, ok := obj.(*batchv1.Job)

	// When a delete is dropped, the relist will notice a job in the store not
	// in the list, leading to the insertion of a tombstone object which contains
	// the deleted key/value. Note that this value might be stale.
	if !ok {
		tombstone, ok := obj.(cache.DeletedFinalStateUnknown)
		if !ok {
			utilruntime.HandleError(fmt.Errorf("couldn't get object from tombstone %#v", obj))
			return
		}
		job, ok = tombstone.Obj.(*batchv1.Job)
		if !ok {
			utilruntime.HandleError(fmt.Errorf("tombstone contained object that is not a ReplicaSet %#v", obj))
			return
		}
	}

	controllerRef := metav1.GetControllerOf(job)
	if controllerRef == nil {
		// No controller should care about orphans being deleted.
		return
	}
	cronJob := jm.resolveControllerRef(job.Namespace, controllerRef)
	if cronJob == nil {
		return
	}
	jm.enqueueController(cronJob)
}
```

###### cronjob的回调

```go

	cronJobsInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
		AddFunc: func(obj interface{}) {
			jm.enqueueController(obj)
		},
		UpdateFunc: jm.updateCronJob,
		DeleteFunc: func(obj interface{}) {
			jm.enqueueController(obj)
		},
	})
```

- 这里看到AddFunc和DeleteFunc 都是调用 enqueueController入队

> UpdateFunc对应的 jm.updateCronJob稍显复杂一些

- 首先获取新老cronjob对象

```go
	oldCJ, okOld := old.(*batchv1.CronJob)
	newCJ, okNew := curr.(*batchv1.CronJob)

	if !okOld || !okNew {
		// typecasting of one failed, handle this better, may be log entry
		return
	}
```

- 然后判断新老cronjob对象的 Schedule时间是否变化了
  - 如果发生变化那么解析新的调度时间，并计算马上一下次要执行的时间
  - 用这个时间作为延时入队的时间把curr入队
- 对比Schedule的代码如下

```go
	if oldCJ.Spec.Schedule != newCJ.Spec.Schedule {
		// schedule changed, change the requeue time
		sched, err := cron.ParseStandard(newCJ.Spec.Schedule)
		if err != nil {
			// this is likely a user error in defining the spec value
			// we should log the error and not reconcile this cronjob until an update to spec
			klog.V(2).InfoS("Unparseable schedule for cronjob", "cronjob", klog.KRef(newCJ.GetNamespace(), newCJ.GetName()), "schedule", newCJ.Spec.Schedule, "err", err)
			jm.recorder.Eventf(newCJ, corev1.EventTypeWarning, "UnParseableCronJobSchedule", "unparseable schedule for cronjob: %s", newCJ.Spec.Schedule)
			return
		}
		now := jm.now()
		t := nextScheduledTimeDuration(sched, now)

		jm.enqueueControllerAfter(curr, *t)
		return
	}
```

- 如果走到这里说明Schedule以外的其它参数变化了，那么立即重新入队

```go
	// TODO: need to handle the change of spec.JobTemplate.metadata.labels explicitly
	//   to cleanup jobs with old labels
	jm.enqueueController(curr)
```

### 02 控制器的运行

- Run中的逻辑比较清晰 ，根据jobListerSynced 和cronJobListerSynced判断本地至少有一份资源了
- 然后启动workers个数个 worker 执行同步的工作

```go
// Run starts the main goroutine responsible for watching and syncing jobs.
func (jm *ControllerV2) Run(workers int, stopCh <-chan struct{}) {
	defer utilruntime.HandleCrash()
	defer jm.queue.ShutDown()

	klog.InfoS("Starting cronjob controller v2")
	defer klog.InfoS("Shutting down cronjob controller v2")

	if !cache.WaitForNamedCacheSync("cronjob", stopCh, jm.jobListerSynced, jm.cronJobListerSynced) {
		return
	}

	for i := 0; i < workers; i++ {
		go wait.Until(jm.worker, time.Second, stopCh)
	}

	<-stopCh
}
```

- worker中还是沿用了之前job中的for next模式

```go
func (jm *ControllerV2) worker() {
	for jm.processNextWorkItem() {
	}
}
```

#### processNextWorkItem分析

- 从队列中获取一个元素，然后用sync做同步，得到再入队的时间和错误

```go
func (jm *ControllerV2) processNextWorkItem() bool {
	key, quit := jm.queue.Get()
	if quit {
		return false
	}
	defer jm.queue.Done(key)

	requeueAfter, err := jm.sync(key.(string))

	return true
}

```

- 如果错误不为空，那么将key再推入队列中，不过是通过限速之后在入队

```go
	switch {
	case err != nil:
		utilruntime.HandleError(fmt.Errorf("error syncing CronJobController %v, requeuing: %v", key.(string), err))
		jm.queue.AddRateLimited(key)

```

- 如果requeueAfter不为空，那么先forget这个key，然后延迟requeueAfter再推入

```go
	case requeueAfter != nil:
		jm.queue.Forget(key)
		jm.queue.AddAfter(key, *requeueAfter)
	}
```

#### sync 解析

- 从cronJobKey解析 ns和name，然后调用cronJobLister本地存储获取cronjob对象，并判断返回错误

```go
	ns, name, err := cache.SplitMetaNamespaceKey(cronJobKey)
	if err != nil {
		return nil, err
	}

	cronJob, err := jm.cronJobLister.CronJobs(ns).Get(name)
	switch {
	case errors.IsNotFound(err):
		// may be cronjob is deleted, don't need to requeue this key
		klog.V(4).InfoS("CronJob not found, may be it is deleted", "cronjob", klog.KRef(ns, name), "err", err)
		return nil, nil
	case err != nil:
		// for other transient apiserver error requeue with exponential backoff
		return nil, err
	}
```

> 然后获取cronjob关联的job列表

- 从getJobsToBeReconciled中可以看到
  - 先通过 informer 过滤cronjob中配置的job选择器的job
- getJobsToBeReconciled代码如下

```go
	jobsToBeReconciled, err := jm.getJobsToBeReconciled(cronJob)
	if err != nil {
		return nil, err
	}
func (jm *ControllerV2) getJobsToBeReconciled(cronJob *batchv1.CronJob) ([]*batchv1.Job, error) {
	var jobSelector labels.Selector
	if len(cronJob.Spec.JobTemplate.Labels) == 0 {
		jobSelector = labels.Everything()
	} else {
		jobSelector = labels.Set(cronJob.Spec.JobTemplate.Labels).AsSelector()
	}
	jobList, err := jm.jobLister.Jobs(cronJob.Namespace).List(jobSelector)
	if err != nil {
		return nil, err
	}

	jobsToBeReconciled := []*batchv1.Job{}

	for _, job := range jobList {
		// If it has a ControllerRef, that's all that matters.
		if controllerRef := metav1.GetControllerOf(job); controllerRef != nil && controllerRef.Name == cronJob.Name {
			// this job is needs to be reconciled
			jobsToBeReconciled = append(jobsToBeReconciled, job)
		}
	}
	return jobsToBeReconciled, nil
}
```

> 然后就是调用 syncCronJob 进行同步

- 至于syncCronJob的逻辑我们下面再看

```go
	cronJobCopy, requeueAfter, err := jm.syncCronJob(cronJob, jobsToBeReconciled)
	if err != nil {
		klog.V(2).InfoS("Error reconciling cronjob", "cronjob", klog.KRef(cronJob.GetNamespace(), cronJob.GetName()), "err", err)
		return nil, err
	}
```

- 然后就是根据配置的job上限进行清理的操作， 对应的配置就是FailedJobsHistoryLimit和SuccessfulJobsHistoryLimit

```go
	err = jm.cleanupFinishedJobs(cronJobCopy, jobsToBeReconciled)
	if err != nil {
		klog.V(2).InfoS("Error cleaning up jobs", "cronjob", klog.KRef(cronJob.GetNamespace(), cronJob.GetName()), "resourceVersion", cronJob.GetResourceVersion(), "err", err)
		return nil, err
	}
```

> cleanupFinishedJobs内部逻辑比较清晰

- 首先判断如果limit没有设置就不做清理

```go
	// If neither limits are active, there is no need to do anything.
	if cj.Spec.FailedJobsHistoryLimit == nil && cj.Spec.SuccessfulJobsHistoryLimit == nil {
		return nil
	}
```

- 然后在传入的job中根据状态做区分，区分出 failedJobs和successfulJobs

```go
	failedJobs := []*batchv1.Job{}
	successfulJobs := []*batchv1.Job{}

	for _, job := range js {
		isFinished, finishedStatus := jm.getFinishedStatus(job)
		if isFinished && finishedStatus == batchv1.JobComplete {
			successfulJobs = append(successfulJobs, job)
		} else if isFinished && finishedStatus == batchv1.JobFailed {
			failedJobs = append(failedJobs, job)
		}
	}
```

- 然后就是根据相关limit配置，清理job

```go
	if cj.Spec.SuccessfulJobsHistoryLimit != nil {
		jm.removeOldestJobs(cj,
			successfulJobs,
			*cj.Spec.SuccessfulJobsHistoryLimit)
	}

	if cj.Spec.FailedJobsHistoryLimit != nil {
		jm.removeOldestJobs(cj,
			failedJobs,
			*cj.Spec.FailedJobsHistoryLimit)
	}


```

- 最后更新一下状态，同步到apiserver

```go
	// Update the CronJob, in case jobs were removed from the list.
	_, err := jm.cronJobControl.UpdateStatus(cj)
func (c *realCJControl) UpdateStatus(cj *batchv1.CronJob) (*batchv1.CronJob, error) {
	return c.KubeClient.BatchV1().CronJobs(cj.Namespace).UpdateStatus(context.TODO(), cj, metav1.UpdateOptions{})
}
```

- 至此cronjob sync的整体流程已经分析完毕了，还差其中的syncCronJob

# 本节重点总结

- CronJobControllerV1 虽然也使用了控制器模式，但是它的实现与其他的控制器不太一样：
- 他没有从 Informer 中接受其他消息变动的通知，而是直接访问 apiserver 中的数据：
  - 每隔 10s 会从 apiserver 中取出job和cronjob资源并进行检查是否应该触发调度创建新的资源
  - ![cronjob.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/908/1636075752000/1a5f0f55a6b8417a94204b77540bc9d8.png)
- CronJobControllerV2 使用了延迟队列和 informer 缓存
- 可以看到cronjobv2控制器跟job控制器一样的两步走，先初始化控制器对象，在运行Run方法
  - 初始化控制器中可以看到初始化了job和cronjob为一对的诸多对象
    - 和v1的显著不同就是有了workqueue
    - JobLister/cronJobLister 代表job/cronjob的存储
    - JobControl/cronJobControl类，代表更新job/CronJob的接口
    - 同时添加了job/cronJob的事件回调
  - Run 启动workers个数个 worker 执行同步的工作
    - 同步任务就是获取cronJob和关联的job进行协调同步