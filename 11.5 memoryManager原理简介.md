# 本节重点总结 :

- memoryManager简单说来就是为了给对内存性能要求极高的容器提供服务
  - 避免跨numa 节点使用内存，尽量使用本地内存，这样性能会更好
- 内存管理器主要做两件事：
  - 向拓扑管理器提供拓扑提示
  - 为容器分配内存并更新状态
- memoryManager 当前3个限制
  - 01 在单个和跨越多个numa节点上分配的问题
  - 02 memoryManager只能为Guaranteed pods 服务
  - 03 可能会带来内存碎片

# memoryManager是做什么的

- 文档地址[memoryManager](https://kubernetes.io/blog/2021/08/11/kubernetes-1-22-feature-memory-manager-moves-to-beta/)
- 主要是为了多NUMA 节点保障内存和大内存页分配
- 这个特性对于对内存性能要求高的应用非常有用，比如数据库类或者使用 DPDK 进行高性能数据包处理的应用要部署到k8s中
- 为了保证效率，会按内存和 CPU 的相对距离来按 node 定义是否为 local memory 或者说本地内存，同时由于实际位置不同，所以就可能会产生内存分配不均匀的情况了

> numa本地内存的问题

- 当某个进程达到了其 local memory 的上限，那势必要使用numa中远端的内存，那自然就会影响到它的性能

# memoryManager是如何工作的

> 内存管理器主要做两件事：

- 向拓扑管理器提供拓扑提示
- 为容器分配内存并更新状态

> Kubelet下Memory Manager的整体时序图
> ![memorymanagerdiagram.svg](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/908/1634434389000/6b5e5634e54044f3bc5a912e50f3b139.svg)

## 在准入控制阶段

- 当第一次处理一个新的 pod 时，kubelet 会调用 TopologyManager 的Admit()方法。
- 拓扑管理器正在调用GetTopologyHints()每个提示提供程序，包括内存管理器。
- 内存管理器为 Pod 内的每个容器计算所有可能的 NUMA 节点组合，并将提示返回给拓扑管理器。
- 拓扑管理器Allocate()为每个提示提供程序调用 ，包括内存管理器。
- 内存管理器根据拓扑管理器选择的提示分配该状态下的内存。

## 在 Pod 创建期间：

- kubelet 调用PreCreateContainer().
- 对于每个容器，内存管理器会查找为容器分配内存的 NUMA 节点，然后将该信息返回给 kubelet。
- kubelet 使用容器规范通过 CRI 创建容器，该规范包含来自内存管理器信息的信息

# memoryManager 的配置方法

> 跟cpuManager类似也是有policy策略控制

- 默认情况下，内存管理器按照None策略运行，这意味着它只会放松而不做任何事情。
- 要使用内存管理器，您应该为 kubelet 设置 --memory-manager-policy=Static

> 同时还要设置 --reserved-memory

- 设置的范式如`--reserved-memory="<numaNodeID>:<resourceName>=<quantity>"` ，同时还应遵守两条规则
  - memory资源的保留内存量必须大于零。
  - 设置的预留内存量必须等于资源的NodeAllocable ( kube-reserved + system-reserved + eviction-hard)
- 下面是正确和错误对比图![mm_reservedmemory.svg](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/908/1634434389000/122c6d1aedfb4666abf11c19bd094777.svg)
- 可以看到左面配置了两个 numaNodeID=0,1 两者预留的和为1.1Gi 等同于节点可分配的 500Mi+ 500Mi+ 100Mi

# memoryManager 当前3个限制

- 1.22 版本和 beta 升级带来了增强和修复，但内存管理器仍然有一些限制。

## 01 在单个和跨越多个numa节点上分配的问题

- NUMA 节点不能同时具有单和跨 NUMA 节点分配。当容器内存被固定到两个或多个 NUMA 节点时，我们无法知道容器将从哪个 NUMA 节点消耗内存。![singlecrossnumaallocation.svg](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/908/1634434389000/e0d02c5496194ea28f451991c3b4259b.svg)
- 下面解读一下
  - 在container1该NUMA节点0上启动并请求5Gi内存，但目前仅消耗3Gi内存。
  - container2 的内存请求是 10Gi，没有一个 NUMA 节点可以满足它。
  - 该container2消耗3.5Gi从NUMA节点0的内存，但一旦container1将需要更多的内存，它不会有它，内核会杀死与容器的一个OOM错误。
- 为防止出现此类问题，内存管理器在container2准入控制阶段就禁止请求，
  - 直到机器有两个 NUMA 节点，它们中没没有单个 NUMA 节点分配。

## 02 memoryManager只能为Guaranteed pods 服务

- Memory Manager 不能保证 Burstable pod 的内存分配，即使 Burstable pod 指定了相等的内存限制和请求。
- 假设您有两个 Burstable pod：
  - pod1的容器是配置了具有相同内存请求和限制的
  - pod2容器，仅配置了内存请求集的
  - 对Linux内核来说，任何一个pod中的进程都有相同的OOM分数，一旦内核发现自己没有足够的内存，就可以杀死属于pod的进程pod1。
  - 意思是对内核来说两个Burstable的pod 容器 oom 打分是一样的

## 03 可能会带来内存碎片

- 启动和停止的 Pod 和容器的顺序可以在 NUMA 节点上分割内存，这肯能会带来内存碎片
- 内存管理器的 alpha 实现没有任何机制来平衡 pod 和碎片整理内存

# 本节重点总结 :

- memoryManager简单说来就是为了给对内存性能要求极高的容器提供服务
  - 避免跨numa 节点使用内存，尽量使用本地内存，这样性能会更好
- 内存管理器主要做两件事：
  - 向拓扑管理器提供拓扑提示
  - 为容器分配内存并更新状态
- memoryManager 当前3个限制
  - 01 在单个和跨越多个numa节点上分配的问题
  - 02 memoryManager只能为Guaranteed pods 服务
  - 03 可能会带来内存碎片