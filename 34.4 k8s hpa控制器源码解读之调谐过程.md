# 本节重点总结
- 根据不同ResourceType调用不同的计算方法
    - 但相同的流程为，先从远端的metrics提供方获取pod的指标使用情况
    - 然后根据配置的阈值进行判断
    - 比如类型为resource，底层使用rest-client访问 /metrics.k8s.io/v1beta1的 api，底层调用metrics-server的接口获取 这个ns下这个pod的数据 



- hpa中的metric 判断阈值类型
    - 如果是targetAverageUtilization 代表百分比：
        - metrics使用值除以pod的request值获取使用百分比，然后和目标百分比相除，大于1说明要扩容，小于1要缩容 
    - 如果是targetAverageValue 代表值：
        - metrics使用值除以目标值，大于1说明要扩容，小于1要缩容 


# 接上回
- 上回讲到了hpa控制器的初始化，那么这次来分析一下Run

# run入口
- 位置 D:\go_path\src\github.com\kubernetes\kubernetes\pkg\controller\podautoscaler\horizontal.go
```go
// Run begins watching and syncing.
func (a *HorizontalController) Run(stopCh <-chan struct{}) {
	defer utilruntime.HandleCrash()
	defer a.queue.ShutDown()

	klog.Infof("Starting HPA controller")
	defer klog.Infof("Shutting down HPA controller")

	if !cache.WaitForNamedCacheSync("HPA", stopCh, a.hpaListerSynced, a.podListerSynced) {
		return
	}

	// start a single worker (we may wish to start more in the future)
	go wait.Until(a.worker, time.Second, stopCh)

	<-stopCh
}

```
- 可以看出run还是沿用了一罐的controller思路，等待informer资源在本地list一次 后启动worker
- 而且worker中还是消费队列数据，然后调用 reconcileKey进行主流程
```go
func (a *HorizontalController) worker() {
	for a.processNextWorkItem() {
	}
	klog.Infof("horizontal pod autoscaler controller worker shutting down")
}

func (a *HorizontalController) processNextWorkItem() bool {
	key, quit := a.queue.Get()
	if quit {
		return false
	}
	defer a.queue.Done(key)

	deleted, err := a.reconcileKey(key.(string))
	if err != nil {
		utilruntime.HandleError(err)
	}
	if !deleted {
		a.queue.AddRateLimited(key)
	}

	return true
}

```
- 同时 reconcileKey会在 hpa informer本地缓存中获取这个hpa，然后调用reconcileAutoscaler进行同步
```go
func (a *HorizontalController) reconcileKey(key string) (deleted bool, err error) {
	namespace, name, err := cache.SplitMetaNamespaceKey(key)
	if err != nil {
		return true, err
	}

	hpa, err := a.hpaLister.HorizontalPodAutoscalers(namespace).Get(name)
	if errors.IsNotFound(err) {
		klog.Infof("Horizontal Pod Autoscaler %s has been deleted in %s", name, namespace)
		delete(a.recommendations, key)
		delete(a.scaleUpEvents, key)
		delete(a.scaleDownEvents, key)
		return true, nil
	}
	if err != nil {
		return false, err
	}

	return false, a.reconcileAutoscaler(hpa, key)
}
```

## reconcileAutoscaler 调谐解析
- 首先是对 informer中的hpa对象进行深拷贝，得到全新的对象，避免修改informer中的缓存
- 然后将hpav1转换为hpav2 ，注释中说hpav2 更好计算metrics
```go

func (a *HorizontalController) reconcileAutoscaler(hpav1Shared *autoscalingv1.HorizontalPodAutoscaler, key string) error {
	// make a copy so that we never mutate the shared informer cache (conversion can mutate the object)
	hpav1 := hpav1Shared.DeepCopy()
	// then, convert to autoscaling/v2, which makes our lives easier when calculating metrics
	hpaRaw, err := unsafeConvertToVersionVia(hpav1, autoscalingv2.SchemeGroupVersion)
	if err != nil {
		a.eventRecorder.Event(hpav1, v1.EventTypeWarning, "FailedConvertHPA", err.Error())
		return fmt.Errorf("failed to convert the given HPA to %s: %v", autoscalingv2.SchemeGroupVersion.String(), err)
	}
```
- 然后获取这个hpa要控制的对象，比如一个dep对象 Deployment/default/nginx01
```go
	hpa := hpaRaw.(*autoscalingv2.HorizontalPodAutoscaler)
	hpaStatusOriginal := hpa.Status.DeepCopy()

	reference := fmt.Sprintf("%s/%s/%s", hpa.Spec.ScaleTargetRef.Kind, hpa.Namespace, hpa.Spec.ScaleTargetRef.Name)

```
- 构造 gvk对象
```go
	targetGV, err := schema.ParseGroupVersion(hpa.Spec.ScaleTargetRef.APIVersion)
	if err != nil {
		a.eventRecorder.Event(hpa, v1.EventTypeWarning, "FailedGetScale", err.Error())
		setCondition(hpa, autoscalingv2.AbleToScale, v1.ConditionFalse, "FailedGetScale", "the HPA controller was unable to get the target's current scale: %v", err)
		a.updateStatusIfNeeded(hpaStatusOriginal, hpa)
		return fmt.Errorf("invalid API version in scale target reference: %v", err)
	}

	targetGK := schema.GroupKind{
		Group: targetGV.Group,
		Kind:  hpa.Spec.ScaleTargetRef.Kind,
	}
```
- 从hpa中解析scale对象，设置condition
```go
	scale, targetGR, err := a.scaleForResourceMappings(hpa.Namespace, hpa.Spec.ScaleTargetRef.Name, mappings)
	if err != nil {
		a.eventRecorder.Event(hpa, v1.EventTypeWarning, "FailedGetScale", err.Error())
		setCondition(hpa, autoscalingv2.AbleToScale, v1.ConditionFalse, "FailedGetScale", "the HPA controller was unable to get the target's current scale: %v", err)
		a.updateStatusIfNeeded(hpaStatusOriginal, hpa)
		return fmt.Errorf("failed to query scale subresource for %s: %v", reference, err)
	}
	setCondition(hpa, autoscalingv2.AbleToScale, v1.ConditionTrue, "SucceededGetScale", "the HPA controller was able to get the target's current scale")
	currentReplicas := scale.Spec.Replicas
	a.recordInitialRecommendation(currentReplicas, key)

```
- 这里准备了几个重要的对象
```go
	var (
		metricStatuses        []autoscalingv2.MetricStatus // 代表pod的指标
		metricDesiredReplicas int32 // 指标给出期望的副本数
		metricName            string // 指标的名字
	)

	desiredReplicas := int32(0)  // 真实期望的副本数
	rescaleReason := ""  

	var minReplicas int32 // 最新副本数
```
- 然后就是几个判断条件，条件01  Autoscaling关闭
```go
if scale.Spec.Replicas == 0 && minReplicas != 0 {
		// Autoscaling is disabled for this resource
		desiredReplicas = 0
		rescale = false
		setCondition(hpa, autoscalingv2.ScalingActive, v1.ConditionFalse, "ScalingDisabled", "scaling is disabled since the replica count of the target is zero")
	
```
- 当前副本数大于上限或者小于下限，那么调整desiredReplicas
```go
	} else if currentReplicas > hpa.Spec.MaxReplicas {
		rescaleReason = "Current number of replicas above Spec.MaxReplicas"
		desiredReplicas = hpa.Spec.MaxReplicas
	} else if currentReplicas < minReplicas {
		rescaleReason = "Current number of replicas below Spec.MinReplicas"
		desiredReplicas = minReplicas
```
- 走到else这里说明 当前副本数 处在上限和下限之间，比如 当前5个 下限1个 上限10个
- 首先是调用 computeReplicasForMetrics根据 pod的metrics负载计算目标副本数
```go
	} else {
		var metricTimestamp time.Time
		metricDesiredReplicas, metricName, metricStatuses, metricTimestamp, err = a.computeReplicasForMetrics(hpa, scale, hpa.Spec.Metrics)
		if err != nil {
			a.setCurrentReplicasInStatus(hpa, currentReplicas)
			if err := a.updateStatusIfNeeded(hpaStatusOriginal, hpa); err != nil {
				utilruntime.HandleError(err)
			}
			a.eventRecorder.Event(hpa, v1.EventTypeWarning, "FailedComputeMetricsReplicas", err.Error())
			return fmt.Errorf("failed to compute desired number of replicas based on listed metrics for %s: %v", reference, err)
		}
```
- 使用metricDesiredReplicas调整desiredReplicas
```go
		rescaleMetric := ""
		if metricDesiredReplicas > desiredReplicas {
			desiredReplicas = metricDesiredReplicas
			rescaleMetric = metricName
		}
```
- 确定rescaleReason，根据 desiredReplicas和currentReplicas判断是否要做scale
```go
		if desiredReplicas > currentReplicas {
			rescaleReason = fmt.Sprintf("%s above target", rescaleMetric)
		}
		if desiredReplicas < currentReplicas {
			rescaleReason = "All metrics below target"
		}
		if hpa.Spec.Behavior == nil {
			desiredReplicas = a.normalizeDesiredReplicas(hpa, key, currentReplicas, desiredReplicas, minReplicas)
		} else {
			desiredReplicas = a.normalizeDesiredReplicasWithBehaviors(hpa, key, currentReplicas, desiredReplicas, minReplicas)
		}
		rescale = desiredReplicas != currentReplicas
```
- 最后就是根据rescale标志位判断是否要执行Scale，底层调用scaleNamespacer.Scales
```go
	if rescale {
		scale.Spec.Replicas = desiredReplicas
		_, err = a.scaleNamespacer.Scales(hpa.Namespace).Update(context.TODO(), targetGR, scale, metav1.UpdateOptions{})
	} else {
		klog.V(4).Infof("decided not to scale %s to %v (last scale time was %s)", reference, desiredReplicas, hpa.Status.LastScaleTime)
		desiredReplicas = currentReplicas
	}

```
- 然后我们看下scale底层调用了哪些，追踪这个Update方法，可以看到，位置 D:\go_path\src\github.com\kubernetes\kubernetes\staging\src\k8s.io\client-go\scale\client.go
- 具体逻辑就是找到这个资源，生成它的子资源scale的updateByte，调用kube-client执行put方法
```go
func (c *namespacedScaleClient) Update(ctx context.Context, resource schema.GroupResource, scale *autoscaling.Scale, opts metav1.UpdateOptions) (*autoscaling.Scale, error) {
	path, gvr, err := c.client.pathAndVersionFor(resource)
	if err != nil {
		return nil, fmt.Errorf("unable to get client for %s: %v", resource.String(), err)
	}

	// figure out what scale we actually need here
	desiredGVK, err := c.client.scaleKindResolver.ScaleForResource(gvr)
	if err != nil {
		return nil, fmt.Errorf("could not find proper group-version for scale subresource of %s: %v", gvr.String(), err)
	}

	// convert this to whatever this endpoint wants
	scaleUpdate, err := scaleConverter.ConvertToVersion(scale, desiredGVK.GroupVersion())
	if err != nil {
		return nil, fmt.Errorf("could not convert scale update to external Scale: %v", err)
	}
	encoder := scaleConverter.codecs.LegacyCodec(desiredGVK.GroupVersion())
	scaleUpdateBytes, err := runtime.Encode(encoder, scaleUpdate)
	if err != nil {
		return nil, fmt.Errorf("could not encode scale update to external Scale: %v", err)
	}

	result := c.client.clientBase.Put().
		AbsPath(path).
		NamespaceIfScoped(c.namespace, c.namespace != "").
		Resource(gvr.Resource).
		Name(scale.Name).
		SubResource("scale").
		SpecificallyVersionedParams(&opts, dynamicParameterCodec, versionV1).
		Body(scaleUpdateBytes).
		Do(ctx)
	if err := result.Error(); err != nil {
		// propagate "raw" error from the API
		// this allows callers to interpret underlying Reason field
		// for example: errors.IsConflict(err)
		return nil, err
	}

	return convertToScale(&result)
}
```

# 重点来看下如何根据metrics计算 scale的数量  computeReplicasForMetrics
- 位置 D:\go_path\src\github.com\kubernetes\kubernetes\pkg\controller\podautoscaler\horizontal.go
- computeReplicasForMetrics 的代码逻辑可以简化为如下
```go
func (a *HorizontalController) computeReplicasForMetrics(hpa *autoscalingv2.HorizontalPodAutoscaler, scale *autoscalingv1.Scale,
	metricSpecs []autoscalingv2.MetricSpec) (replicas int32, metric string, statuses []autoscalingv2.MetricStatus, timestamp time.Time, err error) {
    
	for i, metricSpec := range metricSpecs {
		replicaCountProposal, metricNameProposal, timestampProposal, condition, err := a.computeReplicasForMetric(hpa, metricSpec, specReplicas, statusReplicas, selector, &statuses[i])

		if err == nil && (replicas == 0 || replicaCountProposal > replicas) {
			timestamp = timestampProposal
			replicas = replicaCountProposal
			metric = metricNameProposal
		}
	}

}
```
- 也就是会调用 computeReplicasForMetric进行计算
- 在computeReplicasForMetric可以清晰的看到是根据hpa配置的 ObjectMetricSourceType 进行不同的计算
- 首先解释一下这几种类型
    -  "Resource" 代表pod 的cpu和内存的资源 ，使用metrics-server获取
    -  "Pods" 代表pod 的自定义指标 ，使用prometheus-adapter获取
    -  "ContainerResource" 代表容器 的自定义指标 ，使用prometheus-adapter获取
    -  "Object" 代表 k8s对象
    -  "External" 代表 非k8s对象
    
## 01 Resource对应的computeStatusForResourceMetric解析
- 发现又调用了computeStatusForResourceMetricGeneric
```go
// computeStatusForResourceMetric computes the desired number of replicas for the specified metric of type ResourceMetricSourceType.
func (a *HorizontalController) computeStatusForResourceMetric(currentReplicas int32, metricSpec autoscalingv2.MetricSpec, hpa *autoscalingv2.HorizontalPodAutoscaler,
	selector labels.Selector, status *autoscalingv2.MetricStatus) (replicaCountProposal int32, timestampProposal time.Time,
	metricNameProposal string, condition autoscalingv2.HorizontalPodAutoscalerCondition, err error) {
	replicaCountProposal, metricValueStatus, timestampProposal, metricNameProposal, condition, err := a.computeStatusForResourceMetricGeneric(currentReplicas, metricSpec.Resource.Target, metricSpec.Resource.Name, hpa.Namespace, "", selector)
	if err != nil {
		condition = a.getUnableComputeReplicaCountCondition(hpa, "FailedGetResourceMetric", err)
		return replicaCountProposal, timestampProposal, metricNameProposal, condition, err
	}
	*status = autoscalingv2.MetricStatus{
		Type: autoscalingv2.ResourceMetricSourceType,
		Resource: &autoscalingv2.ResourceMetricStatus{
			Name:    metricSpec.Resource.Name,
			Current: *metricValueStatus,
		},
	}
	return replicaCountProposal, timestampProposal, metricNameProposal, condition, nil
}

```
- 在 computeStatusForResourceMetricGeneric中发现首先是对AverageValue值的判断，
```go
func (a *HorizontalController) computeStatusForResourceMetricGeneric(currentReplicas int32, target autoscalingv2.MetricTarget,
	resourceName v1.ResourceName, namespace string, container string, selector labels.Selector) (replicaCountProposal int32,
	metricStatus *autoscalingv2.MetricValueStatus, timestampProposal time.Time, metricNameProposal string,
	condition autoscalingv2.HorizontalPodAutoscalerCondition, err error) {
	if target.AverageValue != nil {
		var rawProposal int64
		replicaCountProposal, rawProposal, timestampProposal, err := a.replicaCalc.GetRawResourceReplicas(currentReplicas, target.AverageValue.MilliValue(), resourceName, namespace, selector, container)
		if err != nil {
			return 0, nil, time.Time{}, "", condition, fmt.Errorf("failed to get %s utilization: %v", resourceName, err)
		}
		metricNameProposal = fmt.Sprintf("%s resource", resourceName.String())
		status := autoscalingv2.MetricValueStatus{
			AverageValue: resource.NewMilliQuantity(rawProposal, resource.DecimalSI),
		}
		return replicaCountProposal, &status, timestampProposal, metricNameProposal, autoscalingv2.HorizontalPodAutoscalerCondition{}, nil
	}
```
### 底层调用GetRawResourceReplicas 
- 追踪后发现首先是获取metrics然后是计算
```go
// GetRawResourceReplicas calculates the desired replica count based on a target resource utilization (as a raw milli-value)
// for pods matching the given selector in the given namespace, and the current replica count
func (c *ReplicaCalculator) GetRawResourceReplicas(currentReplicas int32, targetUtilization int64, resource v1.ResourceName, namespace string, selector labels.Selector, container string) (replicaCount int32, utilization int64, timestamp time.Time, err error) {
	metrics, timestamp, err := c.metricsClient.GetResourceMetric(resource, namespace, selector, container)
	if err != nil {
		return 0, 0, time.Time{}, fmt.Errorf("unable to get metrics for resource %s: %v", resource, err)
	}

	replicaCount, utilization, err = c.calcPlainMetricReplicas(metrics, currentReplicas, targetUtilization, namespace, selector, resource)
	return replicaCount, utilization, timestamp, err
}
```
- 追踪获取metrics的过程也就是 c.metricsClient.GetResourceMetric，其中最重要的就是c.client.PodMetricses(namespace).List
```go
func (c *resourceMetricsClient) GetResourceMetric(resource v1.ResourceName, namespace string, selector labels.Selector, container string) (PodMetricsInfo, time.Time, error) {
	metrics, err := c.client.PodMetricses(namespace).List(context.TODO(), metav1.ListOptions{LabelSelector: selector.String()})
	if err != nil {
		return nil, time.Time{}, fmt.Errorf("unable to fetch metrics from resource metrics API: %v", err)
	}
```
- list底层使用rest-client访问 /metrics.k8s.io/v1beta1的 api，底层调用metrics-server的接口获取 这个ns下这个pod的数据 
- 位置D:\go_path\src\github.com\kubernetes\kubernetes\staging\src\k8s.io\metrics\pkg\client\clientset\versioned\typed\metrics\v1beta1\podmetrics.go

```go
// List takes label and field selectors, and returns the list of PodMetricses that match those selectors.
func (c *podMetricses) List(ctx context.Context, opts v1.ListOptions) (result *v1beta1.PodMetricsList, err error) {
	var timeout time.Duration
	if opts.TimeoutSeconds != nil {
		timeout = time.Duration(*opts.TimeoutSeconds) * time.Second
	}
	result = &v1beta1.PodMetricsList{}
	err = c.client.Get().
		Namespace(c.ns).
		Resource("pods").
		VersionedParams(&opts, scheme.ParameterCodec).
		Timeout(timeout).
		Do(ctx).
		Into(result)
	return
}
```
- 然后回到GetRawResourceReplicas 的计算逻辑，也就是calcPlainMetricReplicas


#### calcPlainMetricReplicas计算的逻辑
- 首先从pod-informer本地数据拿到相关的podList
```go
// calcPlainMetricReplicas calculates the desired replicas for plain (i.e. non-utilization percentage) metrics.
func (c *ReplicaCalculator) calcPlainMetricReplicas(metrics metricsclient.PodMetricsInfo, currentReplicas int32, targetUtilization int64, namespace string, selector labels.Selector, resource v1.ResourceName) (replicaCount int32, utilization int64, err error) {

	podList, err := c.podLister.Pods(namespace).List(selector)
	if err != nil {
		return 0, 0, fmt.Errorf("unable to get pods while calculating replica count: %v", err)
	}

	if len(podList) == 0 {
		return 0, 0, fmt.Errorf("no pods returned by selector while calculating replica count")
	}
```
- 然后根据本地的podList和拿到的pod-metrics的map进行对比，找出readyPodCount, unreadyPods, missingPods, ignoredPods
```go
	readyPodCount, unreadyPods, missingPods, ignoredPods := groupPods(podList, metrics, resource, c.cpuInitializationPeriod, c.delayOfInitialReadinessStatus)

```
- 其中 
    - missingPods代表本地有，pod-metrics map 中没有
    - unreadyPods代表 pending的pod
    - ignoredPods代表PodFailed或者被删除了
    - readyPodCount代表正常的
- groupPods部分代码如下
```go
func groupPods(pods []*v1.Pod, metrics metricsclient.PodMetricsInfo, resource v1.ResourceName, cpuInitializationPeriod, delayOfInitialReadinessStatus time.Duration) (readyPodCount int, unreadyPods, missingPods, ignoredPods sets.String) {
	missingPods = sets.NewString()
	unreadyPods = sets.NewString()
	ignoredPods = sets.NewString()
	for _, pod := range pods {
		if pod.DeletionTimestamp != nil || pod.Status.Phase == v1.PodFailed {
			ignoredPods.Insert(pod.Name)
			continue
		}
		// Pending pods are ignored.
		if pod.Status.Phase == v1.PodPending {
			unreadyPods.Insert(pod.Name)
			continue
		}
```
- 然后从pod-metrics map 删掉ignoredPods和unreadyPods中的
```go
	removeMetricsForPods(metrics, ignoredPods)
	removeMetricsForPods(metrics, unreadyPods)

	if len(metrics) == 0 {
		return 0, 0, fmt.Errorf("did not receive metrics for any ready pods")
	}
```
- 计算平均利用率和平均值
    - utilization= metricsTotal除以个数就是平均值
    - usageRatio = 平均值除以目标平均值就是利用率
- GetMetricUtilizationRatio计算代码如下
```go
	usageRatio, utilization := metricsclient.GetMetricUtilizationRatio(metrics, targetUtilization)
func GetMetricUtilizationRatio(metrics PodMetricsInfo, targetUtilization int64) (utilizationRatio float64, currentUtilization int64) {
	metricsTotal := int64(0)
	for _, metric := range metrics {
		metricsTotal += metric.Value
	}

	currentUtilization = metricsTotal / int64(len(metrics))

	return float64(currentUtilization) / float64(targetUtilization), currentUtilization
}

```
- 下面就是一堆判断条件了，首先就是没有missing pods的case
```go
	rebalanceIgnored := len(unreadyPods) > 0 && usageRatio > 1.0

	if !rebalanceIgnored && len(missingPods) == 0 {
		if math.Abs(1.0-usageRatio) <= c.tolerance {
			// return the current replicas if the change would be too small
			return currentReplicas, utilization, nil
		}

		// if we don't have any unready or missing pods, we can calculate the new replica count now
		return int32(math.Ceil(usageRatio * float64(readyPodCount))), utilization, nil
	}
```
- 然后是有missingPods的情况，需要根据usageRatio 区别对待： 
    - usageRatio小于1 代表使用率低于目标值，需要缩容，把missing pods的利用率设置为100% 资源情况
    - usageRatio大于1 代表使用率高于目标值，需要扩容，把missing pods的利用率设置为0% 资源情况
- 判断如下
```go
	if len(missingPods) > 0 {
		if usageRatio < 1.0 {
			// on a scale-down, treat missing pods as using 100% of the resource request
			for podName := range missingPods {
				metrics[podName] = metricsclient.PodMetric{Value: targetUtilization}
			}
		} else {
			// on a scale-up, treat missing pods as using 0% of the resource request
			for podName := range missingPods {
				metrics[podName] = metricsclient.PodMetric{Value: 0}
			}
		}
	}
```
- 扩容情况下把unreadyPods的利用率设置为0 
```go
	if rebalanceIgnored {
		// on a scale-up, treat unready pods as using 0% of the resource request
		for podName := range unreadyPods {
			metrics[podName] = metricsclient.PodMetric{Value: 0}
		}
	}
```
- 重新计算利用率，因为上面把missing 和unready的pod 的利用率重新设置了
- 如果变化较小，那么返回当前副本数，即不需要scale
```go
	// re-run the utilization calculation with our new numbers
	newUsageRatio, _ := metricsclient.GetMetricUtilizationRatio(metrics, targetUtilization)

	if math.Abs(1.0-newUsageRatio) <= c.tolerance || (usageRatio < 1.0 && newUsageRatio > 1.0) || (usageRatio > 1.0 && newUsageRatio < 1.0) {
		// return the current replicas if the change would be too small,
		// or if the new usage ratio would cause a change in scale direction
		return currentReplicas, utilization, nil
	}

```
- 最终使用新利用率乘以metric的个数
```go
newReplicas := int32(math.Ceil(newUsageRatio * float64(len(metrics))))
```

###  回到 computeStatusForResourceMetricGeneric中处理 targetAverageUtilization 代表百分比的分支
```go
// D:\go_path\src\github.com\kubernetes\kubernetes\pkg\controller\podautoscaler\horizontal.go
replicaCountProposal, percentageProposal, rawProposal, timestampProposal, err := a.replicaCalc.GetResourceReplicas(currentReplicas, targetUtilization, resourceName, namespace, selector, container)

 
```
- 可以看到调用GetResourceReplicas计算，底层的计算逻辑是GetResourceUtilizationRatio
- 从传参可以看到依赖容器配置的request ，这很好理解：因为targetAverageUtilization 代表百分比
    - 所以需要先用metricsTotal真实使用量 除以requestsTotal请求量代表使用百分比
    - 然后再和设置的目标百分比进行对比
- GetResourceUtilizationRatio代码如下
```go
// D:\go_path\src\github.com\kubernetes\kubernetes\pkg\controller\podautoscaler\metrics\utilization.go
func GetResourceUtilizationRatio(metrics PodMetricsInfo, requests map[string]int64, targetUtilization int32) (utilizationRatio float64, currentUtilization int32, rawAverageValue int64, err error) {
	metricsTotal := int64(0)
	requestsTotal := int64(0)
	numEntries := 0

	for podName, metric := range metrics {
		request, hasRequest := requests[podName]
		if !hasRequest {
			// we check for missing requests elsewhere, so assuming missing requests == extraneous metrics
			continue
		}

		metricsTotal += metric.Value
		requestsTotal += request
		numEntries++
	}

	// if the set of requests is completely disjoint from the set of metrics,
	// then we could have an issue where the requests total is zero
	if requestsTotal == 0 {
		return 0, 0, 0, fmt.Errorf("no metrics returned matched known pods")
	}

	currentUtilization = int32((metricsTotal * 100) / requestsTotal)

	return float64(currentUtilization) / float64(targetUtilization), currentUtilization, metricsTotal / int64(numEntries), nil
}
```

# 本节重点总结
- 根据不同ResourceType调用不同的计算方法
    - 但相同的流程为，先从远端的metrics提供方获取pod的指标使用情况
    - 然后根据配置的阈值进行判断
    - 比如类型为resource，底层使用rest-client访问 /metrics.k8s.io/v1beta1的 api，底层调用metrics-server的接口获取 这个ns下这个pod的数据 



- hpa中的metric 判断阈值类型
    - 如果是targetAverageUtilization 代表百分比：
        - metrics使用值除以pod的request值获取使用百分比，然后和目标百分比相除，大于1说明要扩容，小于1要缩容 
    - 如果是targetAverageValue 代表值：
        - metrics使用值除以目标值，大于1说明要扩容，小于1要缩容 