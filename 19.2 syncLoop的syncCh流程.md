# 本节重点总结

- syncCh定时器管道，每隔一秒去同步最新保存的 Pod 状态；
- 从源码上看syncCh处理podWorkers的managePodLoop 循环中需要resync的任务

  - 对应就是调用completeWork方法的任务
- HandlePodSyncs逻辑就是遍历传入待sync的pod ，先获取mirrorPod 再调用dispatchWork
- dispatchWork底层调用 managePodLoop中会对update.WorkType 进行判断，对应podWorker中定义的状态

  - SyncPodWork 代表pod应该要start和running，会调用syncTerminatingPodFn回调处理
  - TerminatingPodWork 代表pod应该停止，但是其中还有容器在运行，即将被销毁，会调用syncTerminatingPodFn回调处理
  - TerminatedPodWork代表pod已经终止，容器也停止了，可以进行清理操作了会调用syncTerminatedPodFn回调处理

# syncCh的作用

- syncCh定时器管道，每隔一秒去同步最新保存的 Pod 状态；

# 入口位置

- 在syncLoop中会新建syncTicker，然后传入syncLoopIteration中，位置D:\go_path\src\github.com\kubernetes\kubernetes\pkg\kubelet\kubelet.go

```go
	syncTicker := time.NewTicker(time.Second)
	defer syncTicker.Stop()
```

## syncLoopIteration的逻辑也比较简单

- 就是getPodsToSync获取待同步的pods，然后调用HandlePodSyncs

```go
	case <-syncCh:
		// Sync pods waiting for sync
		podsToSync := kl.getPodsToSync()
		if len(podsToSync) == 0 {
			break
		}
		klog.V(4).InfoS("SyncLoop (SYNC) pods", "total", len(podsToSync), "pods", format.Pods(podsToSync))
		handler.HandlePodSyncs(podsToSync)
```

### getPodsToSync逻辑

- 从注释可以得知有两种情况需要sync
  - pod的work 已经ready了
  - 内部的模块请求了sync

```go
//   * pod whose work is ready.
//   * internal modules that request sync of a pod.
```

- 从podManager获取属于这个kubelet的allPods
- 从工作队列中获取所有ready的 项目

```go
	allPods := kl.podManager.GetPods()
	podUIDs := kl.workQueue.GetWork()
	podUIDSet := sets.NewString()
	for _, podUID := range podUIDs {
		podUIDSet.Insert(string(podUID))
	}
```

- 遍历allPods 如果pod在podUIDSet中也就是pod的work 已经准备好了，那么需要添加到podsToSync中

```go
	var podsToSync []*v1.Pod
	for _, pod := range allPods {
		if podUIDSet.Has(string(pod.UID)) {
			// The work of the pod is ready
			podsToSync = append(podsToSync, pod)
			continue
		}
```

> 那么这里就要看一下到底什么算是workQueue ready的项目

- 查看GetWork方法，发现就是把queue中的item全部拿出来

```go
func (q *basicWorkQueue) GetWork() []types.UID {
	q.lock.Lock()
	defer q.lock.Unlock()
	now := q.clock.Now()
	var items []types.UID
	for k, v := range q.queue {
		if v.Before(now) {
			items = append(items, k)
			delete(q.queue, k)
		}
	}
	return items
}

```

> 再次观察这个queue中的入队操作

- Enqueue动作就是向queue 这个map中添加元素

```go

func (q *basicWorkQueue) Enqueue(item types.UID, delay time.Duration) {
	q.lock.Lock()
	defer q.lock.Unlock()
	q.queue[item] = q.clock.Now().Add(delay)
}

```

- 调用Enqueue的是completeWork，completeWork不断向WorkQueue 插入重试的pod或者下一次sync的

```go
// completeWork requeues on error or the next sync interval and then immediately executes any pending
// work.
func (p *podWorkers) completeWork(pod *v1.Pod, syncErr error) {
	// Requeue the last update if the last sync returned error.
	switch {
	case syncErr == nil:
		// No error; requeue at the regular resync interval.
		p.workQueue.Enqueue(pod.UID, wait.Jitter(p.resyncInterval, workerResyncIntervalJitterFactor))
	case strings.Contains(syncErr.Error(), NetworkNotReadyErrorMsg):
		// Network is not ready; back off for short period of time and retry as network might be ready soon.
		p.workQueue.Enqueue(pod.UID, wait.Jitter(backOffOnTransientErrorPeriod, workerBackOffPeriodJitterFactor))
	default:
		// Error occurred during the sync; back off and then retry.
		p.workQueue.Enqueue(pod.UID, wait.Jitter(p.backOffPeriod, workerBackOffPeriodJitterFactor))
	}
	p.completeWorkQueueNext(pod.UID)
}

```

> 所以getPodsToSync的第一层就是处理completeWork插入的任务

- 这里还有个时间的问题，也就是调用getPodsToSync时WorkQueue 已经到时间的任务
  - 因为上面可以看到 completeWork插入时是带有一定的时间偏移的

> 回到getPodsToSync的第二个条件

- 是遍历PodSyncLoopHandlers 判断podSyncLoopHandler.ShouldSync是否已经触发

```go
		for _, podSyncLoopHandler := range kl.PodSyncLoopHandlers {
			if podSyncLoopHandler.ShouldSync(pod) {
				podsToSync = append(podsToSync, pod)
				break
			}
		}
```

- 追踪这个ShouldSync方法可以发现只有activeDeadlineHandler实现了

```go

// ShouldSync returns true if the pod is past its active deadline.
func (m *activeDeadlineHandler) ShouldSync(pod *v1.Pod) bool {
	return m.pastActiveDeadline(pod)
}
```

- pastActiveDeadline如下 :就是判断pod存活时间大于配置的ActiveDeadlineSeconds，
  - [ActiveDeadlineSeconds](https://kubernetes.io/zh/docs/concepts/workloads/controllers/job/#job-%E7%BB%88%E6%AD%A2%E4%B8%8E%E6%B8%85%E7%90%86) 的含义是至多运行多少秒

```go

// pastActiveDeadline returns true if the pod has been active for more than its ActiveDeadlineSeconds
func (m *activeDeadlineHandler) pastActiveDeadline(pod *v1.Pod) bool {
	// no active deadline was specified
	if pod.Spec.ActiveDeadlineSeconds == nil {
		return false
	}
	// get the latest status to determine if it was started
	podStatus, ok := m.podStatusProvider.GetPodStatus(pod.UID)
	if !ok {
		podStatus = pod.Status
	}
	// we have no start time so just return
	if podStatus.StartTime.IsZero() {
		return false
	}
	// determine if the deadline was exceeded
	start := podStatus.StartTime.Time
	duration := m.clock.Since(start)
	allowedDuration := time.Duration(*pod.Spec.ActiveDeadlineSeconds) * time.Second
	return duration >= allowedDuration
}

```

### HandlePodSyncs逻辑

- 逻辑就是遍历传入待sync的pod ，先获取mirrorPod 再调用dispatchWork
- 传入的kubetypes为SyncPodSync

```go
// HandlePodSyncs is the callback in the syncHandler interface for pods
// that should be dispatched to pod workers for sync.
func (kl *Kubelet) HandlePodSyncs(pods []*v1.Pod) {
	start := kl.clock.Now()
	for _, pod := range pods {
		mirrorPod, _ := kl.podManager.GetMirrorPodByPod(pod)
		kl.dispatchWork(pod, kubetypes.SyncPodSync, mirrorPod, start)
	}
}

```

- dispatchWork的逻辑就是拼接UpdatePod请求

```go
func (kl *Kubelet) dispatchWork(pod *v1.Pod, syncType kubetypes.SyncPodType, mirrorPod *v1.Pod, start time.Time) {
	// Run the sync in an async worker.
	kl.podWorkers.UpdatePod(UpdatePodOptions{
		Pod:        pod,
		MirrorPod:  mirrorPod,
		UpdateType: syncType,
		StartTime:  start,
	})
	// Note the number of containers for new pods.
	if syncType == kubetypes.SyncPodCreate {
		metrics.ContainersPerPodCount.Observe(float64(len(pod.Spec.Containers)))
	}
}

```

### UpdatePod逻辑

- UpdatePod 逻辑比较复杂，我们这里只关心sync的逻辑
- 可以看到启动了managePodLoop处理podUpdates chan，同时在外部传入了work

```go
		go func() {
			defer runtime.HandleCrash()
			p.managePodLoop(podUpdates)
		}()
	}

	// dispatch a request to the pod worker if none are running
	if !status.IsWorking() {
		status.working = true
		podUpdates <- work
		return
	}
```

- 而在 managePodLoop中会对update.WorkType 进行判断，对应podWorker中定义的状态
  - SyncPodWork 代表pod应该要start和running，会调用syncPod回调处理
  - TerminatingPodWork 代表pod应该停止，但是其中还有容器在运行，即将被销毁，会调用syncTerminatingPodFn回调处理
  - TerminatedPodWork代表pod已经终止，容器也停止了，可以进行清理操作了会调用syncTerminatedPodFn回调处理
  - ```go
    			switch {
    			case update.WorkType == TerminatedPodWork:
    				err = p.syncTerminatedPodFn(ctx, pod, status)

    			case update.WorkType == TerminatingPodWork:
    				var gracePeriod *int64
    				if opt := update.Options.KillPodOptions; opt != nil {
    					gracePeriod = opt.PodTerminationGracePeriodSecondsOverride
    				}
    				podStatusFn := p.acknowledgeTerminating(pod)

    				err = p.syncTerminatingPodFn(ctx, pod, status, update.Options.RunningPod, gracePeriod, podStatusFn)

    			default:
    				err = p.syncPodFn(ctx, update.Options.UpdateType, pod, update.Options.MirrorPod, status)
    			}
    ```

### 追踪syncPodFn

- 对应的就是Kubelet的syncPod方法，内部会为pod做一系列准备工作
  - 为pod创建数据目录（如果不存在）
  - 等待卷连接/装载
  - 拉取pod的secret
  - 调用容器运行时的SyncPod回调
- 位置 D:\go_path\src\github.com\kubernetes\kubernetes\pkg\kubelet\kubelet.go

# syncLoop的plegCh流程 在12.1有讲解

# 本节重点总结

- syncCh定时器管道，每隔一秒去同步最新保存的 Pod 状态；
- 从源码上看syncCh处理podWorkers的managePodLoop 循环中需要resync的任务

  - 对应就是调用completeWork方法的任务
- HandlePodSyncs逻辑就是遍历传入待sync的pod ，先获取mirrorPod 再调用dispatchWork
- dispatchWork底层调用 managePodLoop中会对update.WorkType 进行判断，对应podWorker中定义的状态

  - SyncPodWork 代表pod应该要start和running，会调用syncPod回调处理
  - TerminatingPodWork 代表pod应该停止，但是其中还有容器在运行，即将被销毁，会调用syncTerminatingPodFn回调处理
  - TerminatedPodWork代表pod已经终止，容器也停止了，可以进行清理操作了会调用syncTerminatedPodFn回调处理