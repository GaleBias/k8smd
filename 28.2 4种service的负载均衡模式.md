# 本节重点总结

- 01 userspace 模式 转发由kube-proxy做
- 02 iptables 模式 转发由iptables做
- 03 ipvs 模式 转发由ipvs做
- 04 kernelspace 主要是在 windows 下使用的

> ipvs比iptables的优点

- ipvs 和iptables的实现都基于 netfilter 的钩子函数，
  - iptables使用链表，对路由规则的增删改查都要涉及遍历一次链表
  - ipvs使用哈希表作为底层的数据结构并且工作在内核态，速度快

# service 的负载均衡

- service 实际的路由转发都是由 kube-proxy 组件来实现的，service 仅以一种 VIP（ClusterIP） 的形式存在
- kube-proxy 主要实现了集群内部从 pod 到 service 和集群外部从 nodePort 到 service 的访问
- kube-proxy 的路由转发规则是通过其后端的代理模块实现的
- kube-proxy 的代理模块目前有四种实现方案，userspace、iptables、ipvs、kernelspace，其发展历程如下所示：
  - kubernetes v1.0：services 仅是一个“4层”代理，代理模块只有 userspace
  - kubernetes v1.1：Ingress API 出现，其代理“7层”服务，并且增加了 iptables 代理模块
  - kubernetes v1.2：iptables 成为默认代理模式
  - kubernetes v1.8：引入 ipvs 代理模块
  - kubernetes v1.9：ipvs 代理模块成为 beta 版本
  - kubernetes v1.11：ipvs 代理模式 GA

## 01 userspace 模式 转发由kube-proxy做

![svc_userspace.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/908/1636075973000/5174320bb0054eaab95274a251d87384.png)

- 在 userspace 模式下，访问服务的请求到达节点后首先进入内核 iptables，然后回到用户空间，由 kube-proxy 转发到后端的 pod
  - 这样流量从用户空间进出内核带来的性能损耗是不可接受的，所以也就有了 iptables 模式。

> 为什么 userspace 模式要建立 iptables 规则?

- 因为 kube-proxy 监听的端口在用户空间，这个端口不是服务的访问端口也不是服务的 nodePort
- 因此需要一层 iptables 把访问服务的连接重定向给 kube-proxy 服务。

## 02 iptables 模式 转发由iptables做

![svc_iptables.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/908/1636075973000/2f9ec6867b2543c09b55ebd583b5e777.png)

- iptables 模式是目前默认的代理方式，基于 netfilter 实现。当客户端请求 service 的 ClusterIP 时，根据 iptables 规则路由到各 pod 上，iptables 使用 DNAT 来完成转发，其采用了随机数实现负载均衡。

> iptables 模式与 userspace 模式区别

- 最大的区别在于iptables 模块使用 DNAT 模块实现了 service 入口地址到 pod 实际地址的转换，免去了一次内核态到用户态的切换
  - 使用 iptables 处理流量具有较低的系统开销，因为流量由 Linux netfilter 处理， 而无需在用户空间和内核空间之间切换。 这种方法也可能更可靠
- 另一个与 userspace 代理模式不同的是，如果 iptables 代理最初选择的那个 pod 没有响应，它不会自动重试其他 pod。

> 问题

- iptables 模式最主要的问题是在 service 数量大的时候会产生太多的 iptables 规则，使用非增量式更新会引入一定的时延，大规模情况下有明显的性能问题。

## 03 ipvs 模式 转发由ipvs做

> iptables 问题

- 当集群规模比较大时，iptables 规则刷新会非常慢，难以支持大规模集群，因其底层路由表的实现是链表，对路由规则的增删改查都要涉及遍历一次链表，ipvs 的问世正是解决此问题的
- ipvs 是 LVS 的负载均衡模块，与 iptables 比较像的是，ipvs 的实现虽然也基于 netfilter 的钩子函数，但是它却使用哈希表作为底层的数据结构并且工作在内核态，也就是说 ipvs 在重定向流量和同步代理规则有着更好的性能，几乎允许无限的规模扩张。

> ipvs在转发svc流量时采用NAT模式

- ipvs 支持三种负载均衡模式：DR模式（Direct Routing）、NAT 模式（Network Address Translation）、Tunneling（也称 ipip 模式）
- 三种模式中只有 NAT 支持端口映射，所以 ipvs 使用 NAT 模式。linux 内核原生的 ipvs 只支持 DNAT
- 当在数据包过滤，SNAT 和支持 NodePort 类型的服务这几个场景中ipvs 还是会使用 iptables。

> ipvs支持更多的负载均衡算法

- userspace、iptables、ipvs 三种模式中默认的负载均衡策略都是通过 round-robin 算法来选择后端 pod 的
- ipvs 也支持更多的负载均衡算法
  - rr：round-robin/轮询
  - lc：least connection/最少连接
  - dh：destination hashing/目标哈希
  - sh：source hashing/源哈希
  - sed：shortest expected delay/预计延迟时间最短
  - nq：never queue/从不排队

## 04 kernelspace 主要是在 windows 下使用的

# 本节重点总结

- 01 userspace 模式 转发由kube-proxy做
- 02 iptables 模式 转发由iptables做
- 03 ipvs 模式 转发由ipvs做
- 04 kernelspace 主要是在 windows 下使用的

> ipvs比iptables的优点

- ipvs 和iptables的实现都基于 netfilter 的钩子函数，
  - iptables使用链表，对路由规则的增删改查都要涉及遍历一次链表
  - ipvs使用哈希表作为底层的数据结构并且工作在内核态，速度快