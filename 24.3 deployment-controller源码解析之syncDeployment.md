# 本节重点总结
- 从代码上看 DeploymentController的启动也是两步走
  - 第一步就是deployment.NewDeploymentController 初始化DeploymentController控制器对象 dc
  - 第二步就是调用dc的Run方法运行
  - 这里我们来分析一下dc的Run
  
- 和其它控制器一样，首先会根据3个informer的ListerSynced方法判断本地是否list到数据了，然后并发启动worker执行同步
- syncDeployment内部在真正执行动作前会做一些准备工作
    - 获取dep关联的rsList
    - 根据rsList和dep获取podMap


# 接上回
- 从代码上看 DeploymentController的启动也是两步走
  - 第一步就是deployment.NewDeploymentController 初始化DeploymentController控制器对象 dc
  - 第二步就是调用dc的Run方法运行
  - 这里我们来分析一下dc的Run
  
# Run解析
## 入参解析
```go
go dc.Run(int(ctx.ComponentConfig.DeploymentController.ConcurrentDeploymentSyncs), ctx.Stop)
```
- ConcurrentDeploymentSyncs代表并发同步Deployment的个数，默认是10个
- ctx.Stop代表退出通知的ctx


## Run第一层解析
- 和其它控制器一样，首先会根据3个informer的ListerSynced方法判断本地是否list到数据了
- 然后并发启动worker执行同步
```go
// Run begins watching and syncing.
func (dc *DeploymentController) Run(workers int, stopCh <-chan struct{}) {
	defer utilruntime.HandleCrash()
	defer dc.queue.ShutDown()

	klog.InfoS("Starting controller", "controller", "deployment")
	defer klog.InfoS("Shutting down controller", "controller", "deployment")

	if !cache.WaitForNamedCacheSync("deployment", stopCh, dc.dListerSynced, dc.rsListerSynced, dc.podListerSynced) {
		return
	}

	for i := 0; i < workers; i++ {
		go wait.Until(dc.worker, time.Second, stopCh)
	}

	<-stopCh
}

```

## worker 解析
- worker内部还是不断读取processNextWorkItem的结果
- processNextWorkItem逻辑就是消费queue中的数据，调用syncHandler执行同步
```go
func (dc *DeploymentController) worker() {
	for dc.processNextWorkItem() {
	}
}

func (dc *DeploymentController) processNextWorkItem() bool {
	key, quit := dc.queue.Get()
	if quit {
		return false
	}
	defer dc.queue.Done(key)

	err := dc.syncHandler(key.(string))
	dc.handleErr(err, key)

	return true
}
```
- 如果syncHandler执行同步报错了，那么调用handleErr处理错误
> handleErr流程
- 首先判断错误中是否有因为这个ns要被销毁的错误，如果有那么忽略这个错误
```go
func (dc *DeploymentController) handleErr(err error, key interface{}) {
	if err == nil || errors.HasStatusCause(err, v1.NamespaceTerminatingCause) {
		dc.queue.Forget(key)
		return
	}

```
- 然后判断这个dep的入队次数，即重试的次数。达不到maxRetries就再入队，不过是带限速的入队
```go
	ns, name, keyErr := cache.SplitMetaNamespaceKey(key.(string))
	if keyErr != nil {
		klog.ErrorS(err, "Failed to split meta namespace cache key", "cacheKey", key)
	}

	if dc.queue.NumRequeues(key) < maxRetries {
		klog.V(2).InfoS("Error syncing deployment", "deployment", klog.KRef(ns, name), "err", err)
		dc.queue.AddRateLimited(key)
		return
	}
```
- 如果次数太多就不再重试了
```go
	utilruntime.HandleError(err)
	klog.V(2).InfoS("Dropping deployment out of the queue", "deployment", klog.KRef(ns, name), "err", err)
	dc.queue.Forget(key)
```

## syncHandler 解析
- 从传入的key分隔出ns和name
```go
	namespace, name, err := cache.SplitMetaNamespaceKey(key)
	if err != nil {
		klog.ErrorS(err, "Failed to split meta namespace cache key", "cacheKey", key)
		return err
	}

```
- 记录同步dep的时间
```go
	startTime := time.Now()
	klog.V(4).InfoS("Started syncing deployment", "deployment", klog.KRef(namespace, name), "startTime", startTime)
	defer func() {
		klog.V(4).InfoS("Finished syncing deployment", "deployment", klog.KRef(namespace, name), "duration", time.Since(startTime))
	}()

```
- 根据depinformer 的dLister获取这个ns中这个name的dep，如果没有找到说明这个dep已经被删除了
```go
	deployment, err := dc.dLister.Deployments(namespace).Get(name)
	if errors.IsNotFound(err) {
		klog.V(2).InfoS("Deployment has been deleted", "deployment", klog.KRef(namespace, name))
		return nil
	}
	if err != nil {
		return err
	}

```
- 深拷贝一个对象，然后校验dep配置的标签选择器是否是空的，如果是的话更新dep的 generation
```go
	// Deep-copy otherwise we are mutating our cache.
	// TODO: Deep-copy only when needed.
	d := deployment.DeepCopy()

	everything := metav1.LabelSelector{}
	if reflect.DeepEqual(d.Spec.Selector, &everything) {
		dc.eventRecorder.Eventf(d, v1.EventTypeWarning, "SelectingAll", "This deployment is selecting all pods. A non-empty selector is required.")
		if d.Status.ObservedGeneration < d.Generation {
			d.Status.ObservedGeneration = d.Generation
			dc.client.AppsV1().Deployments(d.Namespace).UpdateStatus(context.TODO(), d, metav1.UpdateOptions{})
		}
		return nil
	}

```

### 获取dep关联的rsList
```go
	rsList, err := dc.getReplicaSetsForDeployment(d)
	if err != nil {
		return err
	}
```
> getReplicaSetsForDeployment解析
- 首先从informer本地存储中获取dep所在ns的rsList
```go
	rsList, err := dc.rsLister.ReplicaSets(d.Namespace).List(labels.Everything())
	if err != nil {
		return nil, err
	}
```
- 解析dep的标签选择器
```go
	deploymentSelector, err := metav1.LabelSelectorAsSelector(d.Spec.Selector)
	if err != nil {
		return nil, fmt.Errorf("deployment %s/%s has invalid label selector: %v", d.Namespace, d.Name, err)
	}
```
- 准备一个canAdoptFunc，内部就是直接从apiserver中获取这个dep对象，和本地缓存中的dep对比UID
```go
	canAdoptFunc := controller.RecheckDeletionTimestamp(func() (metav1.Object, error) {
		fresh, err := dc.client.AppsV1().Deployments(d.Namespace).Get(context.TODO(), d.Name, metav1.GetOptions{})
		if err != nil {
			return nil, err
		}
		if fresh.UID != d.UID {
			return nil, fmt.Errorf("original Deployment %v/%v is gone: got uid %v, wanted %v", d.Namespace, d.Name, fresh.UID, d.UID)
		}
		return fresh, nil
	})
```
- 然后初始化ReplicaSet控制器的属主管理器cm，调用cmClaimReplicaSets 
```go
	cm := controller.NewReplicaSetControllerRefManager(dc.rsControl, d, deploymentSelector, controllerKind, canAdoptFunc)
	return cm.ClaimReplicaSets(rsList)
```

#### ClaimReplicaSets 找到相关的RS
- 和job中类似，首先构造RS认领数组和错误列表 
```go
	var claimed []*apps.ReplicaSet
	var errlist []error


```
- 准备三个方法：
    - match代表匹配 rs和dep的标签
    - adopt代表认领就是我们上面canAdoptFunc，直接从apiserver中获取这个dep对象，和本地缓存中的dep对比UID
    - release代表调用apiserver释放rs
- 三个方法如下
```go
	match := func(obj metav1.Object) bool {
		return m.Selector.Matches(labels.Set(obj.GetLabels()))
	}
	adopt := func(obj metav1.Object) error {
		return m.AdoptReplicaSet(obj.(*apps.ReplicaSet))
	}
	release := func(obj metav1.Object) error {
		return m.ReleaseReplicaSet(obj.(*apps.ReplicaSet))
	}
```
- 然后就是遍历rs数组，调用ClaimObject去依次应用match, adopt, release方法，将认领的结果添加到claimed，错误添加到errlist中
```go
	for _, rs := range sets {
		ok, err := m.ClaimObject(rs, match, adopt, release)
		if err != nil {
			errlist = append(errlist, err)
			continue
		}
		if ok {
			claimed = append(claimed, rs)
		}
	}
	return claimed, utilerrors.NewAggregate(errlist)
```

#### 根据rsList和dep获取podMap

```go
	podMap, err := dc.getPodMapForDeployment(d, rsList)
	if err != nil {
		return err
	}
```
- 从代码注释来看这个podMap有两个作用
    - 检查相关的pod已经被正常打上pod-template-hash标签
    - 检查在重新创建Deployments的中间没有旧的pod运行
```go
	// List all Pods owned by this Deployment, grouped by their ReplicaSet.
	// Current uses of the podMap are:
	//
	// * check if a Pod is labeled correctly with the pod-template-hash label.
	// * check that no old Pods are running in the middle of Recreate Deployments.
```
- 代码如下
- 首先解析标签选择器
```go
func (dc *DeploymentController) getPodMapForDeployment(d *apps.Deployment, rsList []*apps.ReplicaSet) (map[types.UID][]*v1.Pod, error) {
	// Get all Pods that potentially belong to this Deployment.
	selector, err := metav1.LabelSelectorAsSelector(d.Spec.Selector)
	if err != nil {
		return nil, err
	}
```
- 然后在podInformer本地缓存中根据标签选择器获取对应的pods
```go
	pods, err := dc.podLister.Pods(d.Namespace).List(selector)
	if err != nil {
		return nil, err
	}
```
- 新建podMap ，长度和rsList一致，初始化podMap内层的pod数组，key为 rs的UID
```go
	// Group Pods by their controller (if it's in rsList).
	podMap := make(map[types.UID][]*v1.Pod, len(rsList))
	for _, rs := range rsList {
		podMap[rs.UID] = []*v1.Pod{}
	}
```
- 遍历本地缓存获取到的pods，找到他们的属主信息也就是rs信息，根据rs的UID塞入podMap中
```go
	for _, pod := range pods {
		// Do not ignore inactive Pods because Recreate Deployments need to verify that no
		// Pods from older versions are running before spinning up new Pods.
		controllerRef := metav1.GetControllerOf(pod)
		if controllerRef == nil {
			continue
		}
		// Only append if we care about this UID.
		if _, ok := podMap[controllerRef.UID]; ok {
			podMap[controllerRef.UID] = append(podMap[controllerRef.UID], pod)
		}
	}
```

- 至此syncDeployment的准备工作已完成



# 本节重点总结
- 从代码上看 DeploymentController的启动也是两步走
  - 第一步就是deployment.NewDeploymentController 初始化DeploymentController控制器对象 dc
  - 第二步就是调用dc的Run方法运行
  - 这里我们来分析一下dc的Run
  
- 和其它控制器一样，首先会根据3个informer的ListerSynced方法判断本地是否list到数据了，然后并发启动worker执行同步
- syncDeployment内部在真正执行动作前会做一些准备工作
    - 获取dep关联的rsList
    - 根据rsList和dep获取podMap
