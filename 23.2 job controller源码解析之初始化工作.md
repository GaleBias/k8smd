# 本节重点总结:

![job_code.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/908/1636075728000/400c66ebbaca4037a4b7cf145fca0c46.png)

- Job 遵循 Kubernetes 的控制器模式进行设计

  - 在发生需要监听的事件时，Informer 就会调用控制器中的回调将需要处理的资源 Job 加入队列
  - 而控制器持有的工作协程就会处理这些任务，示意图如下
- JobController的流程分为初始化和启动两步
- JobController的初始化主要在于

  - 设置jobInformer的增删改回调处理函数
  - 设置podInformer的增删改回调处理函数
  - 设置同步状态和sync的处理函数

# Job 的控制器模式

- Job 遵循 Kubernetes 的控制器模式进行设计
- 在发生需要监听的事件时，Informer 就会调用控制器中的回调将需要处理的资源 Job 加入队列
- 而控制器持有的工作协程就会处理这些任务，示意图如下![job_code.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/908/1636075728000/67797e61dce44a08aa46aef7cc8e7d45.png)

# job-controller

## 定义controller初始化的大map

- job和其他Controller一样定义在NewControllerInitializers中
- 位置 D:\go_path\src\github.com\kubernetes\kubernetes\cmd\kube-controller-manager\app\controllermanager.go

```go
// paired to their InitFunc.  This allows for structured downstream composition and subdivision.
func NewControllerInitializers(loopMode ControllerLoopMode) map[string]InitFunc {
	controllers := map[string]InitFunc{}
	controllers["endpoint"] = startEndpointController
	controllers["job"] = startJobController
}
```

## JobController的初始化和启动

- 追踪startJobController ，发现这就是JobController的初始化和启动入口，位置在D:\go_path\src\github.com\kubernetes\kubernetes\cmd\kube-controller-manager\app\batch.go
- 下面的代码显示其中分了两步走，第一步先通过NewController获得JobController对象，第二步调用JobController的Run启动

```go
func startJobController(ctx ControllerContext) (controller.Interface, bool, error) {
	go job.NewController(
		ctx.InformerFactory.Core().V1().Pods(),
		ctx.InformerFactory.Batch().V1().Jobs(),
		ctx.ClientBuilder.ClientOrDie("job-controller"),
	).Run(int(ctx.ComponentConfig.JobController.ConcurrentJobSyncs), ctx.Stop)
	return nil, true, nil
}
```

### 第一步 NewController获得JobController对象

- 位置D:\go_path\src\github.com\kubernetes\kubernetes\pkg\controller\job\job_controller.go

> 入参分析：

- ctx.InformerFactory.Core().V1().Pods() 代表PodInformer，即要监听的pod对象
- ctx.InformerFactory.Batch().V1().Jobs() 代表JobInformer，即要监听的job对象
- ctx.ClientBuilder.ClientOrDie("job-controller") 代表和apiserver通信的kubeClient对象

#### NewController 分析

- 首先初始化event并设置Broadcaster

```go
	eventBroadcaster := record.NewBroadcaster()
	eventBroadcaster.StartStructuredLogging(0)
	eventBroadcaster.StartRecordingToSink(&v1core.EventSinkImpl{Interface: kubeClient.CoreV1().Events("")})

```

> 设置kubeclient请求限速相关的metrics

```go
	if kubeClient != nil && kubeClient.CoreV1().RESTClient().GetRateLimiter() != nil {
		ratelimiter.RegisterMetricAndTrackRateLimiterUsage("job_controller", kubeClient.CoreV1().RESTClient().GetRateLimiter())
	}
```

- 相关的metric就是 控制器名字开头+rate_limiter_use结尾的指标

```go
func registerRateLimiterMetric(ownerName string) error {
	metricsLock.Lock()
	defer metricsLock.Unlock()

	if _, ok := rateLimiterMetrics[ownerName]; ok {
		// only register once in Prometheus. We happen to see an ownerName reused in parallel integration tests.
		return nil
	}
	metric := metrics.NewGauge(&metrics.GaugeOpts{
		Name:           "rate_limiter_use",
		Subsystem:      ownerName,
		Help:           fmt.Sprintf("A metric measuring the saturation of the rate limiter for %v", ownerName),
		StabilityLevel: metrics.ALPHA,
	})
	if err := legacyregistry.Register(metric); err != nil {
		return fmt.Errorf("error registering rate limiter usage metric: %v", err)
	}
	stopCh := make(chan struct{})
	rateLimiterMetrics[ownerName] = &rateLimiterMetric{
		metric: metric,
		stopCh: stopCh,
	}
	return nil
}

```

- 我们在prometheus中查看相关指标就可以发现有一堆控制器的rate_limiter_use的指标 ，它们的值为0，是用来注册到prometheus做统计有哪些controller开始了rateLimiter
  - 到时候在prometheus 使用`{__name__=~".*rate_limiter_use"}` 就可以查询到那些控制器开启了rateLimiter![rateLimit.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/908/1636075728000/9ad50472caf0463ea43f7452ed679bdc.png)

> 初始化jm对象

```go
	jm := &Controller{
		kubeClient: kubeClient,
		podControl: controller.RealPodControl{
			KubeClient: kubeClient,
			Recorder:   eventBroadcaster.NewRecorder(scheme.Scheme, v1.EventSource{Component: "job-controller"}),
		},
		expectations: controller.NewControllerExpectations(),
		queue:        workqueue.NewNamedRateLimitingQueue(workqueue.NewItemExponentialFailureRateLimiter(DefaultJobBackOff, MaxJobBackOff), "job"),
		orphanQueue:  workqueue.NewNamedRateLimitingQueue(workqueue.NewItemExponentialFailureRateLimiter(DefaultJobBackOff, MaxJobBackOff), "job_orphan_pod"),
		recorder:     eventBroadcaster.NewRecorder(scheme.Scheme, v1.EventSource{Component: "job-controller"}),
	}
```

- 下面我们接着这个Controller初始化来看看JobController中的结构体字段
- 首先 kubeClient不用过多解释
- 然后是podControl，这里它用kube-controller-manager中的通用PodControlInterface，位置在 D:\go_path\src\github.com\kubernetes\kubernetes\pkg\controller\controller_utils.go
- 通过位置可以发现这个是kube-controller-manager的util包，属于公共的库
- 接口中定义的方法也很清晰，就是操作pod增删的方法

```go
// PodControlInterface is an interface that knows how to add or delete pods
// created as an interface to allow testing.
type PodControlInterface interface {
    // CreatePods creates new pods according to the spec, and sets object as the pod's controller.
    CreatePods(namespace string, template *v1.PodTemplateSpec, object runtime.Object, controllerRef *metav1.OwnerReference) error
    // CreatePodsWithGenerateName creates new pods according to the spec, sets object as the pod's controller and sets pod's generateName.
    CreatePodsWithGenerateName(namespace string, template *v1.PodTemplateSpec, object runtime.Object, controllerRef *metav1.OwnerReference, generateName string) error
    // DeletePod deletes the pod identified by podID.
    DeletePod(namespace string, podID string, object runtime.Object) error
    // PatchPod patches the pod.
    PatchPod(namespace, name string, data []byte) error
}
```

- 下面expectations代表 一个带ttl的缓存，用来存储各个rc 的期望个数和状态
- queue 代表的是 需要更新的job的队列
- orphanQueue 代表的是 需要删除的job的队列

##### 新建queue的操作

- 调用的入口

```go
		queue:        workqueue.NewNamedRateLimitingQueue(workqueue.NewItemExponentialFailureRateLimiter(DefaultJobBackOff, MaxJobBackOff), "job"),
	
```

- 首先是创建client-go中workqueue所需的RateLimiter对象，基础重试间隔为10秒，最长为360秒
- 然后带上RateLimiter对象 新建名字为job的workqueue

##### 设置jobInformer的增删改回调处理函数

- 这里不得不祭出之前分析过的informer架构图了![k8s_informer.jpeg](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/908/1636075728000/b4ee227f916c465abb38a134ad9d8fc4.jpeg)

```go
	jobInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
		AddFunc: func(obj interface{}) {
			jm.enqueueController(obj, true)
		},
		UpdateFunc: jm.updateJob,
		DeleteFunc: func(obj interface{}) {
			jm.enqueueController(obj, true)
		},
	})
```

> AddFunc和DeleteFunc调用的都是enqueueController

- 底层设置的回调就是jm.enqueueController 就是根据添加到workingQueue中
- 添加底层调用的AddAfter意思是在backoff时间后再把key添加进去，这里backoff在 immediate=true时是0，如果immediate=false需要根据当前队列的排队情况获取一个backoff等待时长

```go
func (jm *Controller) enqueueController(obj interface{}, immediate bool) {
	key, err := controller.KeyFunc(obj)
	if err != nil {
		utilruntime.HandleError(fmt.Errorf("Couldn't get key for object %+v: %v", obj, err))
		return
	}

	backoff := time.Duration(0)
	if !immediate {
		backoff = getBackoff(jm.queue, key)
	}
	klog.Infof("enqueueing job %s", key)
	jm.queue.AddAfter(key, backoff)
}
```

> job更新的回调

- 这里做的工作很简单就是立即将当前的job对象推入workQueue

```go
func (jm *Controller) updateJob(old, cur interface{}) {
	oldJob := old.(*batch.Job)
	curJob := cur.(*batch.Job)

	// never return error
	key, err := controller.KeyFunc(curJob)
	if err != nil {
		return
	}
	jm.enqueueController(curJob, true)
```

- 然后判断ActiveDeadlineSeconds是否发生了变化
  - 情况1 ，原来job中没有配置 ActiveDeadlineSeconds
  - 情况2 ，job的ActiveDeadlineSeconds发生了改变
- 动作就是以新的ActiveDeadlineSeconds减去job已经存活的时间作为backoff等待时间，把新的job再延迟入队，ActiveDeadlineSeconds代表job的存活时间

```go
	// check if need to add a new rsync for ActiveDeadlineSeconds
	if curJob.Status.StartTime != nil {
		curADS := curJob.Spec.ActiveDeadlineSeconds
		if curADS == nil {
			return
		}
		oldADS := oldJob.Spec.ActiveDeadlineSeconds
		if oldADS == nil || *oldADS != *curADS {
			now := metav1.Now()
			start := curJob.Status.StartTime.Time
			passed := now.Time.Sub(start)
			total := time.Duration(*curADS) * time.Second
			// AddAfter will handle total < passed
			jm.queue.AddAfter(key, total-passed)
			klog.V(4).Infof("job %q ActiveDeadlineSeconds updated, will rsync after %d seconds", key, total-passed)
		}
	}
}
```

- 在完成jobInformer的回调设置后，填充jobLister和jobStoreSynced 字段分别代表，joblist的结果和job已经同步过一次的标志位

```go
	jm.jobLister = jobInformer.Lister()
	jm.jobStoreSynced = jobInformer.Informer().HasSynced

```

##### 设置podInformer的增删改回调处理函数

- AddEventHandler如下

```go
	podInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
		AddFunc:    jm.addPod,
		UpdateFunc: jm.updatePod,
		DeleteFunc: jm.deletePod,
	})

```

###### AddFunc 对应的jm.addPod

- 根据 pod的DeletionTimestamp删除，这通常发生在controller重启时

```go
	pod := obj.(*v1.Pod)
	if pod.DeletionTimestamp != nil {
		// on a restart of the controller, it's possible a new pod shows up in a state that
		// is already pending deletion. Prevent the pod from being a creation observation.
		jm.deletePod(pod)
		return
	}

```

- 获取pod属主

```go
	// If it has a ControllerRef, that's all that matters.
	if controllerRef := metav1.GetControllerOf(pod); controllerRef != nil {

```

> 属主的含义是 某些 Kubernetes 对象是其它一些对象的属主。

- [文档地址](https://kubernetes.io/zh/docs/concepts/workloads/controllers/garbage-collection/)
- 例如，一个 ReplicaSet 是一组 Pod 的属主。 具有属主的对象被称为是属主的 附属
- 每个附属对象具有一个指向其所属对象的 metadata.ownerReferences 字段。
- Kubernetes 会自动为某些对象设置 ownerReference 的值。 这些对象是由 ReplicationController、ReplicaSet、StatefulSet、DaemonSet、Deployment、 Job 和 CronJob 所创建或管理的。
- 比如我们查看一个由deployment创建的pod，可以看到coredns这个pod的ownerReferences就是ReplicaSet

```shell
[root@k8s-master01 grafana]# kubectl get pod coredns-7d75679df-9vzm2 -n kube-system -o yaml |grep -i owner -C 5
  labels:
    k8s-app: kube-dns
    pod-template-hash: 7d75679df
  name: coredns-7d75679df-9vzm2
  namespace: kube-system
  ownerReferences:
  - apiVersion: apps/v1
    blockOwnerDeletion: true
    controller: true
    kind: ReplicaSet
    name: coredns-7d75679df
    uid: 3c157e75-2dfd-4c3a-8f1a-71fd5f909578
  resourceVersion: "763"
```

- 然后再查看这个coredns rs的属主，发现就是deployment，这和我们预期的deployment管理rs，rs管理pod一致

```shell
[root@k8s-master01 grafana]# kubectl get rs -n kube-system  
NAME                            DESIRED   CURRENT   READY   AGE
coredns-7d75679df               2         2         2       16h
kube-state-metrics-647444dd74   1         1         1       16h
[root@k8s-master01 grafana]# kubectl get rs coredns-7d75679df -n kube-system   -o yaml  |grep -i owner -C 5
  labels:
    k8s-app: kube-dns
    pod-template-hash: 7d75679df
  name: coredns-7d75679df
  namespace: kube-system
  ownerReferences:
  - apiVersion: apps/v1
    blockOwnerDeletion: true
    controller: true
    kind: Deployment
    name: coredns
    uid: 36aca5ad-4fd7-4339-baa7-de4e7422999c
  resourceVersion: "766"
```

- 如果获取有属主为job的pod说明是创建，那么走创建流程即可

```go

		job := jm.resolveControllerRef(pod.Namespace, controllerRef)
		if job == nil {
			return
		}
		jobKey, err := controller.KeyFunc(job)
		if err != nil {
			return
		}
		jm.expectations.CreationObserved(jobKey)
		jm.enqueueController(job, true)
		return
```

- 如果能走到这里说明pod没有job的属主信息，是孤儿pod需要删除

```go
	for _, job := range jm.getPodJobs(pod) {
		jm.enqueueController(job, true)
	}
```

###### UpdateFunc对应的jm.updatePod

- 如果新老pod的ResourceVersion一致说明pod没变

```go
	curPod := cur.(*v1.Pod)
	oldPod := old.(*v1.Pod)
	if curPod.ResourceVersion == oldPod.ResourceVersion {
		// Periodic resync will send update events for all known pods.
		// Two different versions of the same pod will always have different RVs.
		return
	}
```

- 根据DeletionTimestamp 删除pod

```go
	if curPod.DeletionTimestamp != nil {
		// when a pod is deleted gracefully it's deletion timestamp is first modified to reflect a grace period,
		// and after such time has passed, the kubelet actually deletes it from the store. We receive an update
		// for modification of the deletion timestamp and expect an job to create more pods asap, not wait
		// until the kubelet actually deletes the pod.
		jm.deletePod(curPod)
		return
	}
```

- 判断新老pod的属主信息是否发生变化，如果变了就同步旧的job

```go
	// the only time we want the backoff to kick-in, is when the pod failed
	immediate := curPod.Status.Phase != v1.PodFailed

	curControllerRef := metav1.GetControllerOf(curPod)
	oldControllerRef := metav1.GetControllerOf(oldPod)
	controllerRefChanged := !reflect.DeepEqual(curControllerRef, oldControllerRef)
	if controllerRefChanged && oldControllerRef != nil {
		// The ControllerRef was changed. Sync the old controller, if any.
		if job := jm.resolveControllerRef(oldPod.Namespace, oldControllerRef); job != nil {
			jm.enqueueController(job, immediate)
		}
	}

```

- 同步新的job

```go
	// If it has a ControllerRef, that's all that matters.
	if curControllerRef != nil {
		job := jm.resolveControllerRef(curPod.Namespace, curControllerRef)
		if job == nil {
			return
		}
		jm.enqueueController(job, immediate)
		return
	}
```

- 如果标签变了的话就同步job，看看是否有人接管

```go
	// Otherwise, it's an orphan. If anything changed, sync matching controllers
	// to see if anyone wants to adopt it now.
	labelChanged := !reflect.DeepEqual(curPod.Labels, oldPod.Labels)
	if labelChanged || controllerRefChanged {
		for _, job := range jm.getPodJobs(curPod) {
			jm.enqueueController(job, immediate)
		}
	}
```

###### DeleteFunc对应的jm.deletePod

- 根据pod获取属主信息，如果没解析出job就将pod加入孤儿队列enqueueOrphanPod，否则将job推入workqueue 队列中

```go
	pod, ok := obj.(*v1.Pod)
	controllerRef := metav1.GetControllerOf(pod)
	if controllerRef == nil {
		// No controller should care about orphans being deleted.
		return
	}
	job := jm.resolveControllerRef(pod.Namespace, controllerRef)
	if job == nil {
		if hasJobTrackingFinalizer(pod) {
			jm.enqueueOrphanPod(pod)
		}
		return
	}
	jobKey, err := controller.KeyFunc(job)
	if err != nil {
		return
	}
	jm.expectations.DeletionObserved(jobKey)
	jm.enqueueController(job, true)
```

- 在完成podInformer的回调设置后，填充podStore和podStoreSynced 字段分别代表，podlist的结果和pod已经同步过一次的标志位

```go
	jm.podStore = podInformer.Lister()
	jm.podStoreSynced = podInformer.Informer().HasSynced

```

##### jm对象收尾工作

- 设置更新状态和同步的处理函数

```go
	jm.updateStatusHandler = jm.updateJobStatus
	jm.patchJobHandler = jm.patchJob
	jm.syncHandler = jm.syncJob

```

- 注册相关metrics

```go
	metrics.Register()
func Register() {
	registerMetrics.Do(func() {
		legacyregistry.MustRegister(JobSyncDurationSeconds)
		legacyregistry.MustRegister(JobSyncNum)
		legacyregistry.MustRegister(JobFinishedNum)
	})
}

```

# 本节重点总结:

![job_code.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/908/1636075728000/c6006c2e6f564e4f89dc4e1bd90745b3.png)

- Job 遵循 Kubernetes 的控制器模式进行设计

  - 在发生需要监听的事件时，Informer 就会调用控制器中的回调将需要处理的资源 Job 加入队列
  - 而控制器持有的工作协程就会处理这些任务，示意图如下
- JobController的流程分为初始化和启动两步
- JobController的初始化主要在于

  - 设置jobInformer的增删改回调处理函数
  - 设置podInformer的增删改回调处理函数
  - 设置同步状态和sync的处理函数