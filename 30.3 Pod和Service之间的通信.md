# 本机重点总结

> pod访问svc的过程分3个步骤

- 01 coredns解析 svc的vip
- 02 节点上iptables转发，最后做DNAT到后端pod上

```shell
PREROUTING --> KUBE-SERVICE --> KUBE-SVC-XXX --> KUBE-SEP-XXX
```

- 03 不同pod之间请求，走calico的路由
- 架构图如下![pod_to_svc.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/908/1636076065000/ed6de41aaf5343ddac84d388dd798009.png)

> service 到底能不能 ping 通

- ClusterIP/NodePort

  - iptables：clusterIP 只是 iptables 中的规则，只会处理 ip:port 四层数据包，reject 了 icmp。不能 ping 通。
  - IPVS：clusterIP 会绑定到虚拟网卡 kube-ipvs0，配置了 route 路由到回环网卡，icmp 包是 lo 网卡回复的。可以 ping 通。
- 无头svc Headless: ClusterIP=None

  - Headless svc 不会分配 clusterIP，而是返回对应 DNS 的 A 记录，如果 svc 后端有3个 pod 则返回 3 个 pod IP。访问 svc 时会随机选择一个 IP，所以 headless svc 是可以 ping 通的。
- Loadbalancer 类型 svc 也要看云厂商的具体实现。
- ExternalName 对应 DNS 的 CNAME，如果配置的域名可以 ping 通则 svc 可以 ping 通。

# pod 和service的通信模式

- 之前我们提到可以通过iptables实现pod到service之间的流量转发
- 下面来复习一下iptables的基础知识

## iptables五条链

- iptables是linux内核集成的IP信息过滤规则，负责将发往主机的网络包进行分发，转换等
- 当客户端请求服务器的某个服务时，请求信息会先通过网卡进入服务器内核，这时iptables会对包进行过滤，决定这些包是发往用户态的服务进程或是转发出去到别的主机
- 而决定这些路径的方式在iptables中称为链，刚进入内核的请求流会经过PREROUTING链，根据路由规则判断是是不是发往本机请求，是则走INPUT链进入本机用户态进程，否则会走FORWARD链并匹配对应的规则最后流出本机
- 如果是本机发出的请求会走OUTPUT链并进一步到POSTROUTINE链流出本机，或转发到其他机器或回复信息给客户端。![image](https://image-static.segmentfault.com/115/596/1155966283-77cc9ef3aff50391_fix732)

> 总结上述几条链：

- PREROUTINE：流入本机路由前
- POSTROUTINE：流出本机路由前
- FORWARD：转发路径
- OUTPUT：由本机用户程序发出的
- INPUT：发送至本机用户程序的

## 两个动作 SNAT和 DNAT

> SNAT

- 源地址转换，是指将报文发送方的ip地址转换，这样当相应方回复请求时，回复的是发送方的地址。![image](https://image-static.segmentfault.com/369/510/3695109905-5e3241de7a9f9ac1_fix732)
- 当client发送请求给server时，需要经过gateway，如果gateway不对包进行源地址转换(SNAT)，发往server的网络包携带的源地址依然是client，server会对该源地址响应，但client并不识别server的地址，会导致该条请求出现错误。

> DNAT

- 目标地址转换，是将报文的目标地址转换，起到请求转发到别的目的地的作用。

## 通信流程解析

### 01 coredns解析 svc 的vip

- 创建一个dep和clusterip类型的svc，用来模拟svc和后端的pod服务 ，yaml如下

```yaml
cat <<EOF | kubectl create -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-dep08
  labels:
    app: nginx-svc08
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nginx-pod08
  template:
    metadata:
      labels:
        app: nginx-pod08
    spec:
      containers:
      - name: nginx
        image: nginx:1.8
        ports:
        - containerPort: 80
--- 
apiVersion: v1
kind: Service
metadata:
  name: nginx-pod08
spec:
  selector:
    app: nginx-pod08 # 选择我们上面创建dep中的nginx pod，
  ports:
    - protocol: TCP
      port: 8080  # service对外暴露的端口
      targetPort: 80 # 代表目标容器的端口是80
EOF

```

- 创建 一个curl 的pod，用来模拟pod请求端

```yaml
cat <<EOF | kubectl create -f -
apiVersion: v1
kind: Pod
metadata:
  name: pod-client
spec:
  containers:
  - name: curl
    image: yauritux/busybox-curl
    ports:
    command:
        - sleep
        - "3600"
EOF
```

- exec到pod-client 内部执行curl nginx-pod08.default.svc.cluster.local:8080

```shell
[root@k8s-master01 k8s_svc]# kubectl exec pod/pod-client -ti -- /bin/sh
/home # 
/home # curl nginx-pod08.default.svc.cluster.local:8080
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
/home # 
```

- ping对应的域名可以看到 解析出来的vip地址 为10.96.125.237

```shell
/home # ping nginx-pod08.default.svc.cluster.local
PING nginx-pod08.default.svc.cluster.local (10.96.125.237): 56 data bytes
^C
--- nginx-pod08.default.svc.cluster.local ping statistics ---
1 packets transmitted, 0 packets received, 100% packet loss
```

- 同时pod client的ip为10.100.85.247

```shell
/home # ip a
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host 
       valid_lft forever preferred_lft forever
3: eth0@if5562: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1450 qdisc noqueue 
    link/ether 7e:c6:b0:c2:0e:83 brd ff:ff:ff:ff:ff:ff
    inet 10.100.85.247/32 brd 10.100.85.247 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::7cc6:b0ff:fec2:e83/64 scope link 
       valid_lft forever preferred_lft forever
```

### 02 pod ip 10.100.85.247访问 svc vip 10.96.125.237 8080端口

- 到这个pod所在宿主机查看 iptables规则( 假定 kube-proxy的转发模式是iptables的)

```shell
iptables-save  >ipt
```

- 对于iptables的分析如下
- 01 对于进入 PREROUTING 链的都转到 KUBE-SERVICES 链进行处理；

```shell
[root@k8s-node01 ~]# grep KUBE-SERVICES ipt 
-A PREROUTING -m comment --comment "kubernetes service portals" -j KUBE-SERVICES
```

- 02 在 KUBE-SERVICES 链，对于访问 clusterIP 为10.96.125.237 目的端口 8080 的转发到 KUBE-SVC-DDZHMK5AO3JF6KIZ链上

```shell
[root@k8s-node01 ~]# grep 10.96.125.237 ipt 
-A KUBE-SERVICES -d 10.96.125.237/32 -p tcp -m comment --comment "default/nginx-pod08 cluster IP" -m tcp --dport 8080 -j KUBE-SVC-DDZHMK5AO3JF6KIZ
[root@k8s-node01 ~]

```

- 03 访问 KUBE-SVC-DDZHMK5AO3JF6KIZ 的使用随机数负载均衡，--probability 0.50000000000代表有0.5的概率进入上面的规则，并转发到 KUBE-SEP-ATSP76LQOJHSORE7 和  KUBE-SEP-FXZXKRS4RXLGILD5 上；

```shell
[root@k8s-node01 ~]# grep KUBE-SVC-DDZHMK5AO3JF6KIZ ipt 
-A KUBE-SVC-DDZHMK5AO3JF6KIZ -m comment --comment "default/nginx-pod08" -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-ATSP76LQOJHSORE7
-A KUBE-SVC-DDZHMK5AO3JF6KIZ -m comment --comment "default/nginx-pod08" -j KUBE-SEP-FXZXKRS4RXLGILD5

```

- 04 KUBE-SEP-FXZXKRS4RXLGILD5  和 KUBE-SEP-OVNLTDWFHTHII4SC 对应 endpoint 中的 pod 10.100.85.246 和 10.100.85.253，设置 mark 标记，进行 DNAT 并转发到具体的 pod 上
- 如果某个 service 的 endpoints 中没有 pod，那么针对此 service 的请求将会被 drop 掉；

```shell
[root@k8s-node01 ~]# grep KUBE-SEP-FXZXKRS4RXLGILD5 ipt 
-A KUBE-SEP-FXZXKRS4RXLGILD5 -s 10.100.85.253/32 -m comment --comment "default/nginx-pod08" -j KUBE-MARK-MASQ
-A KUBE-SEP-FXZXKRS4RXLGILD5 -p tcp -m comment --comment "default/nginx-pod08" -m tcp -j DNAT --to-destination 10.100.85.253:80

[root@k8s-node01 ~]# grep KUBE-SEP-ATSP76LQOJHSORE7 ipt 
-A KUBE-SEP-ATSP76LQOJHSORE7 -s 10.100.85.246/32 -m comment --comment "default/nginx-pod08" -j KUBE-MARK-MASQ
-A KUBE-SVC-DDZHMK5AO3JF6KIZ -m comment --comment "default/nginx-pod08" -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-ATSP76LQOJHSORE7

# 获取 ep 和pod
[root@k8s-master01 k8s_svc]# kubectl get pod -o wide  |grep dep08
nginx-dep08-584b5f5458-cbksj               1/1     Running   0          34m     10.100.85.246   k8s-node01   <none>           <none>
nginx-dep08-584b5f5458-m7pz8               1/1     Running   0          34m     10.100.85.253   k8s-node01   <none>           <none>

[root@k8s-master01 k8s_svc]# kubectl get ep
NAME                                          ENDPOINTS                                            AGE
nginx-pod08                                   10.100.85.246:80,10.100.85.253:80                    35m
```

- 这里的-j KUBE-MARK-MASQ对应的就是动态SNAT的目的，从服务器的网卡上，自动获取当前ip地址来做NAT，如此配置的话，不用指定SNAT的目标ip了，不管现在eth0的出口获得了怎样的动态ip。

```shell
[root@k8s-node01 ~]# grep KUBE-MARK-MASQ ipt  
-A KUBE-MARK-MASQ -j MARK --set-xmark 0x4000/0x4000
```

```shell
PREROUTING --> KUBE-SERVICE --> KUBE-SVC-XXX --> KUBE-SEP-XXX
```

### 03 经过iptables转换后变成pod直接的访问

- 经过上面的转换已经变成client pod ip 10.100.85.247 访问svc对应后端pod 10.100.85.246:80 和10.100.85.253:80
- 然后就是不同pod之间走clico或者Flannel转发

# service 到底能不能 ping 通

- 面试中被问了这道题，其实是在考察你对 k8s service 的使用和底层原理的理解，答案显然没有那么简单。
- 这里要针对不同类型的svc进行说明

## 01 针对ClusterIP/NodePort

- 这类 svc 都会分配 ClusterIP，这个 IP 地址是 VIP（虚拟 IP），是在所有 node 上添加一些 netfilter 规则，主要有 iptables 和 ipvs 两种方案，能不能 ping 通要看具体实现。

  - iptables：clusterIP 只是 iptables 中的规则，只会处理 ip:port 四层数据包，reject 了 icmp。不能 ping 通。
  - IPVS：clusterIP 会绑定到虚拟网卡 kube-ipvs0，配置了 route 路由到回环网卡，icmp 包是 lo 网卡回复的。可以 ping 通。

## 02 无头svc Headless: ClusterIP=None

- Headless svc 不会分配 clusterIP，而是返回对应 DNS 的 A 记录，如果 svc 后端有3个 pod 则返回 3 个 pod IP。访问 svc 时会随机选择一个 IP，所以 headless svc 是可以 ping 通的。

## 03 Loadbalancer

- Loadbalancer 类型 svc 也要看云厂商的具体实现。
  - 普通模式：基于 NodePort，LB -> node:nodePort -> pod。ping 的结果跟 NodePort 一致。
  - 直连模式：LB 和 pod 处于同一个 VPC 子网，LB -> pod。ping 的结果跟 Headless 一致。

## 04 ExternalName

- ExternalName 对应 DNS 的 CNAME，如果配置的域名可以 ping 通则 svc 可以 ping 通。

# 本机重点总结

> pod访问svc的过程分3个步骤

- 01 coredns解析 svc的vip
- 02 节点上iptables转发，最后做DNAT到后端pod上

```shell
PREROUTING --> KUBE-SERVICE --> KUBE-SVC-XXX --> KUBE-SEP-XXX
```

- 03 不同pod之间请求，走calico的路由
- 架构图如下![pod_to_svc.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/908/1636076065000/e2de74ab93c1491f9b2ca486da2d4f81.png)

> service 到底能不能 ping 通

- ClusterIP/NodePort

  - iptables：clusterIP 只是 iptables 中的规则，只会处理 ip:port 四层数据包，reject 了 icmp。不能 ping 通。
  - IPVS：clusterIP 会绑定到虚拟网卡 kube-ipvs0，配置了 route 路由到回环网卡，icmp 包是 lo 网卡回复的。可以 ping 通。
- 无头svc Headless: ClusterIP=None

  - Headless svc 不会分配 clusterIP，而是返回对应 DNS 的 A 记录，如果 svc 后端有3个 pod 则返回 3 个 pod IP。访问 svc 时会随机选择一个 IP，所以 headless svc 是可以 ping 通的。
- Loadbalancer 类型 svc 也要看云厂商的具体实现。
- ExternalName 对应 DNS 的 CNAME，如果配置的域名可以 ping 通则 svc 可以 ping 通。