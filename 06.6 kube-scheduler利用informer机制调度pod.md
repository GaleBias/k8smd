# 回顾scheduler结构体

- 位置 D:\go_path\src\github.com\kubernetes\kubernetes\pkg\scheduler\scheduler.go
- 可以看到其中和pod调度直接相关的两个字段

```go
type Scheduler struct {
  
    NextPod func() *framework.QueuedPodInfo  // 获取下一个需要调度的Pod
    SchedulingQueue internalqueue.SchedulingQueue  // 等待调度的Pod队列，我们重点看看这个队列是什么
}
```

## SchedulingQueue的初始化

- 在create函数中，创建了podQueue，位置  D:\go_path\src\github.com\kubernetes\kubernetes\pkg\scheduler\factory.go

```go
	podQueue := internalqueue.NewSchedulingQueue(
		lessFn,
		c.informerFactory,
		internalqueue.WithPodInitialBackoffDuration(time.Duration(c.podInitialBackoffSeconds)*time.Second),
		internalqueue.WithPodMaxBackoffDuration(time.Duration(c.podMaxBackoffSeconds)*time.Second),
		internalqueue.WithPodNominator(nominator),
		internalqueue.WithClusterEventMap(c.clusterEventMap),
	)
```

- 可以看出这是一个带有优先级的队列

```go
// NewSchedulingQueue initializes a priority queue as a new scheduling queue.
func NewSchedulingQueue(
	lessFn framework.LessFunc,
	informerFactory informers.SharedInformerFactory,
	opts ...Option) SchedulingQueue {
	return NewPriorityQueue(lessFn, informerFactory, opts...)
}
```

### 为何要有优先级

- 因为有些pod比较主要，需要优先调度
- [调度优先级文档](https://kubernetes.io/zh/docs/concepts/scheduling-eviction/pod-priority-preemption/)

> 获取集群默认的调度优先级

```shell
[root@k8s-master01 k8s-informer]# kubectl get   PriorityClass  
NAME                      VALUE        GLOBAL-DEFAULT   AGE
calico-priority           1000000000   false            162d
system-cluster-critical   2000000000   false            162d
system-node-critical      2000001000   false            162d
```

> pod调度优先级实例，之前讲的prometheus statefulset中的配置

```yaml
  template:
    metadata:
      labels:
        k8s-app: prometheus
    
      annotations:
        scheduler.alpha.kubernetes.io/critical-pod: ''
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: k8s-app
                operator: In
                values:
                - prometheus
            topologyKey: "kubernetes.io/hostname"
      priorityClassName: system-cluster-critical
      hostNetwork: true
      dnsPolicy: ClusterFirstWithHostNet
```

## NextPod的初始化

- 可以看到就是从 podQueue中 pop一个

```go
// MakeNextPodFunc returns a function to retrieve the next pod from a given
// scheduling queue
func MakeNextPodFunc(queue SchedulingQueue) func() *framework.QueuedPodInfo {
	return func() *framework.QueuedPodInfo {
		podInfo, err := queue.Pop()
		if err == nil {
			klog.V(4).InfoS("About to try and schedule pod", "pod", klog.KObj(podInfo.Pod))
			return podInfo
		}
		klog.ErrorS(err, "Error while retrieving next pod from scheduling queue")
		return nil
	}
}
```

# pod信息什么时候推入SchedulingQueue

- 还记得之前scheduler的New中会有添加 回调的函数

```go
addAllEventHandlers(sched, informerFactory, dynInformerFactory, unionedGVKs(clusterEventMap))
```

## 过滤未调度的pod回调

```go
	// scheduled pod cache
	informerFactory.Core().V1().Pods().Informer().AddEventHandler(
		cache.FilteringResourceEventHandler{
			FilterFunc: func(obj interface{}) bool {
				switch t := obj.(type) {
				case *v1.Pod:
					return assignedPod(t)
				case cache.DeletedFinalStateUnknown:
					if pod, ok := t.Obj.(*v1.Pod); ok {
						return assignedPod(pod)
					}
					utilruntime.HandleError(fmt.Errorf("unable to convert object %T to *v1.Pod in %T", obj, sched))
					return false
				default:
					utilruntime.HandleError(fmt.Errorf("unable to handle object in %T: %T", sched, obj))
					return false
				}
			},
			Handler: cache.ResourceEventHandlerFuncs{
				AddFunc:    sched.addPodToCache,
				UpdateFunc: sched.updatePodInCache,
				DeleteFunc: sched.deletePodFromCache,
			},
		},
	)
```

- FilterFunc为一个过滤的函数，assignedPod代表pod信息中有node信息了，说明pod已被调度到node

```go
// assignedPod selects pods that are assigned (scheduled and running).
func assignedPod(pod *v1.Pod) bool {
	return len(pod.Spec.NodeName) != 0
}

```

- add对应的触发动作就是 sched.addPodToCache。比如我们之前创建的nginx-pod就该走这里
- 在下面的addPodToCache可以看到调用了SchedulingQueue.AssignedPodAdded将pod推入队列中

```go
func (sched *Scheduler) addPodToCache(obj interface{}) {
	pod, ok := obj.(*v1.Pod)
	if !ok {
		klog.ErrorS(nil, "Cannot convert to *v1.Pod", "obj", obj)
		return
	}
	klog.V(3).InfoS("Add event for scheduled pod", "pod", klog.KObj(pod))

	if err := sched.SchedulerCache.AddPod(pod); err != nil {
		klog.ErrorS(err, "Scheduler cache AddPod failed", "pod", klog.KObj(pod))
	}

	sched.SchedulingQueue.AssignedPodAdded(pod)
}
```

- 至此 创建的pod入队出队我们都了解了

# 执行调度

- 我们可以追踪NextPod合适被调用，追查到在scheduleOne中

```go
func (sched *Scheduler) scheduleOne(ctx context.Context) {
	podInfo := sched.NextPod()
```

- 在向上追查可以看到是在scheduler启动的时候，选主成功的OnStartedLeading回调中有sched.Run执行调度

```go
	// If leader election is enabled, runCommand via LeaderElector until done and exit.
	if cc.LeaderElection != nil {
		cc.LeaderElection.Callbacks = leaderelection.LeaderCallbacks{
			OnStartedLeading: func(ctx context.Context) {
				close(waitingForLeader)
				sched.Run(ctx)
			},
			OnStoppedLeading: func() {
				select {
				case <-ctx.Done():
					// We were asked to terminate. Exit 0.
					klog.Info("Requested to terminate. Exiting.")
					os.Exit(0)
				default:
					// We lost the lock.
					klog.Exitf("leaderelection lost")
				}
			},
		}
```

## scheduleOne分析

- podInfo 就是从队列中获取到的pod对象，检查pod的有效性

```go
	podInfo := sched.NextPod()
	// pod could be nil when schedulerQueue is closed
	if podInfo == nil || podInfo.Pod == nil {
		return
	}
	pod := podInfo.Pod
```

- 根据定义的 pod.Spec.SchedulerName 查到对应的profile

```go
	fwk, err := sched.frameworkForPod(pod)
	if err != nil {
		// This shouldn't happen, because we only accept for scheduling the pods
		// which specify a scheduler name that matches one of the profiles.
		klog.ErrorS(err, "Error occurred")
		return
	}
```

- 根据调度算法获取结果，

```go
scheduleResult, err := sched.Algorithm.Schedule(schedulingCycleCtx, sched.Extenders, fwk, state, pod)
```

- 调用assume对调度算法的结果进行验证

```go
	assumedPodInfo := podInfo.DeepCopy()
	assumedPod := assumedPodInfo.Pod
	// assume modifies `assumedPod` by setting NodeName=scheduleResult.SuggestedHost
	err = sched.assume(assumedPod, scheduleResult.SuggestedHost)
```

- 下面的go func 进行异步绑定

```go
// bind the pod to its host asynchronously (we can do this b/c of the assumption step above).
err := sched.bind(bindingCycleCtx, fwk, assumedPod, scheduleResult.SuggestedHost, state)
```

- 绑定成功后就会打几个metrics

```go
			metrics.PodScheduled(fwk.ProfileName(), metrics.SinceInSeconds(start))
			metrics.PodSchedulingAttempts.Observe(float64(podInfo.Attempts))
			metrics.PodSchedulingDuration.WithLabelValues(getAttemptsLabel(podInfo)).Observe(metrics.SinceInSeconds(podInfo.InitialAttemptTimestamp))

```

- 比如 平均调度时间

```shell
scheduler_pod_scheduling_duration_seconds_sum /scheduler_pod_scheduling_duration_seconds_count
```

### Schedule调度解析

- 对当前信息保存快照，如果快照中的node数量为0就返回没有节点可用

```go
	if err := g.snapshot(); err != nil {
		return result, err
	}
	trace.Step("Snapshotting scheduler cache and node infos done")

	if g.nodeInfoSnapshot.NumNodes() == 0 {
		return result, ErrNoNodesAvailable
	}
```

- Predict阶段：找到所有满足调度条件的节点feasibleNodes，不满足的就直接过滤

```go

	feasibleNodes, diagnosis, err := g.findNodesThatFitPod(ctx, extenders, fwk, state, pod)
	if err != nil {
		return result, err
	}
	trace.Step("Computing predicates done")

	if len(feasibleNodes) == 0 {
		return result, &framework.FitError{
			Pod:         pod,
			NumAllNodes: g.nodeInfoSnapshot.NumNodes(),
			Diagnosis:   diagnosis,
		}
	}
```

- 如果Predict阶段只找到一个节点就用它

```go
	if len(feasibleNodes) == 0 {
		return result, &framework.FitError{
			Pod:         pod,
			NumAllNodes: g.nodeInfoSnapshot.NumNodes(),
			Diagnosis:   diagnosis,
		}
	}

```

- Priority阶段：通过打分，找到一个分数最高、也就是最优的节点

```go
	priorityList, err := prioritizeNodes(ctx, extenders, fwk, state, pod, feasibleNodes)
	if err != nil {
		return result, err
	}
```

> Predict 和 Priority

- Predict 和 Priority 是选择调度节点的两个关键性步骤， 它的底层调用了各种algorithm算法
- 我们之前提到的NodeName 匹配属于Predict阶段

### Assume验证解读

- 将 host 填入到 pod spec字段的nodename，假定分配到对应的节点上
- 调用 SchedulerCache 下的 AssumePod测试，如果出错则验证失败

```go
func (sched *Scheduler) assume(assumed *v1.Pod, host string) error {
	// Optimistically assume that the binding will succeed and send it to apiserver
	// in the background.
	// If the binding fails, scheduler will release resources allocated to assumed pod
	// immediately.
	assumed.Spec.NodeName = host

	if err := sched.SchedulerCache.AssumePod(assumed); err != nil {
		klog.ErrorS(err, "Scheduler cache AssumePod failed")
		return err
	}
	// if "assumed" is a nominated pod, we should remove it from internal cache
	if sched.SchedulingQueue != nil {
		sched.SchedulingQueue.DeleteNominatedPodIfExists(assumed)
	}

	return nil
}
```

#### AssumePod解读

- 根据pod uid去cache中寻找 ， 正常是找不到的

```go
func (cache *schedulerCache) AssumePod(pod *v1.Pod) error {
	key, err := framework.GetPodKey(pod)
	if err != nil {
		return err
	}

	cache.mu.Lock()
	defer cache.mu.Unlock()
	if _, ok := cache.podStates[key]; ok {
		return fmt.Errorf("pod %v is in the cache, so can't be assumed", key)
	}

	cache.addPod(pod)
	ps := &podState{
		pod: pod,
	}
	cache.podStates[key] = ps
	cache.assumedPods.Insert(key)
	return nil
}

```

- cache.addPod(pod)代表把pod 信息填入node中

```go
// Assumes that lock is already acquired.
func (cache *schedulerCache) addPod(pod *v1.Pod) {
	n, ok := cache.nodes[pod.Spec.NodeName]
	if !ok {
		n = newNodeInfoListItem(framework.NewNodeInfo())
		cache.nodes[pod.Spec.NodeName] = n
	}
	n.info.AddPod(pod)
	cache.moveNodeInfoToHead(pod.Spec.NodeName)
}
```

- AddPodInfo 会更新node的信息，把新来的pod叠加上去

```go
// Consider using this instead of AddPod if a PodInfo is already computed.
func (n *NodeInfo) AddPodInfo(podInfo *PodInfo) {
	res, non0CPU, non0Mem := calculateResource(podInfo.Pod)
	n.Requested.MilliCPU += res.MilliCPU
	n.Requested.Memory += res.Memory
	n.Requested.EphemeralStorage += res.EphemeralStorage
	if n.Requested.ScalarResources == nil && len(res.ScalarResources) > 0 {
		n.Requested.ScalarResources = map[v1.ResourceName]int64{}
	}
	for rName, rQuant := range res.ScalarResources {
		n.Requested.ScalarResources[rName] += rQuant
	}
	n.NonZeroRequested.MilliCPU += non0CPU
	n.NonZeroRequested.Memory += non0Mem
	n.Pods = append(n.Pods, podInfo)
	if podWithAffinity(podInfo.Pod) {
		n.PodsWithAffinity = append(n.PodsWithAffinity, podInfo)
	}
	if podWithRequiredAntiAffinity(podInfo.Pod) {
		n.PodsWithRequiredAntiAffinity = append(n.PodsWithRequiredAntiAffinity, podInfo)
	}

	// Consume ports when pods added.
	n.updateUsedPorts(podInfo.Pod, true)
	n.updatePVCRefCounts(podInfo.Pod, true)

	n.Generation = nextGeneration()
}
```

### bind绑定操作解读

- 将assumed验证过的pod信息bind到node上

```go
func (sched *Scheduler) bind(ctx context.Context, fwk framework.Framework, assumed *v1.Pod, targetNode string, state *framework.CycleState) (err error) {
	defer func() {
		sched.finishBinding(fwk, assumed, targetNode, err)
	}()

	bound, err := sched.extendersBinding(assumed, targetNode)
	if bound {
		return err
	}
	bindStatus := fwk.RunBindPlugins(ctx, state, assumed, targetNode)
	if bindStatus.IsSuccess() {
		return nil
	}
	if bindStatus.Code() == framework.Error {
		return bindStatus.AsError()
	}
	return fmt.Errorf("bind status: %s, %v", bindStatus.Code().String(), bindStatus.Message())
}
```

- 底层的请求

```go
// Bind delegates the action of binding a pod to a node to the extender.
func (h *HTTPExtender) Bind(binding *v1.Binding) error {
	var result extenderv1.ExtenderBindingResult
	if !h.IsBinder() {
		// This shouldn't happen as this extender wouldn't have become a Binder.
		return fmt.Errorf("unexpected empty bindVerb in extender")
	}
	req := &extenderv1.ExtenderBindingArgs{
		PodName:      binding.Name,
		PodNamespace: binding.Namespace,
		PodUID:       binding.UID,
		Node:         binding.Target.Name,
	}
	if err := h.send(h.bindVerb, req, &result); err != nil {
		return err
	}
	if result.Error != "" {
		return fmt.Errorf(result.Error)
	}
	return nil
}
```

### 解读一个最简单的过滤器node_name

- 首先打开scheduler的插件目录，D:\go_path\src\github.com\kubernetes\kubernetes\pkg\scheduler\framework\plugins
- 可以看到一堆类似过滤器的目录，在其中找打node_name ，位置 D:\go_path\src\github.com\kubernetes\kubernetes\pkg\scheduler\framework\plugins\nodename\node_name.go

```go
// Filter invoked at the filter extension point.
func (pl *NodeName) Filter(ctx context.Context, _ *framework.CycleState, pod *v1.Pod, nodeInfo *framework.NodeInfo) *framework.Status {
	if nodeInfo.Node() == nil {
		return framework.NewStatus(framework.Error, "node not found")
	}
	if !Fits(pod, nodeInfo) {
		return framework.NewStatus(framework.UnschedulableAndUnresolvable, ErrReason)
	}
	return nil
}

// Fits actually checks if the pod fits the node.
func Fits(pod *v1.Pod, nodeInfo *framework.NodeInfo) bool {
	return len(pod.Spec.NodeName) == 0 || pod.Spec.NodeName == nodeInfo.Node().Name
}

// New initializes a new plugin and returns it.
func New(_ runtime.Object, _ framework.Handle) (framework.Plugin, error) {
	return &NodeName{}, nil
}

```

#### 这里看到New函数，疑似注册

- 往上追查可以看到在registry有注册的动作，位置 D:\go_path\src\github.com\kubernetes\kubernetes\pkg\scheduler\framework\plugins\registry.go

```go
func NewInTreeRegistry() runtime.Registry {
	fts := plfeature.Features{
		EnablePodAffinityNamespaceSelector: feature.DefaultFeatureGate.Enabled(features.PodAffinityNamespaceSelector),
		EnablePodDisruptionBudget:          feature.DefaultFeatureGate.Enabled(features.PodDisruptionBudget),
		EnablePodOverhead:                  feature.DefaultFeatureGate.Enabled(features.PodOverhead),
		EnableReadWriteOncePod:             feature.DefaultFeatureGate.Enabled(features.ReadWriteOncePod),
	}

	return runtime.Registry{
		selectorspread.Name:      selectorspread.New,
		imagelocality.Name:       imagelocality.New,
		tainttoleration.Name:     tainttoleration.New,
		nodename.Name:            nodename.New,
```

- 再向上追溯可以看到是Scheduler的New中调用的NewInTreeRegistry

```go
registry := frameworkplugins.NewInTreeRegistry()
```

#### 回到node_name 的Filter函数

- Filter调用Fits判断 pod的spec nodename 是否和目标node相等
- 追踪Filter调用过程，发现是RunFilterPlugins遍历插件调用，位置 D:\go_path\src\github.com\kubernetes\kubernetes\pkg\scheduler\framework\runtime\framework.go

```go
func (f *frameworkImpl) RunFilterPlugins(
	ctx context.Context,
	state *framework.CycleState,
	pod *v1.Pod,
	nodeInfo *framework.NodeInfo,
) framework.PluginToStatus {
	statuses := make(framework.PluginToStatus)
	for _, pl := range f.filterPlugins {
		pluginStatus := f.runFilterPlugin(ctx, pl, state, pod, nodeInfo)
		if !pluginStatus.IsSuccess() {
			if !pluginStatus.IsUnschedulable() {
				// Filter plugins are not supposed to return any status other than
				// Success or Unschedulable.
				errStatus := framework.AsStatus(fmt.Errorf("running %q filter plugin: %w", pl.Name(), pluginStatus.AsError())).WithFailedPlugin(pl.Name())
				return map[string]*framework.Status{pl.Name(): errStatus}
			}
			pluginStatus.SetFailedPlugin(pl.Name())
			statuses[pl.Name()] = pluginStatus
			if !f.runAllFilters {
				// Exit early if we don't need to run all filters.
				return statuses
			}
		}
	}

	return statuses
}

func (f *frameworkImpl) runFilterPlugin(ctx context.Context, pl framework.FilterPlugin, state *framework.CycleState, pod *v1.Pod, nodeInfo *framework.NodeInfo) *framework.Status {
	if !state.ShouldRecordPluginMetrics() {
		return pl.Filter(ctx, state, pod, nodeInfo)
	}
	startTime := time.Now()
	status := pl.Filter(ctx, state, pod, nodeInfo)
	f.metricsRecorder.observePluginDurationAsync(Filter, pl.Name(), status, metrics.SinceInSeconds(startTime))
	return status
}
```

- 最终追查到是 findNodesThatPassFilters调用了RunFilterPluginsWithNominatedPods ，位置在D:\go_path\src\github.com\kubernetes\kubernetes\pkg\scheduler\generic_scheduler.go

```go
func (g *genericScheduler) findNodesThatPassFilters(
   status := fwk.RunFilterPluginsWithNominatedPods(ctx, state, pod, nodeInfo)
)
```

# 本节重点总结 :

![image](https://d33wubrfki0l68.cloudfront.net/4e9fa4651df31b7810c851b142c793776509e046/61a36/images/docs/scheduling-framework-extensions.png)

- Pod的调度是通过一个队列SchedulingQueue异步工作的
  - 监听到对应pod事件后，放入队列
  - 有个消费者从队列中获取pod，进行调度
- 单个pod的调度主要分为3个步骤：
  - 根据Predict和Priority两个阶段，调用各自的算法插件，选择最优的Node
  - Assume这个Pod被调度到对应的Node，保存到cache
  - 用extender和plugins进行验证，如果通过则绑定
- 绑定成功后，将数据通过client向kube-apiserver发送，更新etcd