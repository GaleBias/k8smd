# 本节重点总结:

- Run中会启动workersNum个 worker任务
- 每个worker任务执行syncJob，会进行如下的操作
  - 从 lister 中获取 job 对象；
  - 调用 jm.getPodsForJob 通过 selector 获取 job 关联的 pod，
  - 判断 job 的重试次数是否超过了 job.Spec.BackoffLimit(默认是6次)
  - 判断 job 的运行时间是否达到 job.Spec.ActiveDeadlineSeconds 中设定的值
  - 判断如果 job 处于 failed 状态，则调用 jm.deleteJobPods 并发删除所有 active pods ；
  - 根据 jobNeedsSync 判断是否要进行同步，若需要同步则调用 jm.manageJob 进行同步；
  - 如果 job 的 status 有变化，将 job 的 status 更新到 apiserver；

# 接上回

- 上节课我们分析了job-controller的初始化操作
- 下面来分析一下运行的主流程

# Run调用入口

- 在startJobController中，位置D:\go_path\src\github.com\kubernetes\kubernetes\pkg\controller\job\job_controller.go

```go
func startJobController(ctx ControllerContext) (controller.Interface, bool, error) {
	go job.NewController(
		ctx.InformerFactory.Core().V1().Pods(),
		ctx.InformerFactory.Batch().V1().Jobs(),
		ctx.ClientBuilder.ClientOrDie("job-controller"),
	).Run(int(ctx.ComponentConfig.JobController.ConcurrentJobSyncs), ctx.Stop)
	return nil, true, nil
}
```

## 入参分析

- ctx.ComponentConfig.JobController.ConcurrentJobSyncs 代表并发同步job的个数，默认是5，调大会消耗资源，调小job的响应不及时
- ctx.Stop代表同步退出的context

## Run 分析

- 首先是退出清理的defer函数

```go
	defer utilruntime.HandleCrash()
	defer jm.queue.ShutDown()
	defer jm.orphanQueue.ShutDown()

```

- 等待job和pod 至少 list同步一次

```go
	if !cache.WaitForNamedCacheSync("job", stopCh, jm.podStoreSynced, jm.jobStoreSynced) {
		return
	}
```

- 启动workersNum个 worker任务

```go

	for i := 0; i < workers; i++ {
		go wait.Until(jm.worker, time.Second, stopCh)
	}
```

- 启动一个orphanWorker处理孤儿god

```go
	go wait.Until(jm.orphanWorker, time.Second, stopCh)

```

### 主流程worker解析

- 从函数注释可以看到 worker启动一个线程，从队列中消费任务，处理任务，标记任务为已完成
- 内容就是不断的调用jm.processNextWorkItem，知道jm.processNextWorkItem返回false退出

```go
// worker runs a worker thread that just dequeues items, processes them, and marks them done.
// It enforces that the syncHandler is never invoked concurrently with the same key.
func (jm *Controller) worker() {
	for jm.processNextWorkItem() {
	}
}

```

> processNextWorkItem流程分析

- 从队列中获取一个key，用defer 标记这个任务完成

```go
	key, quit := jm.queue.Get()
	if quit {
		return false
	}
	defer jm.queue.Done(key)
```

- 然后调用syncHandler处理，返回的forget代表这个key不需要重试了

```go

	forget, err := jm.syncHandler(key.(string))
	if err == nil {
		if forget {
			jm.queue.Forget(key)
		}
		return true
	}
```

- 不然的话打印错误，将key再推入队列中等待下一次调度

```go
	utilruntime.HandleError(fmt.Errorf("Error syncing job: %v", err))
	jm.queue.AddRateLimited(key)

	return true
```

- 而对应的syncHandler就是在jm初始化时传入的syncJob

#### syncJob解析

- 首先用defer做函数调用完成的耗时，用日志打印

```go
	startTime := time.Now()
	defer func() {
		klog.V(4).Infof("Finished syncing job %q (%v)", key, time.Since(startTime))
	}()
```

- 从key中分隔获取namespace和name

```go
	ns, name, err := cache.SplitMetaNamespaceKey(key)
	if err != nil {
		return false, err
	}
	if len(ns) == 0 || len(name) == 0 {
		return false, fmt.Errorf("invalid job key %q: either namespace or name is missing", key)
	}
```

- 通过informer的本地存储获取job对象，并且做一个深拷贝

```go
	sharedJob, err := jm.jobLister.Jobs(ns).Get(name)
	if err != nil {
		if apierrors.IsNotFound(err) {
			klog.V(4).Infof("Job has been deleted: %v", key)
			jm.expectations.DeleteExpectations(key)
			return true, nil
		}
		return false, err
	}
	// make a copy so we don't mutate the shared cache
	job := *sharedJob.DeepCopy()

```

- 判断job是否已处于完成状态，就是判断job的condition 是JobComplete或者JobFailed

```go
	// if job was finished previously, we don't want to redo the termination
	if IsJobFinished(&job) {
		return true, nil
	}
// IsJobFinished checks whether the given Job has finished execution.
// It does not discriminate between successful and failed terminations.
func IsJobFinished(j *batch.Job) bool {
	for _, c := range j.Status.Conditions {
		if (c.Type == batch.JobComplete || c.Type == batch.JobFailed) && c.Status == v1.ConditionTrue {
			return true
		}
	}
	return false
}

```

- 判断是否开启索引job的特性
  - 索引job就是completionMode为Indexed，pod中会被设置环境变量 JOB_COMPLETION_INDEX代表这是第几个pod
- 代码如下

```go
	// Cannot create Pods if this is an Indexed Job and the feature is disabled.
	if !feature.DefaultFeatureGate.Enabled(features.IndexedJob) && isIndexedJob(&job) {
		jm.recorder.Event(&job, v1.EventTypeWarning, "IndexedJobDisabled", "Skipped Indexed Job sync because feature is disabled.")
		return false, nil
	}
	if job.Spec.CompletionMode != nil && *job.Spec.CompletionMode != batch.NonIndexedCompletion && *job.Spec.CompletionMode != batch.IndexedCompletion {
		jm.recorder.Event(&job, v1.EventTypeWarning, "UnknownCompletionMode", "Skipped Job sync because completion mode is unknown")
		return false, nil
	}

	completionMode := string(batch.NonIndexedCompletion)
	if isIndexedJob(&job) {
		completionMode = string(batch.IndexedCompletion)
	}
	action := metrics.JobSyncActionReconciling

	defer func() {
		result := "success"
		if rErr != nil {
			result = "error"
		}

		metrics.JobSyncDurationSeconds.WithLabelValues(completionMode, result, action).Observe(time.Since(startTime).Seconds())
		metrics.JobSyncNum.WithLabelValues(completionMode, result, action).Inc()
	}()

```

- 检查是否启用了JobTrackingWithFinalizers特性，表现为 JobStatus添加有UncountedTerminatedPods字段用来追踪pod状态

```go
	var uncounted *uncountedTerminatedPods
	if trackingUncountedPods(&job) {
		klog.V(4).InfoS("Tracking uncounted Pods with pod finalizers", "job", klog.KObj(&job))
		if job.Status.UncountedTerminatedPods == nil {
			job.Status.UncountedTerminatedPods = &batch.UncountedTerminatedPods{}
		}
		uncounted = newUncountedTerminatedPods(*job.Status.UncountedTerminatedPods)
	} else if patch := removeTrackingAnnotationPatch(&job); patch != nil {
		if err := jm.patchJobHandler(&job, patch); err != nil {
			return false, fmt.Errorf("removing tracking finalizer from job %s: %w", key, err)
		}
	}

```

- 获取 job 关联的所有 pod

```go
	pods, err := jm.getPodsForJob(&job, uncounted != nil)
	if err != nil {
		return false, err
	}
```

##### getPodsForJob分析

- 首先获取job的标签选择器

```go
	selector, err := metav1.LabelSelectorAsSelector(j.Spec.Selector)
	if err != nil {
		return nil, fmt.Errorf("couldn't convert Job selector: %v", err)
	}
```

- 从本地的pod存储中获取job所在的ns的所有pod

```go
	pods, err := jm.podStore.Pods(j.Namespace).List(labels.Everything())
	if err != nil {
		return nil, err
	}
```

- 设置一个对比job的方法

```go
	canAdoptFunc := controller.RecheckDeletionTimestamp(func() (metav1.Object, error) {
		fresh, err := jm.kubeClient.BatchV1().Jobs(j.Namespace).Get(context.TODO(), j.Name, metav1.GetOptions{})
		if err != nil {
			return nil, err
		}
		if fresh.UID != j.UID {
			return nil, fmt.Errorf("original Job %v/%v is gone: got uid %v, wanted %v", j.Namespace, j.Name, fresh.UID, j.UID)
		}
		return fresh, nil
	})
```

- 初始化一个 PodControllerRefManager ，使用ClaimPods方法过滤这些pod

```go
	cm := controller.NewPodControllerRefManager(jm.podControl, j, selector, controllerKind, canAdoptFunc, finalizers...)
	// When adopting Pods, this operation adds an ownerRef and finalizers.
	pods, err = cm.ClaimPods(pods)
	if err != nil || !withFinalizers {
		return pods, err
	}
```

###### ClaimPods分析

> ClaimPods主要的作用是获取pod的属主信息

- 从调用方来看就知道是 rs 、ss、dc、job等对象
  ![claim.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/908/1636075738000/c9bd1c55cac74628b3dac3ed51f507c1.png)
- 具体流程就是遍历传入的pods
- 调用ClaimObject依次应用 match 、adopt、release方法
- 如果ok就添加到claimed中，如果出错就添加到errlist

```go
	for _, pod := range pods {
		ok, err := m.ClaimObject(pod, match, adopt, release)
		if err != nil {
			errlist = append(errlist, err)
			continue
		}
		if ok {
			claimed = append(claimed, pod)
		}
	}
```

> 首先来看match方法

- 就是根据标签选择器进行匹配，还有传入的filter方法过滤，只不过job这里没有filter方法

```go
	match := func(obj metav1.Object) bool {
		pod := obj.(*v1.Pod)
		// Check selector first so filters only run on potentially matching Pods.
		if !m.Selector.Matches(labels.Set(pod.Labels)) {
			return false
		}
		for _, filter := range filters {
			if !filter(pod) {
				return false
			}
		}
		return true
	}
```

> 然后是adopt方法

- 就是调用PodControllerRefManager的CanAdopt方法

```go

	adopt := func(obj metav1.Object) error {
		return m.AdoptPod(obj.(*v1.Pod))
	}
// AdoptPod sends a patch to take control of the pod. It returns the error if
// the patching fails.
func (m *PodControllerRefManager) AdoptPod(pod *v1.Pod) error {
	if err := m.CanAdopt(); err != nil {
		return fmt.Errorf("can't adopt Pod %v/%v (%v): %v", pod.Namespace, pod.Name, pod.UID, err)
	}
	// Note that ValidateOwnerReferences() will reject this patch if another
	// OwnerReference exists with controller=true.

	patchBytes, err := ownerRefControllerPatch(m.Controller, m.controllerKind, pod.UID, m.finalizers...)
	if err != nil {
		return err
	}
	return m.podControl.PatchPod(pod.Namespace, pod.Name, patchBytes)
}
```

- canAdoptFunc就是去apiserver中获取一下job然后跟本地缓存进行对比，如果uid发生变化返回一个错误，在CanAdopt会判断这个返回的错误决定是否可以认领

```go
	canAdoptFunc := controller.RecheckDeletionTimestamp(func() (metav1.Object, error) {
		fresh, err := jm.kubeClient.BatchV1().Jobs(j.Namespace).Get(context.TODO(), j.Name, metav1.GetOptions{})
		if err != nil {
			return nil, err
		}
		if fresh.UID != j.UID {
			return nil, fmt.Errorf("original Job %v/%v is gone: got uid %v, wanted %v", j.Namespace, j.Name, fresh.UID, j.UID)
		}
		return fresh, nil
	})
```

> 最后是release方法

- release调用ReleasePod，具体就是发送一个patch请求，从控制器上释放pod

```go
	release := func(obj metav1.Object) error {
		return m.ReleasePod(obj.(*v1.Pod))
	}
func (m *PodControllerRefManager) ReleasePod(pod *v1.Pod) error {
	klog.V(2).Infof("patching pod %s_%s to remove its controllerRef to %s/%s:%s",
		pod.Namespace, pod.Name, m.controllerKind.GroupVersion(), m.controllerKind.Kind, m.Controller.GetName())
	patchBytes, err := deleteOwnerRefStrategicMergePatch(pod.UID, m.Controller.GetUID(), m.finalizers...)
	if err != nil {
		return err
	}
	err = m.podControl.PatchPod(pod.Namespace, pod.Name, patchBytes)
	if err != nil {
		if errors.IsNotFound(err) {
			// If the pod no longer exists, ignore it.
			return nil
		}
		if errors.IsInvalid(err) {
			return nil
		}
	}
	return err
}
```

##### 回到syncjob

- 从得到的pods中统计activePods，判断标准就是pod不是PodSucceeded也不是PodFailed

```go
	activePods := controller.FilterActivePods(pods)
	active := int32(len(activePods))
```

- 使用getStatus统计succeeded, failed的pod个数
  - succeeded代表pod状态为PodSucceeded
  - failed代表pod状态为 PodFailed或者 finalizers不为空时并且pod处于deleted的状态
- getStatus代码如下

```go
	succeeded, failed := getStatus(&job, pods, uncounted)
// getStatus returns number of succeeded and failed pods running a job
func getStatus(job *batch.Job, pods []*v1.Pod, uncounted *uncountedTerminatedPods) (succeeded, failed int32) {
	if uncounted != nil {
		succeeded = job.Status.Succeeded
		failed = job.Status.Failed
	}
	succeeded += int32(countValidPodsWithFilter(job, pods, uncounted.Succeeded(), func(p *v1.Pod) bool {
		return p.Status.Phase == v1.PodSucceeded
	}))
	failed += int32(countValidPodsWithFilter(job, pods, uncounted.Failed(), func(p *v1.Pod) bool {
		if p.Status.Phase == v1.PodFailed {
			return true
		}
		// When tracking with finalizers: counting deleted Pods as failures to
		// account for orphan Pods that never have a chance to reach the Failed
		// phase.
		return uncounted != nil && p.DeletionTimestamp != nil && p.Status.Phase != v1.PodSucceeded
	}))
	return succeeded, failed
}
```

- 如果job第一次启动，并且不处于挂起状态，那么设置StartTime，并根据ActiveDeadlineSeconds启动timer。

```go
	// Job first start. Set StartTime and start the ActiveDeadlineSeconds timer
	// only if the job is not in the suspended state.
	if job.Status.StartTime == nil && !jobSuspended(&job) {
		now := metav1.Now()
		job.Status.StartTime = &now
		// enqueue a sync to check if job past ActiveDeadlineSeconds
		if job.Spec.ActiveDeadlineSeconds != nil {
			klog.V(4).Infof("Job %s has ActiveDeadlineSeconds will sync after %d seconds",
				key, *job.Spec.ActiveDeadlineSeconds)
			jm.queue.AddAfter(key, time.Duration(*job.Spec.ActiveDeadlineSeconds)*time.Second)
		}
	}

```

- 判断 job 的重启次数是否已达到上限，即处于 BackoffLimitExceeded
  - 如果根据pod 状态算出的failed数量大于缓存中的job的Status.Failed，说明有新的job有新的错误
  - exceedsBackoffLimit 的判断为 job有新的错误并且 active不等于并发度 已经失败的大于BackoffLimit
- BackoffLimit 判断代码如下

```go
	jobHasNewFailure := failed > Status.Failed
	// new failures happen when status does not reflect the failures and active
	// is different than parallelism, otherwise the previous controller loop
	// failed updating status so even if we pick up failure it is not a new one
	exceedsBackoffLimit := jobHasNewFailure && (active != *job.Spec.Parallelism) &&
		(failed > *job.Spec.BackoffLimit)
```

- 构造BackoffLimitExceeded Condition ，前提是job处于 BackoffLimitExceeded或者pod的重启次数超过了BackoffLimit

```go
	if exceedsBackoffLimit || pastBackoffLimitOnFailure(&job, pods) {
		// check if the number of pod restart exceeds backoff (for restart OnFailure only)
		// OR if the number of failed jobs increased since the last syncJob
		finishedCondition = newCondition(batch.JobFailed, v1.ConditionTrue, "BackoffLimitExceeded", "Job has reached the specified backoff limit")

```

- 或者构造DeadlineExceeded的 finishedCondition，前提是 job存活时间已经超过 ActiveDeadlineSeconds

```go
 else if pastActiveDeadline(&job) {
		finishedCondition = newCondition(batch.JobFailed, v1.ConditionTrue, "DeadlineExceeded", "Job was active longer than specified deadline")
	}
```

> 在job处于failed状态时移除ActivePods

- 根据deleteActivePods返回的结果更新 active和failed的数量

```go
	if finishedCondition != nil {
		deleted, err := jm.deleteActivePods(&job, activePods)
		if uncounted == nil {
			// Legacy behavior: pretend all active pods were successfully removed.
			deleted = active
		} else if deleted != active {
			// Can't declare the Job as finished yet, as there might be remaining
			// pod finalizers.
			finishedCondition = nil
		}
		active -= deleted
		failed += deleted
		manageJobErr = err
```

- deleteActivePods 就是并发的删除ActivePods

```go
func (jm *Controller) deleteActivePods(job *batch.Job, pods []*v1.Pod) (int32, error) {
	errCh := make(chan error, len(pods))
	successfulDeletes := int32(len(pods))
	wg := sync.WaitGroup{}
	wg.Add(len(pods))
	for i := range pods {
		go func(pod *v1.Pod) {
			defer wg.Done()
			if err := jm.podControl.DeletePod(job.Namespace, pod.Name, job); err != nil && !apierrors.IsNotFound(err) {
				atomic.AddInt32(&successfulDeletes, -1)
				errCh <- err
				utilruntime.HandleError(err)
			}
		}(pods[i])
	}
	wg.Wait()
	return successfulDeletes, errorFromChannel(errCh)
}

```

> job不在failed状态那么根据 jobNeedsSync 判断是否要进行同步

```go
		manageJobCalled := false
		if jobNeedsSync && job.DeletionTimestamp == nil {
			active, action, manageJobErr = jm.manageJob(&job, activePods, succeeded, succeededIndexes)
			manageJobCalled = true
		}
```

- 判断pod的完成情况：
  - 如果没设置Completions就是active为0且至少一个成功了
  - 如果设置了Completions就是active为0且 成功的数量大于等于Completions
- complete判断的代码如下

```go
		complete := false
		if job.Spec.Completions == nil {
			// This type of job is complete when any pod exits with success.
			// Each pod is capable of
			// determining whether or not the entire Job is done.  Subsequent pods are
			// not expected to fail, but if they do, the failure is ignored.  Once any
			// pod succeeds, the controller waits for remaining pods to finish, and
			// then the job is complete.
			complete = succeeded > 0 && active == 0
		} else {
			// Job specifies a number of completions.  This type of job signals
			// success by having that number of successes.  Since we do not
			// start more pods than there are remaining completions, there should
			// not be any remaining active pods once this count is reached.
			complete = succeeded >= *job.Spec.Completions && active == 0
		}
```

- 如果完成就更新finishedCondition

```go
		if complete {
			finishedCondition = newCondition(batch.JobComplete, v1.ConditionTrue, "", "")
```

- 如果pod succeeded大于job缓存中记录的就将forget置为true，意味着不需要重试了

```go
	forget = false
	// Check if the number of jobs succeeded increased since the last check. If yes "forget" should be true
	// This logic is linked to the issue: https://github.com/kubernetes/kubernetes/issues/56853 that aims to
	// improve the Job backoff policy when parallelism > 1 and few Jobs failed but others succeed.
	// In this case, we should clear the backoff delay.
	if job.Status.Succeeded < succeeded {
		forget = true
	}

```

> 如果 job 的 status 有变化，将 job 的 status 更新到 apiserver

```go
	// no need to update the job if the status hasn't changed since last time
	if job.Status.Active != active || job.Status.Succeeded != succeeded || job.Status.Failed != failed || suspendCondChanged || finishedCondition != nil {
		job.Status.Active = active
		job.Status.Succeeded = succeeded
		job.Status.Failed = failed
		if isIndexedJob(&job) {
			job.Status.CompletedIndexes = succeededIndexes.String()
		}
		job.Status.UncountedTerminatedPods = nil
		jm.enactJobFinished(&job, finishedCondition)

		if err := jm.updateStatusHandler(&job); err != nil {
			return forget, err
		}

		if jobHasNewFailure && !IsJobFinished(&job) {
			// returning an error will re-enqueue Job after the backoff period
			return forget, fmt.Errorf("failed pod(s) detected for job key %q", key)
		}

		forget = true
	}
```

# 本节重点总结:

- Run中会启动workersNum个 worker任务
- 每个worker任务执行syncJob，会进行如下的操作
  - 从 lister 中获取 job 对象；
  - 调用 jm.getPodsForJob 通过 selector 获取 job 关联的 pod，
  - 判断 job 的重试次数是否超过了 job.Spec.BackoffLimit(默认是6次)
  - 判断 job 的运行时间是否达到 job.Spec.ActiveDeadlineSeconds 中设定的值
  - 判断如果 job 处于 failed 状态，则调用 jm.deleteJobPods 并发删除所有 active pods ；
  - 根据 jobNeedsSync 判断是否要进行同步，若需要同步则调用 jm.manageJob 进行同步；
  - 如果 job 的 status 有变化，将 job 的 status 更新到 apiserver；