# 接上回

- 我们已经知道了istio-ingressgateway对应的二进制就是pilot-agent
- 而且从之前的逻辑中也可以看到init容器和sidecar容器使用的都是这个 pilot-agent二进制，我们在node节点上ps 可以发现众多pilot-agent进程
- 通过参数控制功能，其中 proxy route 代表ingressgateway 和egressgateway ，proxy sidecar代表 业务容器中注入的envoy

```shell
[root@k8s-node01 ~]# ps -ef |grep pilot-agent
1337      1489 26397  0 Nov15 ?        00:00:47 /usr/local/bin/pilot-agent proxy router --domain istio-system.svc.cluster.local --proxyLogLevel=warning --proxyComponentLogLevel=misc:error --log_output_level=default:info
1337      3515 27442  0 Nov15 ?        00:00:48 /usr/local/bin/pilot-agent proxy router --domain istio-system.svc.cluster.local --proxyLogLevel=warning --proxyComponentLogLevel=misc:error --log_output_level=default:info
1337      5863  4810  0 Nov15 ?        00:00:48 /usr/local/bin/pilot-agent proxy sidecar --domain default.svc.cluster.local --proxyLogLevel=warning --proxyComponentLogLevel=misc:error --log_output_level=default:info --concurrency 2
1337      5916  4814  0 Nov15 ?        00:00:47 /usr/local/bin/pilot-agent proxy sidecar --domain default.svc.cluster.local --proxyLogLevel=warning --proxyComponentLogLevel=misc:error --log_output_level=default:info --concurrency 2
1337      5985  5058  0 Nov15 ?        00:00:49 /usr/local/bin/pilot-agent proxy sidecar --domain default.svc.cluster.local --proxyLogLevel=warning --proxyComponentLogLevel=misc:error --log_output_level=default:info --concurrency 2
1337      6173  5536  0 Nov15 ?        00:00:50 /usr/local/bin/pilot-agent proxy sidecar --domain default.svc.cluster.local --proxyLogLevel=warning --proxyComponentLogLevel=misc:error --log_output_level=default:info --concurrency 2
1337      8663  7760  0 Nov15 ?        00:00:51 /usr/local/bin/pilot-agent proxy sidecar --domain default.svc.cluster.local --proxyLogLevel=warning --proxyComponentLogLevel=misc:error --log_output_level=default:info --concurrency 2
1337     10914 10412  0 Nov15 ?        00:00:52 /usr/local/bin/pilot-agent proxy sidecar --domain default.svc.cluster.local --proxyLogLevel=warning --proxyComponentLogLevel=misc:error --log_output_level=default:info --concurrency 2
1337     12552 11706  0 Nov15 ?        00:01:40 /usr/local/bin/pilot-agent proxy sidecar --domain default.svc.cluster.local --proxyLogLevel=warning --proxyComponentLogLevel=misc:error --log_output_level=default:info --concurrency 2
1337     13332 12660  0 Nov15 ?        00:00:50 /usr/local/bin/pilot-agent proxy sidecar --domain default.svc.cluster.local --proxyLogLevel=warning --proxyComponentLogLevel=misc:error --log_output_level=default:info --concurrency 2
1337     14196 13815  0 Nov15 ?        00:00:48 /usr/local/bin/pilot-agent proxy sidecar --domain default.svc.cluster.local --proxyLogLevel=warning --proxyComponentLogLevel=misc:error --log_output_level=default:info --concurrency 2
1337     14216 13615  0 Nov15 ?        00:00:51 /usr/local/bin/pilot-agent proxy sidecar --domain default.svc.cluster.local --proxyLogLevel=warning --proxyComponentLogLevel=misc:error --log_output_level=default:info --concurrency 2
1337     14427 14047  0 Nov15 ?        00:00:51 /usr/local/bin/pilot-agent proxy sidecar --domain default.svc.cluster.local --proxyLogLevel=warning --proxyComponentLogLevel=misc:error --log_output_level=default:info --concurrency 2
1337     15345 14906  0 Nov15 ?        00:00:47 /usr/local/bin/pilot-agent proxy sidecar --domain default.svc.cluster.local --proxyLogLevel=warning --proxyComponentLogLevel=misc:error --log_output_level=default:info --concurrency 2
```

- 那么ps 1个proxy router的pid 会发现 有1个envoy 的进行父进程就是这个1489，意味着istio-ingressgateway 执行cmd拉起了envoy
- 这里呼应了Dockerfile中写的内容

```shell
[root@k8s-node01 ~]# ps -ef |grep 1489
1337      1489 26397  0 Nov15 ?        00:00:55 /usr/local/bin/pilot-agent proxy router --domain istio-system.svc.cluster.local --proxyLogLevel=warning --proxyComponentLogLevel=misc:error --log_output_level=default:info
root      4111  1430  0 19:44 pts/1    00:00:00 grep --color=auto 1489
1337      5126  1489  0 Nov15 ?        00:08:08 /usr/local/bin/envoy -c etc/istio/proxy/envoy-rev0.json --restart-epoch 0 --drain-time-s 45 --drain-strategy immediate --parent-shutdown-time-s 60 --local-address-ip-version v4 --bootstrap-version 3 --file-flush-interval-msec 1000 --disable-hot-restart --log-format %Y-%m-%dT%T.%fZ?%l?envoy %n?%v -l warning --component-log-level misc:error
```

# istio-ingressgateway的作用

- 拉起一个专门的envoy处理 进入mesh的流量
- 和其它的sidecar的区别就在于 sidecar 的启动命令是 pilot-agent proxy sidecar ，gateway是 pilot-agent proxy router

# pilot-agent 的作用

- 启动envoy
- 启动grpc-server接收envoy的 ads请求
- 生成Envoy的Bootstrap配置文件；
- 监视证书的变化，通知Envoy进程热重启，实现证书的热加载；
- 提供Envoy守护功能，当Envoy异常退出的时候重启Envoy；

## 追查 pilot-agent proxy  启动流程

- 入口位置 D:\go_path\src\github.com\istio\istio\pilot\cmd\pilot-agent\main.go
- 可以首先看到的是定义了变量rootCmd，类型为cobra的command，作为root

```go
	rootCmd = &cobra.Command{
		Use:          "pilot-agent",
		Short:        "Istio Pilot agent.",
		Long:         "Istio Pilot agent runs in the sidecar or gateway container and bootstraps Envoy.",
		SilenceUsage: true,
		FParseErrWhitelist: cobra.FParseErrWhitelist{
			// Allow unknown flags for backward-compatibility.
			UnknownFlags: true,
		},
	}
```

- 然后同时定义了proxyCmd ，就是我们的proxy 子命令

```go
	proxyCmd = &cobra.Command{
		Use:   "proxy",
		Short: "XDS proxy agent",
		FParseErrWhitelist: cobra.FParseErrWhitelist{
			// Allow unknown flags for backward-compatibility.
			UnknownFlags: true,
		},
		PersistentPreRunE: configureLogging,
```

- 这两者通过 init函数中的 AddCommand关联

```go
func init() {
	rootCmd.AddCommand(proxyCmd)
	rootCmd.AddCommand(version.CobraCommand())
	rootCmd.AddCommand(iptables.GetCommand())
	rootCmd.AddCommand(cleaniptables.GetCommand())

}
```

- 所以对应的主流程就在 proxyCmd的RunE中

### proxyCmd的RunE中解析

- 由于代码很多，我们只看重点流程，RunE中最核心就是底部的agent.Run

```go
wait, err := agent.Run(ctx)
```

#### 01 启动 sds 的grpc-server

- 位置 D:\go_path\src\github.com\istio\istio\pkg\istio-agent\agent.go
- 调用链如下

```go
func (a *Agent) Run(ctx context.Context) (func(), error) {
   a.sdsServer = sds.NewServer(a.secOpts, a.secretCache)
}
// NewServer creates and starts the Grpc server for SDS.
func NewServer(options *security.Options, workloadSecretCache security.SecretManager) *Server {
	s := &Server{stopped: atomic.NewBool(false)}
	s.workloadSds = newSDSService(workloadSecretCache, options)
	s.initWorkloadSdsService(options)
	sdsServiceLog.Infof("SDS server for workload certificates started, listening on %q", options.WorkloadUDSPath)
	return s
}
```

##### sds 服务的意义

- SDS 带来的最大的好处就是简化证书管理
- 要是没有该功能的话，我们就必须使用 Kubernetes 中的 secret 资源创建证书，然后把证书挂载到代理容器中
- 如果证书过期，还需要更新 secret 和需要重新部署代理容器
- 使用 SDS，中央 SDS 服务器将证书推送到所有 Envoy 实例上。如果证书过期，服务器只需将新证书推送到 Envoy 实例，Envoy 可以立即使用新证书而无需重新部署。

> 获取不到证书的话listener将被标记为active

- 如果 listener server 需要从远程获取证书，则 listener server 不会被标记为 active 状态，在获取证书之前不会打开其端口。如果 Envoy 由于连接失败或错误的响应数据而无法获取证书，则 listener server 将被标记为 active，并且打开端口，但是将重置与端口的连接。

#### 02 启动 xds 的grpc-server 提供 AggregatedDiscoveryService

- initXdsProxy 作为初始化xdsProxy的入口

```go
	a.xdsProxy, err = initXdsProxy(a)
	if err != nil {
		return nil, fmt.Errorf("failed to start xds proxy: %v", err)
	}

func initXdsProxy(ia *Agent) (*XdsProxy, error) {
	if err = proxy.initDownstreamServer(); err != nil {
		return nil, err
	}
	go func() {
		if err := proxy.downstreamGrpcServer.Serve(proxy.downstreamListener); err != nil {
			log.Errorf("failed to accept downstream gRPC connection %v", err)
		}
	}()
}
```

- 对应的grpc注册的地方在initDownstreamServer中，这里看到RegisterAggregatedDiscoveryServiceServer代表提供 ads聚合增量服务

```go
func (p *XdsProxy) initDownstreamServer() error {
	l, err := uds.NewListener(p.xdsUdsPath)
	if err != nil {
		return err
	}
	// TODO: Expose keepalive options to agent cmd line flags.
	opts := p.downstreamGrpcOptions
	opts = append(opts, istiogrpc.ServerOptions(istiokeepalive.DefaultOption())...)
	grpcs := grpc.NewServer(opts...)
	discovery.RegisterAggregatedDiscoveryServiceServer(grpcs, p)
	reflection.Register(grpcs)
	p.downstreamGrpcServer = grpcs
	p.downstreamListener = l
	return nil
}
```

- 对应的grpc-service 注册的地方在 D:\go_path\pkg\mod\github.com\envoyproxy\go-control-plane@v0.9.10-0.20210907150352-cf90f659a021\envoy\service\discovery\v3\ads.pb.go
- 这里可以看到 Stream 代表全量，Delta代表增量，AggregatedResources代表多种资源聚合而不是单一资源

```go
var _AggregatedDiscoveryService_serviceDesc = grpc.ServiceDesc{
	ServiceName: "envoy.service.discovery.v3.AggregatedDiscoveryService",
	HandlerType: (*AggregatedDiscoveryServiceServer)(nil),
	Methods:     []grpc.MethodDesc{},
	Streams: []grpc.StreamDesc{
		{
			StreamName:    "StreamAggregatedResources",
			Handler:       _AggregatedDiscoveryService_StreamAggregatedResources_Handler,
			ServerStreams: true,
			ClientStreams: true,
		},
		{
			StreamName:    "DeltaAggregatedResources",
			Handler:       _AggregatedDiscoveryService_DeltaAggregatedResources_Handler,
			ServerStreams: true,
			ClientStreams: true,
		},
	},
	Metadata: "envoy/service/discovery/v3/ads.proto",
}

```

#### 03 分析 pilot-agent 启动  启动 xds grpc-server的数据来源

- pilot-agent 启动grpc-server 下对envoy ，同时还要作为grpc-agent 请求istiod 获取数据，具体流程如下
- 请求istiod的 ticker 在initXdsProxy函数的底部 ，D:\go_path\src\github.com\istio\istio\pkg\istio-agent\xds_proxy.go

```go
	go proxy.healthChecker.PerformApplicationHealthCheck(func(healthEvent *health.ProbeEvent) {
		// Store the same response as Delta and SotW. Depending on how Envoy connects we will use one or the other.
		var req *discovery.DiscoveryRequest
		if healthEvent.Healthy {
			req = &discovery.DiscoveryRequest{TypeUrl: v3.HealthInfoType}
		} else {
			req = &discovery.DiscoveryRequest{
				TypeUrl: v3.HealthInfoType,
				ErrorDetail: &google_rpc.Status{
					Code:    int32(codes.Internal),
					Message: healthEvent.UnhealthyMessage,
				},
			}
		}
		proxy.PersistRequest(req)
		var deltaReq *discovery.DeltaDiscoveryRequest
		if healthEvent.Healthy {
			deltaReq = &discovery.DeltaDiscoveryRequest{TypeUrl: v3.HealthInfoType}
		} else {
			deltaReq = &discovery.DeltaDiscoveryRequest{
				TypeUrl: v3.HealthInfoType,
				ErrorDetail: &google_rpc.Status{
					Code:    int32(codes.Internal),
					Message: healthEvent.UnhealthyMessage,
				},
			}
		}
		proxy.PersistDeltaRequest(deltaReq)
	}, proxy.stopChan)
```

- 分析一下 proxy.healthChecker.PerformApplicationHealthCheck只有1个参数就是上面的func，代表一个回调函数
- 其中 PerformApplicationHealthCheck的逻辑可以简化为

```go
func (w *WorkloadHealthChecker) PerformApplicationHealthCheck(callback func(*ProbeEvent), quit chan struct{}) {
    // 定义一个 doCheck的动作
    doCheck := func() {
    // 进行探活
    healthy, err := w.prober.Probe(w.config.ProbeTimeout)
    // 执行外部的回调
    callback(&ProbeEvent{Healthy: true})
    }
	// 然后周期性执行doCheck
	doCheck()
	periodTicker := time.NewTicker(w.config.CheckFrequency)
	defer periodTicker.Stop()
	for {
		select {
		case <-quit:
			return
		case <-periodTicker.C:
			doCheck()
		}
	}
}
```

- 那么这个回调就是构造 DiscoveryRequest 和DeltaDiscoveryRequest并执行
- 那DeltaDiscoveryRequest举例，具体的执行方法就是 将这个req 放入 ch，ch对应的就是p.connected.deltaRequestsChan

```go
func (p *XdsProxy) PersistDeltaRequest(req *discovery.DeltaDiscoveryRequest) {
	var ch chan *discovery.DeltaDiscoveryRequest
	var stop chan struct{}

	p.connectedMutex.Lock()
	if p.connected != nil {
		ch = p.connected.deltaRequestsChan
		stop = p.connected.stopChan
	}
	p.initialDeltaRequest = req
	p.connectedMutex.Unlock()

	// Immediately send if we are currently connect
	if ch != nil {
		select {
		case ch <- req:
		case <-stop:
		}
	}
}
```

- 那么 谁来消费这个p.connected.deltaRequestsChan，就是执行的动作，位置在 2
- 可以看到往下调用就是sendUpstreamDelta 中的istiogrpc.Send，调用istiod 的grpc服务获取数据

```go
func (p *XdsProxy) handleUpstreamDeltaRequest(con *ProxyConnection) {
	defer func() {
		_ = con.upstreamDeltas.CloseSend()
	}()
	for {
		select {
		case req := <-con.deltaRequestsChan:
			proxyLog.Debugf("delta request for type url %s", req.TypeUrl)
			metrics.XdsProxyRequests.Increment()
			if req.TypeUrl == v3.ExtensionConfigurationType {
				p.ecdsLastNonce.Store(req.ResponseNonce)
			}
			if err := sendUpstreamDelta(con.upstreamDeltas, req); err != nil {
				proxyLog.Errorf("upstream send error for type url %s: %v", req.TypeUrl, err)
				con.upstreamError <- err
				return
			}
		case <-con.stopChan:
			return
		}
	}
}

func sendUpstreamDelta(deltaUpstream discovery.AggregatedDiscoveryService_DeltaAggregatedResourcesClient,
	req *discovery.DeltaDiscoveryRequest) error {
	return istiogrpc.Send(deltaUpstream.Context(), func() error { return deltaUpstream.Send(req) })
}

```

- 那么网上追踪这个 handleUpstreamDeltaRequest 可以发现在HandleDeltaUpstream中会根据con对象处理请求和响应

```go
func (p *XdsProxy) HandleDeltaUpstream(ctx context.Context, con *ProxyConnection, xds discovery.AggregatedDiscoveryServiceClient) error {
	go p.handleUpstreamDeltaRequest(con)
	go p.handleUpstreamDeltaResponse(con)


}
```

- 在handleUpstreamDeltaResponse中可以发现回复ack 和sendToEnvoy 的逻辑(grpc双工？)

```go
func (p *XdsProxy) handleUpstreamDeltaResponse(con *ProxyConnection) {
	for {
		select {
		case resp := <-con.deltaResponsesChan:
			// TODO: separate upstream response handling from requests sending, which are both time costly
			proxyLog.Debugf("response for type url %s", resp.TypeUrl)
			metrics.XdsProxyResponses.Increment()
			if h, f := p.handlers[resp.TypeUrl]; f {
				if len(resp.Resources) == 0 {
					// Empty response, nothing to do
					// This assumes internal types are always singleton
					return
				}
				err := h(resp.Resources[0].Resource)
				var errorResp *google_rpc.Status
				if err != nil {
					errorResp = &google_rpc.Status{
						Code:    int32(codes.Internal),
						Message: err.Error(),
					}
				}
				// Send ACK/NACK
				con.sendDeltaRequest(&discovery.DeltaDiscoveryRequest{
					TypeUrl:       resp.TypeUrl,
					ResponseNonce: resp.Nonce,
					ErrorDetail:   errorResp,
				})
				continue
			}
			switch resp.TypeUrl {
			case v3.ExtensionConfigurationType:
				if features.WasmRemoteLoadConversion {
					// If Wasm remote load conversion feature is enabled, rewrite and send.
					go p.deltaRewriteAndForward(con, resp)
				} else {
					// Otherwise, forward ECDS resource update directly to Envoy.
					forwardDeltaToEnvoy(con, resp)
				}
			default:
				forwardDeltaToEnvoy(con, resp)
			}
		case <-con.stopChan:
			return
		}
	}
}
```

#### 04 初始化envoy对象

- 回到我们的Run中

```go
func (a *Agent) Run(ctx context.Context) (func(), error) {
	if !a.EnvoyDisabled() {
		err = a.initializeEnvoyAgent(ctx)
		if err != nil {
			return nil, fmt.Errorf("failed to start envoy agent: %v", err)
		}


}
```

#### 05 启动envoy

- 这里看到就是封装 envoy的命令行 参数，然后通过exec启动

```go
func (a *Agent) Run(ctx context.Context) {
	log.Info("Starting proxy agent")
	go a.runWait(0, a.abortCh)
}
// runWait runs the start-up command as a go routine and waits for it to finish
func (a *Agent) runWait(epoch int, abortCh <-chan error) {
	log.Infof("Epoch %d starting", epoch)
	err := a.proxy.Run(epoch, abortCh)
	a.proxy.Cleanup(epoch)
	a.statusCh <- exitStatus{epoch: epoch, err: err}
}

func (e *envoy) Run(epoch int, abort <-chan error) error {
	// spin up a new Envoy process
	args := e.args(e.ConfigPath, epoch, istioBootstrapOverrideVar.Get())
	log.Infof("Envoy command: %v", args)

	/* #nosec */
	cmd := exec.Command(e.BinaryPath, args...)
	cmd.Stdout = os.Stdout
	cmd.Stderr = os.Stderr
	if e.AgentIsRoot {
		cmd.SysProcAttr = &syscall.SysProcAttr{}
		cmd.SysProcAttr.Credential = &syscall.Credential{
			Uid: 1337,
			Gid: 1337,
		}
	}

	if err := cmd.Start(); err != nil {
		return err
	}
	done := make(chan error, 1)
	go func() {
		done <- cmd.Wait()
	}()

	select {
	case err := <-abort:
		log.Warnf("Aborting epoch %d", epoch)
		if errKill := cmd.Process.Kill(); errKill != nil {
			log.Warnf("killing epoch %d caused an error %v", epoch, errKill)
		}
		return err
	case err := <-done:
		return err
	}
}

```

# 那么 istio-ingressgateway和istio-proxy的区别在哪里

- 我们知道这两个app底层都是 pilot-agent proxy启动的，只不过1个子命令是 router 1个是sidecar
- 从上面的代码流程上好像看不出什么区别，都是拉取envoy处理流量，下面我们来通过查看istio-ingressgateway和istio-proxy的 envoy 配置查看

## 01 查看istio-ingressgateway的envoy 配置

- exec到pod内部 ps 查看发现有 1个envoy进程和1个pilot-agent进程

```go
[root@k8s-master01 ~]# kubectl get pod -n istio-system -l app=istio-ingressgateway                       

NAME                                    READY   STATUS    RESTARTS   AGE
istio-ingressgateway-8577c57fb6-fbkl8   1/1     Running   0          2d22h
[root@k8s-master01 ~]# 
[root@k8s-master01 ~]# kubectl -n istio-system exec istio-ingressgateway-8577c57fb6-fbkl8  -ti   -- /bin/bash
istio-proxy@istio-ingressgateway-8577c57fb6-fbkl8:/$ 
istio-proxy@istio-ingressgateway-8577c57fb6-fbkl8:/$ ps auxf
USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
istio-p+   456  0.0  0.0   4236  2164 pts/0    Ss   10:39   0:00 /bin/bash
istio-p+   465  0.0  0.0   5892  1516 pts/0    R+   10:40   0:00  \_ ps auxf
istio-p+     1  0.0  0.2 751056 45208 ?        Ssl  Nov15   3:25 /usr/local/bin/pilot-agent proxy router --domain istio-system.svc.cluster.local --proxyLogLevel=warning --proxyComponentLogLevel=misc:error --log_output_level=default:i
istio-p+    22  0.6  0.3 331596 53396 ?        Sl   Nov15  27:38 /usr/local/bin/envoy -c etc/istio/proxy/envoy-rev0.json --restart-epoch 0 --drain-time-s 45 --drain-strategy immediate --parent-shutdown-time-s 60 --local-address-ip-ve
istio-proxy@istio-ingressgateway-8577c57fb6-fbkl8:/$ 
```

- 同时查看 envoy中的admin端口发现是 127.0.0.1:15000

```shell
istio-proxy@istio-ingressgateway-8577c57fb6-fbkl8:/$ grep admin /etc/istio/proxy/envoy-rev0.json -A 10
              "name": "admin",
              "admin_layer": {}
          }
      ]
  },
  "stats_config": {
    "use_all_default_tags": false,
    "stats_tags": [
      {
        "tag_name": "cluster_name",
        "regex": "^cluster\\.((.+?(\\..+?\\.svc\\.cluster\\.local)?)\\.)"
      },
--
  "admin": {
    "access_log_path": "/dev/null",
    "profile_path": "/var/lib/istio/data/envoy.prof",
    "address": {
      "socket_address": {
        "address": "127.0.0.1",
        "port_value": 15000
      }
    }
  },
  "dynamic_resources": {
```

- 通过admin的 config_dump 可以查看动态的配置，这里我们grep  dynamic_route_configs 查看动态路由 RDS服务

```shell
istio-proxy@istio-ingressgateway-8577c57fb6-fbkl8:/$ curl  localhost:15000/config_dump  |grep dynamic_route_configs -A 100 
   "dynamic_route_configs": [
    {
     "version_info": "2021-11-15T14:54:28Z/3",
     "route_config": {
      "@type": "type.googleapis.com/envoy.config.route.v3.RouteConfiguration",
      "name": "http.8080",
      "virtual_hosts": [
       {
        "name": "*:80",
        "domains": [
         "*"
        ],
        "routes": [
         {
          "match": {
           "path": "/productpage",
           "case_sensitive": true
          },
          "route": {
           "cluster": "outbound|9080||productpage.default.svc.cluster.local",
           "timeout": "0s",
           "retry_policy": {
            "retry_on": "connect-failure,refused-stream,unavailable,cancelled,retriable-status-codes",
            "num_retries": 2,
            "retry_host_predicate": [
             {
              "name": "envoy.retry_host_predicates.previous_hosts"
             }
            ],
            "host_selection_retry_max_attempts": "5",
            "retriable_status_codes": [
             503
            ]
           },
           "max_grpc_timeout": "0s"
          },
          "metadata": {
           "filter_metadata": {
            "istio": {
             "config": "/apis/networking.istio.io/v1alpha3/namespaces/default/virtual-service/bookinfo"
            }
           }
          },
          "decorator": {
           "operation": "productpage.default.svc.cluster.local:9080/productpage"
          }
         },
         {
          "match": {
           "prefix": "/static",
           "case_sensitive": true
          },
          "route": {
           "cluster": "outbound|9080||productpage.default.svc.cluster.local",
           "timeout": "0s",
           "retry_policy": {
            "retry_on": "connect-failure,refused-stream,unavailable,cancelled,retriable-status-codes",
            "num_retries": 2,
            "retry_host_predicate": [
             {
              "name": "envoy.retry_host_predicates.previous_hosts"
             }
            ],
            "host_selection_retry_max_attempts": "5",
            "retriable_status_codes": [
             503
            ]
           },
           "max_grpc_timeout": "0s"
          },
          "metadata": {
           "filter_metadata": {
            "istio": {
             "config": "/apis/networking.istio.io/v1alpha3/namespaces/default/virtual-service/bookinfo"
            }
           }
          },
          "decorator": {
           "operation": "productpage.default.svc.cluster.local:9080/static*"
          }
         },
         {
          "match": {
           "path": "/login",
           "case_sensitive": true
          },
          "route": {
           "cluster": "outbound|9080||productpage.default.svc.cluster.local",
           "timeout": "0s",
           "retry_policy": {
            "retry_on": "connect-failure,refused-stream,unavailable,cancelled,retriable-status-codes",
            "num_retries": 2,
            "retry_host_predicate": [
             {
              "name": "envoy.retry_host_predicates.previous_hosts"
             }
            ],
            "host_selection_retry_max_attempts": "5",
            "retriable_status_codes": [
             503
            ]
```

- 看着熟悉么，是不是就是我们部署bookinfo 时部署的[bookinfo-gateway](https://istio.io/latest/zh/docs/setup/getting-started/#ip)

```shell
此时，BookInfo 应用已经部署，但还不能被外界访问。 要开放访问，你需要创建 Istio 入站网关（Ingress Gateway）, 它会在网格边缘把一个路径映射到路由。

把应用关联到 Istio 网关：

$ kubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml
gateway.networking.istio.io/bookinfo-gateway created
virtualservice.networking.istio.io/bookinfo created
```

- 观察bookinfo-gateway.yaml

```yaml
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: bookinfo-gateway
spec:
  selector:
    istio: ingressgateway # use istio default controller
  servers:
  - port:
      number: 80
      name: http
      protocol: HTTP
    hosts:
    - "*"
---
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: bookinfo
spec:
  hosts:
  - "*"
  gateways:
  - bookinfo-gateway
  http:
  - match:
    - uri:
        exact: /productpage
    - uri:
        prefix: /static
    - uri:
        exact: /login
    - uri:
        exact: /logout
    - uri:
        prefix: /api/v1/products
    route:
    - destination:
        host: productpage
        port:
          number: 9080
```

- 比如route_config对应的 bookinfo-gateway Gateway

```shell
     "route_config": {
      "@type": "type.googleapis.com/envoy.config.route.v3.RouteConfiguration",
      "name": "http.8080",
      "virtual_hosts": [
       {
        "name": "*:80",
        "domains": [
         "*"
        ],
        "routes": [
         {
          "match": {
           "path": "/productpage",
           "case_sensitive": true
          },
```

- /static 对应转发到 productpage 的svc的9080端口

```shell
         {
          "match": {
           "prefix": "/static",
           "case_sensitive": true
          },
          "route": {
           "cluster": "outbound|9080||productpage.default.svc.cluster.local",
           "timeout": "0s",
           "retry_policy": {
            "retry_on": "connect-failure,refused-stream,unavailable,cancelled,retriable-status-codes",
            "num_retries": 2,
            "retry_host_predicate": [
             {
              "name": "envoy.retry_host_predicates.previous_hosts"
             }
            ],
            "host_selection_retry_max_attempts": "5",
            "retriable_status_codes": [
             503
            ]
           },
           "max_grpc_timeout": "0s"
          },
```

- 使用下面的命令把 50% 的流量从 reviews:v1 转移到 reviews:v3：

```shell
kubectl apply -f samples/bookinfo/networking/virtual-service-reviews-50-v3.yaml
```

- 我们来解读一下这个基于 权重的路由，看起来就是将到 reviews的流量分别给 v1和v3各 50%

```yaml
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: reviews
spec:
  hosts:
    - reviews
  http:
  - route:
    - destination:
        host: reviews
        subset: v1
      weight: 50
    - destination:
        host: reviews
        subset: v3
      weight: 50

```

- 测试新的路由配置，在浏览器中打开 Bookinfo 站点 访问http://172.20.70.215:19257/productpage
- 然后这时候exec到productpage pod中查看envoy配置，直接grep outbound|80|v1|reviews.default.svc.cluster.local，因为这时候我们有1半的流量 会给reviews:v1

```shell
[root@k8s-master01 ~]# kubectl get pod |grep productpage                                         
productpage-v1-6b746f74dc-7mm7m           2/2     Running   0          2d20h
[root@k8s-master01 ~]# kubectl exec productpage-v1-6b746f74dc-7mm7m  -c istio-proxy -ti  -- /bin/bash
istio-proxy@productpage-v1-6b746f74dc-7mm7m:/$ 

istio-proxy@productpage-v1-6b746f74dc-7mm7m:/$ curl -s localhost:15000/config_dump  |grep "outbound|80|v1|reviews.default.svc.cluster.local" -C 50
          "match": {
           "prefix": "/"
          },
          "route": {
           "cluster": "outbound|80|v1|ratings.default.svc.cluster.local",
           "timeout": "0s",
           "retry_policy": {
            "retry_on": "connect-failure,refused-stream,unavailable,cancelled,retriable-status-codes",
            "num_retries": 2,
            "retry_host_predicate": [
             {
              "name": "envoy.retry_host_predicates.previous_hosts"
             }
            ],
            "host_selection_retry_max_attempts": "5",
            "retriable_status_codes": [
             503
            ]
           },
           "max_grpc_timeout": "0s"
          },
          "metadata": {
           "filter_metadata": {
            "istio": {
             "config": "/apis/networking.istio.io/v1alpha3/namespaces/default/virtual-service/ratings"
            }
           }
          },
          "decorator": {
           "operation": "ratings.default.svc.cluster.local:80/*"
          }
         }
        ],
        "include_request_attempt_count": true
       },
       {
        "name": "reviews.default.svc.cluster.local:80",
        "domains": [
         "reviews.default.svc.cluster.local",
         "reviews.default.svc.cluster.local:80"
        ],
        "routes": [
         {
          "match": {
           "prefix": "/"
          },
          "route": {
           "weighted_clusters": {
            "clusters": [
             {
              "name": "outbound|80|v1|reviews.default.svc.cluster.local",
              "weight": 50
             },
             {
              "name": "outbound|80|v3|reviews.default.svc.cluster.local",
              "weight": 50
             }
            ]
           },
           "timeout": "0s",
           "retry_policy": {
            "retry_on": "connect-failure,refused-stream,unavailable,cancelled,retriable-status-codes",
            "num_retries": 2,
            "retry_host_predicate": [
             {
              "name": "envoy.retry_host_predicates.previous_hosts"
             }
            ],
            "host_selection_retry_max_attempts": "5",
            "retriable_status_codes": [
             503
            ]
           },
           "max_grpc_timeout": "0s"
          },
          "metadata": {
           "filter_metadata": {
            "istio": {
             "config": "/apis/networking.istio.io/v1alpha3/namespaces/default/virtual-service/reviews"
            }
           }
          },
          "decorator": {
           "operation": "reviews:80/*"
          }
         }
        ],
        "include_request_attempt_count": true
       },
       {
        "name": "sleep.default.svc.cluster.local:80",
        "domains": [
         "sleep.default.svc.cluster.local",
         "sleep.default.svc.cluster.local:80",
         "sleep",
         "sleep:80",
         "sleep.default.svc",
         "sleep.default.svc:80",
         "sleep.default",
         "sleep.default:80",
         "10.96.118.138",
```

- 同时我们在ingressgateway pod中查看这条规范，发现不存在，说明什么 ingressgateway和sidecar 拉取的envoy配置不一样

```shell
istio-proxy@istio-ingressgateway-8577c57fb6-fbkl8:/$  curl -s localhost:15000/config_dump  |grep "outbound|80|v1|reviews.default.svc.cluster.local" -C 50
istio-proxy@istio-ingressgateway-8577c57fb6-fbkl8:/$ 
istio-proxy@istio-ingressgateway-8577c57fb6-fbkl8:/$ 
```

# 本节重点总结

- pilot-agent 启动grpc-server 下对envoy ，同时还要作为grpc-agent 请求istiod 获取数据
- 我们知道istio-ingressgateway和istio-proxy两个app底层都是 pilot-agent proxy启动的，只不过1个子命令是 router 1个是sidecar
- 从上面的代码流程上好像看不出什么区别，都是拉取envoy处理流量，下面我们来通过查看istio-ingressgateway和istio-proxy的 envoy 配置查看
- 说明什么 ingressgateway和sidecar 拉取的envoy配置不一样