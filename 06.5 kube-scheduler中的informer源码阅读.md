# 在kube-scheduler中的源码解读

## 初始化shardinformer

- 入口在kube-scheduler的config中，位置 D:\go_path\src\github.com\kubernetes\kubernetes\cmd\kube-scheduler\app\options\options.go
- 这里传入的resync=0，代表不进行周期性的list，而是通过第一次的全量list+增量的更新

```go
c.InformerFactory = scheduler.NewInformerFactory(client, 0)
informerFactory := informers.NewSharedInformerFactory(cs, resyncPeriod)
```

- NewSharedInformerFactory方法最终会调用到NewSharedInformerFactoryWithOptions初始化一个sharedInformerFactory ，位置D:\go_path\src\github.com\kubernetes\kubernetes\staging\src\k8s.io\client-go\informers\factory.go

```go
// NewSharedInformerFactoryWithOptions constructs a new instance of a SharedInformerFactory with additional options.
func NewSharedInformerFactoryWithOptions(client kubernetes.Interface, defaultResync time.Duration, options ...SharedInformerOption) SharedInformerFactory {
	factory := &sharedInformerFactory{
		client:           client,
		namespace:        v1.NamespaceAll,
		defaultResync:    defaultResync,
		informers:        make(map[reflect.Type]cache.SharedIndexInformer),
		startedInformers: make(map[reflect.Type]bool),
		customResync:     make(map[reflect.Type]time.Duration),
	}

	// Apply all options
	for _, opt := range options {
		factory = opt(factory)
	}

	return factory
}

```

### 为何叫sharedInformer

- sharedInformerFactory字段如下

```go
type sharedInformerFactory struct {
	client           kubernetes.Interface
	namespace        string
	tweakListOptions internalinterfaces.TweakListOptionsFunc
	lock             sync.Mutex
	defaultResync    time.Duration
	customResync     map[reflect.Type]time.Duration

	informers map[reflect.Type]cache.SharedIndexInformer
	// startedInformers is used for tracking which informers have been started.
	// This allows Start() to be called multiple times safely.
	startedInformers map[reflect.Type]bool
}

```

- 其中最重要的就是 informers这个map，根据资源的类型更新对应的informer
- 一种资源会对应多个Informer，会导致效率低下，所以让一个资源对应一个sharedInformer，而一个sharedInformer内部自己维护多个Informer

## pod-informer初始化

- 位置 D:\go_path\src\github.com\kubernetes\kubernetes\pkg\scheduler\scheduler.go

```go
// NewInformerFactory creates a SharedInformerFactory and initializes a scheduler specific
// in-place podInformer.
func NewInformerFactory(cs clientset.Interface, resyncPeriod time.Duration) informers.SharedInformerFactory {
	informerFactory := informers.NewSharedInformerFactory(cs, resyncPeriod)
	informerFactory.InformerFor(&v1.Pod{}, newPodInformer)
	return informerFactory
}

```

- 我们可以看到使用informerFactory的InformerFor创建了pod的informer对象
- 具体的InformerFor为sharedInformerFactory的InformerFor ，位置在  D:\go_path\src\github.com\kubernetes\kubernetes\staging\src\k8s.io\client-go\informers\factory.go

```go
// InternalInformerFor returns the SharedIndexInformer for obj using an internal
// client.
func (f *sharedInformerFactory) InformerFor(obj runtime.Object, newFunc internalinterfaces.NewInformerFunc) cache.SharedIndexInformer {
	f.lock.Lock()
	defer f.lock.Unlock()

	informerType := reflect.TypeOf(obj)
	informer, exists := f.informers[informerType]
	if exists {
		return informer
	}

	resyncPeriod, exists := f.customResync[informerType]
	if !exists {
		resyncPeriod = f.defaultResync
	}

	informer = newFunc(f.client, resyncPeriod)
	f.informers[informerType] = informer

	return informer
}

```

> InformerFor解读

- 根据obj的反射类型在 informers map中寻找informer
- 如果找到就返回，找不到就使用传入的newFunc 创建一个，并更新map
- 对应的 newFunc就是 newPodInformer，位置 D:\go_path\src\github.com\kubernetes\kubernetes\pkg\scheduler\scheduler.go

```go
// newPodInformer creates a shared index informer that returns only non-terminal pods.
func newPodInformer(cs clientset.Interface, resyncPeriod time.Duration) cache.SharedIndexInformer {
	selector := fmt.Sprintf("status.phase!=%v,status.phase!=%v", v1.PodSucceeded, v1.PodFailed)
	tweakListOptions := func(options *metav1.ListOptions) {
		options.FieldSelector = selector
	}
	return coreinformers.NewFilteredPodInformer(cs, metav1.NamespaceAll, resyncPeriod, nil, tweakListOptions)
}

```

- 底层的newPod在client-go中，位置 D:\go_path\src\github.com\kubernetes\kubernetes\staging\src\k8s.io\client-go\informers\core\v1\pod.go
- 在其中能看到对应的 listFunc和WatchFunc

```go
// NewFilteredPodInformer constructs a new informer for Pod type.
// Always prefer using an informer factory to get a shared informer instead of getting an independent
// one. This reduces memory footprint and number of connections to the server.
func NewFilteredPodInformer(client kubernetes.Interface, namespace string, resyncPeriod time.Duration, indexers cache.Indexers, tweakListOptions internalinterfaces.TweakListOptionsFunc) cache.SharedIndexInformer {
	return cache.NewSharedIndexInformer(
		&cache.ListWatch{
			ListFunc: func(options metav1.ListOptions) (runtime.Object, error) {
				if tweakListOptions != nil {
					tweakListOptions(&options)
				}
				return client.CoreV1().Pods(namespace).List(context.TODO(), options)
			},
			WatchFunc: func(options metav1.ListOptions) (watch.Interface, error) {
				if tweakListOptions != nil {
					tweakListOptions(&options)
				}
				return client.CoreV1().Pods(namespace).Watch(context.TODO(), options)
			},
		},
		&corev1.Pod{},
		resyncPeriod,
		indexers,
	)
}
```

## Indexer的创建

- 上面我们提到在 NewSharedIndexInformer中会新建indexer存储
- 位置 D:\go_path\src\github.com\kubernetes\kubernetes\staging\src\k8s.io\client-go\tools\cache\store.go

```go
// NewIndexer returns an Indexer implemented simply with a map and a lock.
func NewIndexer(keyFunc KeyFunc, indexers Indexers) Indexer {
	return &cache{
		cacheStorage: NewThreadSafeStore(indexers, Indices{}),
		keyFunc:      keyFunc,
	}
}

```

- 对应的底层数据结构为 threadSafeMap ，位置 D:\go_path\src\github.com\kubernetes\kubernetes\staging\src\k8s.io\client-go\tools\cache\thread_safe_store.go

```go
// threadSafeMap implements ThreadSafeStore
type threadSafeMap struct {
	lock  sync.RWMutex
	items map[string]interface{}  // 代表 对象的缓存，key是 namespace/name ，value就是对象

	// indexers maps a name to an IndexFunc
	indexers Indexers     // 是索引的方法
	// indices maps a name to an Index
	indices Indices      // 是索引数据
}

```

- 位置D:\go_path\src\github.com\kubernetes\kubernetes\staging\src\k8s.io\client-go\tools\cache\index.go

```go
// Indices maps a name to an Index
type Indices map[string]Index
// Index maps the indexed value to a set of keys in the store that match on that value
type Index map[string]sets.String

// sets.String is a set of strings, implemented via map[string]struct{} for minimal memory consumption.
type String map[string]Empty

type Empty struct{}
```

- indices数据结构，看起来是三层的map，key 都是string
- 看起来 threadSafeMap.items存储具体的资源对象，indices是索引，加速查找

### NewIndexer时传入的keyFunc

- 使用的是MetaNamespaceKeyFunc，也就是对象的namespace/name
- 位置 D:\go_path\src\github.com\kubernetes\kubernetes\staging\src\k8s.io\client-go\tools\cache\store.go

```go
func MetaNamespaceKeyFunc(obj interface{}) (string, error) {
	if key, ok := obj.(ExplicitKey); ok {
		return string(key), nil
	}
	meta, err := meta.Accessor(obj)
	if err != nil {
		return "", fmt.Errorf("object has no meta: %v", err)
	}
	if len(meta.GetNamespace()) > 0 {
		return meta.GetNamespace() + "/" + meta.GetName(), nil
	}
	return meta.GetName(), nil
}

```

## 添加eventHandler

- 在 scheduler 的New中，addAllEventHandlers
- 这些handler代表就是使用方能干些什么，informer拿到之后要更新存储，使用方如scheduler拿到后要调度pod
- 位置 D:\go_path\src\github.com\kubernetes\kubernetes\pkg\scheduler\scheduler.go

```go
addAllEventHandlers(sched, informerFactory, dynInformerFactory, unionedGVKs(clusterEventMap))
```

- 添加调度pod时的回调，有对应的AddFunc、UpdateFunc、DeleteFunc

```go
	informerFactory.Core().V1().Pods().Informer().AddEventHandler(
		cache.FilteringResourceEventHandler{
			FilterFunc: func(obj interface{}) bool {
				switch t := obj.(type) {
				case *v1.Pod:
					return assignedPod(t)
				case cache.DeletedFinalStateUnknown:
					if pod, ok := t.Obj.(*v1.Pod); ok {
						return assignedPod(pod)
					}
					utilruntime.HandleError(fmt.Errorf("unable to convert object %T to *v1.Pod in %T", obj, sched))
					return false
				default:
					utilruntime.HandleError(fmt.Errorf("unable to handle object in %T: %T", sched, obj))
					return false
				}
			},
			Handler: cache.ResourceEventHandlerFuncs{
				AddFunc:    sched.addPodToCache,
				UpdateFunc: sched.updatePodInCache,
				DeleteFunc: sched.deletePodFromCache,
			},
		},
	)
```

## 启动informers

- 入口在 scheduler的run中，位置 D:\go_path\src\github.com\kubernetes\kubernetes\cmd\kube-scheduler\app\server.go

```go
	// Start all informers.
	cc.InformerFactory.Start(ctx.Done())

	// Wait for all caches to sync before scheduling.
	cc.InformerFactory.WaitForCacheSync(ctx.Done())
```

- 对应的start为sharedInformerFactory中，遍历informers map，启动每个informer。并且在启动前用startedInformers map做check

```go
// Start initializes all requested informers.
func (f *sharedInformerFactory) Start(stopCh <-chan struct{}) {
	f.lock.Lock()
	defer f.lock.Unlock()

	for informerType, informer := range f.informers {
		if !f.startedInformers[informerType] {
			go informer.Run(stopCh)
			f.startedInformers[informerType] = true
		}
	}
}
```

### sharedIndexInformer Run解读

- 新建fifo队列

```go
	fifo := NewDeltaFIFOWithOptions(DeltaFIFOOptions{
		KnownObjects:          s.indexer,
		EmitDeltaTypeReplaced: true,
	})

```

- 新建controller

```go
	func() {
		s.startedLock.Lock()
		defer s.startedLock.Unlock()

		s.controller = New(cfg)
		s.controller.(*controller).clock = s.clock
		s.started = true
	}()
```

- processor启动listeners

```go
wg.StartWithChannel(processorStopCh, s.processor.run)
```

- 底层调用 processorListener的run
- 位置D:\go_path\src\github.com\kubernetes\kubernetes\staging\src\k8s.io\client-go\tools\cache\shared_informer.go
- 在processorListener的run中执行 eventHandler注册的回调方法

```go
func (p *processorListener) run() {
	// this call blocks until the channel is closed.  When a panic happens during the notification
	// we will catch it, **the offending item will be skipped!**, and after a short delay (one second)
	// the next notification will be attempted.  This is usually better than the alternative of never
	// delivering again.
	stopCh := make(chan struct{})
	wait.Until(func() {
		for next := range p.nextCh {
			switch notification := next.(type) {
			case updateNotification:
				p.handler.OnUpdate(notification.oldObj, notification.newObj)
			case addNotification:
				p.handler.OnAdd(notification.newObj)
			case deleteNotification:
				p.handler.OnDelete(notification.oldObj)
			default:
				utilruntime.HandleError(fmt.Errorf("unrecognized notification: %T", next))
			}
		}
		// the only way to get here is if the p.nextCh is empty and closed
		close(stopCh)
	}, 1*time.Second, stopCh)
}

```

### 运行controller

- 位置 D:\go_path\src\github.com\kubernetes\kubernetes\staging\src\k8s.io\client-go\tools\cache\controller.go

```go
func (c *controller) Run(stopCh <-chan struct{}) {
	defer utilruntime.HandleCrash()
	go func() {
		<-stopCh
		c.config.Queue.Close()
	}()
	r := NewReflector(
		c.config.ListerWatcher,
		c.config.ObjectType,
		c.config.Queue,
		c.config.FullResyncPeriod,
	)
	r.ShouldResync = c.config.ShouldResync
	r.WatchListPageSize = c.config.WatchListPageSize
	r.clock = c.clock
	if c.config.WatchErrorHandler != nil {
		r.watchErrorHandler = c.config.WatchErrorHandler
	}

	c.reflectorMutex.Lock()
	c.reflector = r
	c.reflectorMutex.Unlock()

	var wg wait.Group

	wg.StartWithChannel(stopCh, r.Run)

	wait.Until(c.processLoop, time.Second, stopCh)
	wg.Wait()
}

```

- 其中 新建reflector，r.Run 代表生产，往Queue里放数据

#### reflector.Run 生产者解读

- ListAndWatch会调用watchHandler
- watchHandler顾名思义，就是Watch到对应的事件，调用对应的Handler
- 在watchhandler中可以看到对增删改等动作的处理，位置 D:\go_path\src\github.com\kubernetes\kubernetes\staging\src\k8s.io\client-go\tools\cache\reflector.go

```go
			switch event.Type {
			case watch.Added:
				err := r.store.Add(event.Object)
				if err != nil {
					utilruntime.HandleError(fmt.Errorf("%s: unable to add watch event object (%#v) to store: %v", r.name, event.Object, err))
				}
			case watch.Modified:
				err := r.store.Update(event.Object)
				if err != nil {
					utilruntime.HandleError(fmt.Errorf("%s: unable to update watch event object (%#v) to store: %v", r.name, event.Object, err))
				}
			case watch.Deleted:
				// TODO: Will any consumers need access to the "last known
				// state", which is passed in event.Object? If so, may need
				// to change this.
				err := r.store.Delete(event.Object)
				if err != nil {
					utilruntime.HandleError(fmt.Errorf("%s: unable to delete watch event object (%#v) from store: %v", r.name, event.Object, err))
				}
```

#### 消费者分析

- 位置 D:\go_path\src\github.com\kubernetes\kubernetes\staging\src\k8s.io\client-go\tools\cache\shared_informer.go
- 代码

```go
func (s *sharedIndexInformer) HandleDeltas(obj interface{}) error {
	s.blockDeltas.Lock()
	defer s.blockDeltas.Unlock()

	// from oldest to newest
	for _, d := range obj.(Deltas) {
		switch d.Type {
		case Sync, Replaced, Added, Updated:
			s.cacheMutationDetector.AddObject(d.Object)
			if old, exists, err := s.indexer.Get(d.Object); err == nil && exists {
				if err := s.indexer.Update(d.Object); err != nil {
					return err
				}

				isSync := false
				switch {
				case d.Type == Sync:
					// Sync events are only propagated to listeners that requested resync
					isSync = true
				case d.Type == Replaced:
					if accessor, err := meta.Accessor(d.Object); err == nil {
						if oldAccessor, err := meta.Accessor(old); err == nil {
							// Replaced events that didn't change resourceVersion are treated as resync events
							// and only propagated to listeners that requested resync
							isSync = accessor.GetResourceVersion() == oldAccessor.GetResourceVersion()
						}
					}
				}
				s.processor.distribute(updateNotification{oldObj: old, newObj: d.Object}, isSync)
			} else {
				if err := s.indexer.Add(d.Object); err != nil {
					return err
				}
				s.processor.distribute(addNotification{newObj: d.Object}, false)
			}
		case Deleted:
			if err := s.indexer.Delete(d.Object); err != nil {
				return err
			}
			s.processor.distribute(deleteNotification{oldObj: d.Object}, false)
		}
	}
	return nil
}
```

- 过程如下
  - 在indexer中判断对象是否存在，存在更新，不存在就新增
  - 同时调用distribute函数分发给listener

# 本节重点总结 :

> informer机制的框架
> ![k8s_informer.jpeg](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/908/1634433327000/25929c78a7f049f0b965f31a26591f18.jpeg)

> 生产阶段

- Informer 依赖于 Reflector 模块，它有个组件为 xxxInformer，如kube-scheduler中使用的 podInformer
- 具体资源的 Informer 包含了一个连接到kube-apiserver的client，通过List和Watch接口查询资源变更情况
- 检测到资源发生变化后，通过Controller 将数据放入队列DeltaFIFOQueue里，生产阶段完成

> 消费阶段

- 在DeltaFIFOQueue的另一端，有消费者在不停地处理资源变化的事件，处理逻辑主要分2步
  - 将数据保存到本地存储Indexer，它的底层实现是一个并发安全的threadSafeMap
  - 有些组件需要实时关注资源变化，会实时监听listen，就将事件分发到对应注册上来的listener上，自行处理