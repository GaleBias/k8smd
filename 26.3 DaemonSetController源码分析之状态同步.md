
# 本节重点总结：
- daemonset中的个数统计来源在syncDaemonset中
```shell script
[root@k8s-master01 daemonset]# kubectl get ds
NAME              DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
nginx-daemonset   2         2         2       2            2           <none>          15s
```
- DESIRED对应代码中的desiredNumberScheduled，代表应该运行ds的节点数量，判断依据是根据节点选择器和污点得出的这个node是否应该运行这个ds的pod
- CURRENT对应代码中的currentNumberScheduled，代表实际运行ds的节点数量，判断依据是节点应该运行并且实际上也运行了这个ds的pod
- READY对应代码中的numberReady，代表ds的中podREADY的数量，判断依据是ds中最早的pod处于Ready状态
- AVAILABLE对应代码中的numberAvailable，代表ds的中PodAvailable的数量，判断依据是在pod处于Ready状态的基础上满足minReadySeconds条件，即minReadySeconds为0 或者pod至少存活了minReadySeconds时间
- UP-TO-DATE 对应代码中的updatedNumberScheduled，代表显示为了达到期望状态已经更新的副本数。



# 接上回
- DaemonSetController中的第一步初始化工作前面已经分析完毕
- 下面我们来分析一下syncDaemonSet


# Run入口
- DaemonSetController的run还是沿用其它控制器一样的做法
- 先等待所需资源的informer 资源list一次，然后启动多个worker执行同步

```go
// Run begins watching and syncing daemon sets.
func (dsc *DaemonSetsController) Run(workers int, stopCh <-chan struct{}) {
	defer utilruntime.HandleCrash()
	defer dsc.queue.ShutDown()

	klog.Infof("Starting daemon sets controller")
	defer klog.Infof("Shutting down daemon sets controller")

	if !cache.WaitForNamedCacheSync("daemon sets", stopCh, dsc.podStoreSynced, dsc.nodeStoreSynced, dsc.historyStoreSynced, dsc.dsStoreSynced) {
		return
	}

	for i := 0; i < workers; i++ {
		go wait.Until(dsc.runWorker, time.Second, stopCh)
	}

	go wait.Until(dsc.failedPodsBackoff.GC, BackoffGCInterval, stopCh)

	<-stopCh
}
```

## 工作任务runWorker
- 同样还是消费队列中的数据，然后调用syncHandler执行同步
```go
func (dsc *DaemonSetsController) runWorker() {
	for dsc.processNextWorkItem() {
	}
}

// processNextWorkItem deals with one key off the queue.  It returns false when it's time to quit.
func (dsc *DaemonSetsController) processNextWorkItem() bool {
	dsKey, quit := dsc.queue.Get()
	if quit {
		return false
	}
	defer dsc.queue.Done(dsKey)

	err := dsc.syncHandler(dsKey.(string))
	if err == nil {
		dsc.queue.Forget(dsKey)
		return true
	}

	utilruntime.HandleError(fmt.Errorf("%v failed with : %v", dsKey, err))
	dsc.queue.AddRateLimited(dsKey)

	return true
}
```
- 所以接下来的分析都在 syncHandler中

# syncDaemonSet 分析
## 前期准备工作
- 用defer记录耗时
```go
	startTime := dsc.failedPodsBackoff.Clock.Now()

	defer func() {
		klog.V(4).Infof("Finished syncing daemon set %q (%v)", key, dsc.failedPodsBackoff.Clock.Now().Sub(startTime))
	}()
```
- 根据key分割ns和ds ，然后用dsInformer的本地存储查找这个ns下的这个ds，如果没找到删除expectations中这个key的记录
```go
	namespace, name, err := cache.SplitMetaNamespaceKey(key)
	if err != nil {
		return err
	}
	ds, err := dsc.dsLister.DaemonSets(namespace).Get(name)
	if apierrors.IsNotFound(err) {
		klog.V(3).Infof("daemon set has been deleted %v", key)
		dsc.expectations.DeleteExpectations(key)
		return nil
	}
	if err != nil {
		return fmt.Errorf("unable to retrieve ds %v from store: %v", key, err)
	}
```
- 从nodeInformer本地数据获取所有node
```go

	nodeList, err := dsc.nodeLister.List(labels.Everything())
	if err != nil {
		return fmt.Errorf("couldn't get list of nodes when syncing daemon set %#v: %v", ds, err)
	}
```
- 判断ds设置的pod选择器为空
```go
	everything := metav1.LabelSelector{}
	if reflect.DeepEqual(ds.Spec.Selector, &everything) {
		dsc.eventRecorder.Eventf(ds, v1.EventTypeWarning, SelectingAllReason, "This daemon set is selecting all pods. A non-empty selector is required.")
		return nil
	}
```

## 获取ds的当前和历史controllerRevision
- 入口调度
```go
	// Construct histories of the DaemonSet, and get the hash of current history
	cur, old, err := dsc.constructHistory(ds)
	if err != nil {
		return fmt.Errorf("failed to construct revisions of DaemonSet: %v", err)
	}
	hash := cur.Labels[apps.DefaultDaemonSetUniqueLabelKey]

```
### constructHistory中 
- 第一步先根据controlledHistories获取所有controllerRevision历史
```go
func (dsc *DaemonSetsController) constructHistory(ds *apps.DaemonSet) (cur *apps.ControllerRevision, old []*apps.ControllerRevision, err error) {
	var histories []*apps.ControllerRevision
	var currentHistories []*apps.ControllerRevision
	histories, err = dsc.controlledHistories(ds)
	if err != nil {
		return nil, nil, err
	}
```

#### controlledHistories中
- 构造ds的标签选择器，从hisInformer中获取所有的histories
```go
	selector, err := metav1.LabelSelectorAsSelector(ds.Spec.Selector)
	if err != nil {
		return nil, err
	}

	// List all histories to include those that don't match the selector anymore
	// but have a ControllerRef pointing to the controller.
	histories, err := dsc.historyLister.List(labels.Everything())
	if err != nil {
		return nil, err
	}
```
- 准备一个判断领养的方法canAdoptFunc，将准备好的ds标签选择器、所有histories等传入ClaimControllerRevisions进行判断
```go
	// If any adoptions are attempted, we should first recheck for deletion with
	// an uncached quorum read sometime after listing Pods (see #42639).
	canAdoptFunc := controller.RecheckDeletionTimestamp(func() (metav1.Object, error) {
		fresh, err := dsc.kubeClient.AppsV1().DaemonSets(ds.Namespace).Get(context.TODO(), ds.Name, metav1.GetOptions{})
		if err != nil {
			return nil, err
		}
		if fresh.UID != ds.UID {
			return nil, fmt.Errorf("original DaemonSet %v/%v is gone: got uid %v, wanted %v", ds.Namespace, ds.Name, fresh.UID, ds.UID)
		}
		return fresh, nil
	})
	// Use ControllerRefManager to adopt/orphan as needed.
	cm := controller.NewControllerRevisionControllerRefManager(dsc.crControl, ds, selector, controllerKind, canAdoptFunc)
	return cm.ClaimControllerRevisions(histories)
```
- 在ClaimControllerRevisions中准备了match 、adopt、release等方法，然后就是遍历传入的histories执行ClaimObject进行过滤
```go
func (m *ControllerRevisionControllerRefManager) ClaimControllerRevisions(histories []*apps.ControllerRevision) ([]*apps.ControllerRevision, error) {
	var claimed []*apps.ControllerRevision
	var errlist []error

	match := func(obj metav1.Object) bool {
		return m.Selector.Matches(labels.Set(obj.GetLabels()))
	}
	adopt := func(obj metav1.Object) error {
		return m.AdoptControllerRevision(obj.(*apps.ControllerRevision))
	}
	release := func(obj metav1.Object) error {
		return m.ReleaseControllerRevision(obj.(*apps.ControllerRevision))
	}

	for _, h := range histories {
		ok, err := m.ClaimObject(h, match, adopt, release)
		if err != nil {
			errlist = append(errlist, err)
			continue
		}
		if ok {
			claimed = append(claimed, h)
		}
	}
	return claimed, utilerrors.NewAggregate(errlist)
}
```
- ClaimObject中可以看到就是对传入的his对象依次执行过滤方法，找到和ds匹配的对象
```go
func (m *BaseControllerRefManager) ClaimObject(obj metav1.Object, match func(metav1.Object) bool, adopt, release func(metav1.Object) error) (bool, error) {
	controllerRef := metav1.GetControllerOfNoCopy(obj)
	if controllerRef != nil {
		if controllerRef.UID != m.Controller.GetUID() {
			// Owned by someone else. Ignore.
			return false, nil
		}
		if match(obj) {
			// We already own it and the selector matches.
			// Return true (successfully claimed) before checking deletion timestamp.
			// We're still allowed to claim things we already own while being deleted
			// because doing so requires taking no actions.
			return true, nil
		}
		// Owned by us but selector doesn't match.
		// Try to release, unless we're being deleted.
		if m.Controller.GetDeletionTimestamp() != nil {
			return false, nil
		}
		if err := release(obj); err != nil {
			// If the pod no longer exists, ignore the error.
			if errors.IsNotFound(err) {
				return false, nil
			}
			// Either someone else released it, or there was a transient error.
			// The controller should requeue and try again if it's still stale.
			return false, err
		}
		// Successfully released.
		return false, nil
	}

	// It's an orphan.
	if m.Controller.GetDeletionTimestamp() != nil || !match(obj) {
		// Ignore if we're being deleted or selector doesn't match.
		return false, nil
	}
	if obj.GetDeletionTimestamp() != nil {
		// Ignore if the object is being deleted
		return false, nil
	}
	// Selector matches. Try to adopt.
	if err := adopt(obj); err != nil {
		// If the pod no longer exists, ignore the error.
		if errors.IsNotFound(err) {
			return false, nil
		}
		// Either someone else claimed it first, or there was a transient error.
		// The controller should requeue and try again if it's still orphaned.
		return false, err
	}
	// Successfully adopted.
	return true, nil
}
```

### 回到constructHistory中
- 通过 dsc.constructHistory获取到了由这个ds控制的历史controllerRevision对象
- 接下来就是遍历histories对象，根据Match判断是否是当前的，将history归类为当前的或者历史的
```go
	for _, history := range histories {
		// Add the unique label if it's not already added to the history
		// We use history name instead of computing hash, so that we don't need to worry about hash collision
		if _, ok := history.Labels[apps.DefaultDaemonSetUniqueLabelKey]; !ok {
			toUpdate := history.DeepCopy()
			toUpdate.Labels[apps.DefaultDaemonSetUniqueLabelKey] = toUpdate.Name
			history, err = dsc.kubeClient.AppsV1().ControllerRevisions(ds.Namespace).Update(context.TODO(), toUpdate, metav1.UpdateOptions{})
			if err != nil {
				return nil, nil, err
			}
		}
		// Compare histories with ds to separate cur and old history
		found := false
		found, err = Match(ds, history)
		if err != nil {
			return nil, nil, err
		}
		if found {
			currentHistories = append(currentHistories, history)
		} else {
			old = append(old, history)
		}
	}

```
- 判断currentHistories的长度，如果为0证明没有历史记录，需要用snapshot创建一个新的
```go
	currRevision := maxRevision(old) + 1
	switch len(currentHistories) {
	case 0:
		// Create a new history if the current one isn't found
		cur, err = dsc.snapshot(ds, currRevision)
		if err != nil {
			return nil, nil, err
		}
```
- snapshot的主要逻辑就是构造apps.ControllerRevision对象，OwnerReferences指向这个ds，然后调用发往apiserver
```go
func (dsc *DaemonSetsController) snapshot(ds *apps.DaemonSet, revision int64) (*apps.ControllerRevision, error) {
	patch, err := getPatch(ds)
	if err != nil {
		return nil, err
	}
	hash := controller.ComputeHash(&ds.Spec.Template, ds.Status.CollisionCount)
	name := ds.Name + "-" + hash
	history := &apps.ControllerRevision{
		ObjectMeta: metav1.ObjectMeta{
			Name:            name,
			Namespace:       ds.Namespace,
			Labels:          labelsutil.CloneAndAddLabel(ds.Spec.Template.Labels, apps.DefaultDaemonSetUniqueLabelKey, hash),
			Annotations:     ds.Annotations,
			OwnerReferences: []metav1.OwnerReference{*metav1.NewControllerRef(ds, controllerKind)},
		},
		Data:     runtime.RawExtension{Raw: patch},
		Revision: revision,
	}

	history, err = dsc.kubeClient.AppsV1().ControllerRevisions(ds.Namespace).Create(context.TODO(), history, metav1.CreateOptions{})
	
```
- currentHistories中如果有记录，那么调用dedupCurHistories去重，得到cur，判断版本号是否要变更，如需要就update ControllerRevisions
```go
	default:
		cur, err = dsc.dedupCurHistories(ds, currentHistories)
		if err != nil {
			return nil, nil, err
		}
		// Update revision number if necessary
		if cur.Revision < currRevision {
			toUpdate := cur.DeepCopy()
			toUpdate.Revision = currRevision
			_, err = dsc.kubeClient.AppsV1().ControllerRevisions(ds.Namespace).Update(context.TODO(), toUpdate, metav1.UpdateOptions{})
			if err != nil {
				return nil, nil, err
			}
		}
	}
```


## 如果expectations返回无需sync，那么更新ds的状态
```go
	if !dsc.expectations.SatisfiedExpectations(dsKey) {
		// Only update status. Don't raise observedGeneration since controller didn't process object of that generation.
		return dsc.updateDaemonSetStatus(ds, nodeList, hash, false)
	}

```
- 上面是入口
- 下面来看看updateDaemonSetStatus中的操作，
### 首先就是获取一个每个node上应该运行这个ds-pod的map
```go

func (dsc *DaemonSetsController) updateDaemonSetStatus(ds *apps.DaemonSet, nodeList []*v1.Node, hash string, updateObservedGen bool) error {
	klog.V(4).Infof("Updating daemon set status")
	nodeToDaemonPods, err := dsc.getNodesToDaemonPods(ds)
	if err != nil {
		return fmt.Errorf("couldn't get node to daemon pod mapping for daemon set %q: %v", ds.Name, err)
	}
```
- 获取ds-pod的map如下，可以看到map的key就是 nodeName
```go
func (dsc *DaemonSetsController) getNodesToDaemonPods(ds *apps.DaemonSet) (map[string][]*v1.Pod, error) {
	claimedPods, err := dsc.getDaemonPods(ds)
	if err != nil {
		return nil, err
	}
	// Group Pods by Node name.
	nodeToDaemonPods := make(map[string][]*v1.Pod)
	for _, pod := range claimedPods {
		nodeName, err := util.GetTargetNodeName(pod)
		if err != nil {
			klog.Warningf("Failed to get target node name of Pod %v/%v in DaemonSet %v/%v",
				pod.Namespace, pod.Name, ds.Namespace, ds.Name)
			continue
		}

		nodeToDaemonPods[nodeName] = append(nodeToDaemonPods[nodeName], pod)
	}

	return nodeToDaemonPods, nil
}
```


### 然后遍历nodeList 数个数
- 可以看到这里准备了一大批统计个数的变量，下面我们来看下他们的具体含义
```go
var desiredNumberScheduled, currentNumberScheduled, numberMisscheduled, numberReady, updatedNumberScheduled, numberAvailable int

```
- 调用上一节讲到的nodeShouldRunDaemonPod判断这个node上是否应该运行这个ds，就是依次过滤配置的节点名字、节点选择器和污点进行判断
```go
	for _, node := range nodeList {
		shouldRun, _ := dsc.nodeShouldRunDaemonPod(node, ds)
		scheduled := len(nodeToDaemonPods[node.Name]) > 0

```
- 那么shouldRun=true代表这个ds应该运行在这个node上
- scheduled代表这个node上实际有没有运行这个ds的pod

> 这里结合  kubectl get ds的输出
```shell script
[root@k8s-master01 daemonset]# kubectl get ds
NAME              DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
nginx-daemonset   2         2         2       2            2           <none>          15s
```

> 如果应该运行
- 那么首先将desiredNumberScheduled+1 ，所以DESIRED对应代码中的desiredNumberScheduled，判断依据是根据节点选择器和污点得出的这个node是否应该运行这个ds的pod
```go
		if shouldRun {
			desiredNumberScheduled++
```
- 然后scheduled=true代表实际上也运行了，将currentNumberScheduled++， CURRENT对应代码中的currentNumberScheduled，判断依据是节点应该运行并且实际上也运行了这个ds的pod
```go
			if scheduled {
				currentNumberScheduled++
```
- 然后判断第一个pod是否是Ready状态，READY对应代码中的numberReady，ds的中podREADY的数量，判断依据是ds中最早的pod处于Ready状态
```go
				// Sort the daemon pods by creation time, so that the oldest is first.
				daemonPods, _ := nodeToDaemonPods[node.Name]
				sort.Sort(podByCreationTimestampAndPhase(daemonPods))
				pod := daemonPods[0]
				if podutil.IsPodReady(pod) {
					numberReady++
```
- 然后会在pod Ready的基础上判断 pod处于PodAvailable状态 ，AVAILABLE对应代码中的numberAvailable，代表ds的中PodAvailable的数量，判断依据是在pod处于Ready状态的基础上满足minReadySeconds条件，即minReadySeconds为0 或者pod至少存活了minReadySeconds时间

```go
				if podutil.IsPodReady(pod) {
					numberReady++
					if podutil.IsPodAvailable(pod, ds.Spec.MinReadySeconds, metav1.Time{Time: now}) {
						numberAvailable++
					}
				}
```
- 然后根据pod templateGeneration 或者hash标签判断pod是否已经更新，
```go
			generation, err := util.GetTemplateGeneration(ds)
				if err != nil {
					generation = nil
				}
				if util.IsPodUpdated(pod, hash, generation) {
					updatedNumberScheduled++
				}
```
> 如果node不应该运行这个ds
- 并且实际上运行了，那么numberMisscheduled+1 
```go
			if scheduled {
				numberMisscheduled++
			}
```
- 计算numberUnavailable
```go
	numberUnavailable := desiredNumberScheduled - numberAvailable
```
- 使用上面统计诸多技术更新ds的状态
```go
err = storeDaemonSetStatus(dsc.kubeClient.AppsV1().DaemonSets(ds.Namespace), ds, desiredNumberScheduled, currentNumberScheduled, numberMisscheduled, numberReady, updatedNumberScheduled, numberAvailable, numberUnavailable, updateObservedGen)
	if err != nil {
		return fmt.Errorf("error storing status for daemon set %#v: %v", ds, err)
	}

```
- 如果配置了MinReadySeconds并且 numberReady和numberAvailable不匹配就在MinReadySeconds时间后将ds入队
```go
	// Resync the DaemonSet after MinReadySeconds as a last line of defense to guard against clock-skew.
	if ds.Spec.MinReadySeconds > 0 && numberReady != numberAvailable {
		dsc.enqueueDaemonSetAfter(ds, time.Duration(ds.Spec.MinReadySeconds)*time.Second)
	}
```

# 本节重点总结：
- daemonset中的个数统计来源在syncDaemonset中
```shell script
[root@k8s-master01 daemonset]# kubectl get ds
NAME              DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
nginx-daemonset   2         2         2       2            2           <none>          15s
```
- DESIRED对应代码中的desiredNumberScheduled，代表应该运行ds的节点数量，判断依据是根据节点选择器和污点得出的这个node是否应该运行这个ds的pod
- CURRENT对应代码中的currentNumberScheduled，代表实际运行ds的节点数量，判断依据是节点应该运行并且实际上也运行了这个ds的pod
- READY对应代码中的numberReady，代表ds的中podREADY的数量，判断依据是ds中最早的pod处于Ready状态
- AVAILABLE对应代码中的numberAvailable，代表ds的中PodAvailable的数量，判断依据是在pod处于Ready状态的基础上满足minReadySeconds条件，即minReadySeconds为0 或者pod至少存活了minReadySeconds时间
- UP-TO-DATE 对应代码中的updatedNumberScheduled，代表显示为了达到期望状态已经更新的副本数。
