
# cadvisor的作用
- 提供运行容器的资源使用情况和性能特征


# kubelet和cadvisor的关系
- kubelet内置了cadvisor


# kubelet中cadvisor源码

## 入口在kubelet的run中
- 调用 cadvisor.New 生成kubeDeps.CAdvisorInterface对象
```go
	if kubeDeps.CAdvisorInterface == nil {
		imageFsInfoProvider := cadvisor.NewImageFsInfoProvider(s.ContainerRuntime, s.RemoteRuntimeEndpoint)
		kubeDeps.CAdvisorInterface, err = cadvisor.New(imageFsInfoProvider, s.RootDirectory, cgroupRoots, cadvisor.UsingLegacyCadvisorStats(s.ContainerRuntime, s.RemoteRuntimeEndpoint))
		if err != nil {
			return err
		}
	}
```
- 后续kubeDeps.CAdvisorInterface对象会被赋值给kubelet的cadvisor 
```go
klet := &Kubelet{
    cadvisor:                                kubeDeps.CAdvisorInterface,
}
```
## 追踪cadvisor.New 
- 位置 D:\go_path\src\github.com\kubernetes\kubernetes\pkg\kubelet\cadvisor\cadvisor_linux.go
- 在这个文件的import可以看到几个 只导入包但没有使用的地方
```go
	// Register supported container handlers.
	_ "github.com/google/cadvisor/container/containerd/install"
	_ "github.com/google/cadvisor/container/crio/install"
	_ "github.com/google/cadvisor/container/systemd/install"

```
### 注册cadvisor中的容器handlers
- 追踪这三个install包
> 以containerd/install为例
```go
package install

import (
	"github.com/google/cadvisor/container"
	"github.com/google/cadvisor/container/containerd"
	"k8s.io/klog/v2"
)

func init() {
	err := container.RegisterPlugin("containerd", containerd.NewPlugin())
	if err != nil {
		klog.Fatalf("Failed to register containerd plugin: %v", err)
	}
}

```
- 发现调用container.RegisterPlugin 将自身作为插件注册进去，对应的RegisterPlugin函数如下
```go
func RegisterPlugin(name string, plugin Plugin) error {
	pluginsLock.Lock()
	defer pluginsLock.Unlock()
	if _, found := plugins[name]; found {
		return fmt.Errorf("Plugin %q was registered twice", name)
	}
	klog.V(4).Infof("Registered Plugin %q", name)
	plugins[name] = plugin
	return nil
}
```
- 填充的就是 plugins这个map ，供后续调用插件使用
```go
var plugins = make(map[string]Plugin)

```

### init方法 覆盖 cAdvisor 命令行参数默认值
```go
func init() {
	// Override cAdvisor flag defaults.
	flagOverrides := map[string]string{
		// Override the default cAdvisor housekeeping interval.
		"housekeeping_interval": defaultHousekeepingInterval.String(),
		// Disable event storage by default.
		"event_storage_event_limit": "default=0",
		"event_storage_age_limit":   "default=0",
	}
	for name, defaultValue := range flagOverrides {
		if f := flag.Lookup(name); f != nil {
			f.DefValue = defaultValue
			f.Value.Set(defaultValue)
		} else {
			klog.ErrorS(nil, "Expected cAdvisor flag not found", "flag", name)
		}
	}
}
```


### New解析
- 首先注册metrics指标大类
```go
	sysFs := sysfs.NewRealSysFs()

	includedMetrics := cadvisormetrics.MetricSet{
		cadvisormetrics.CpuUsageMetrics:     struct{}{},
		cadvisormetrics.MemoryUsageMetrics:  struct{}{},
		cadvisormetrics.CpuLoadMetrics:      struct{}{},
		cadvisormetrics.DiskIOMetrics:       struct{}{},
		cadvisormetrics.NetworkUsageMetrics: struct{}{},
		cadvisormetrics.AppMetrics:          struct{}{},
		cadvisormetrics.ProcessMetrics:      struct{}{},
	}

	// Only add the Accelerator metrics if the feature is inactive
	if !utilfeature.DefaultFeatureGate.Enabled(kubefeatures.DisableAcceleratorUsageMetrics) {
		includedMetrics[cadvisormetrics.AcceleratorUsageMetrics] = struct{}{}
	}

	if usingLegacyStats || utilfeature.DefaultFeatureGate.Enabled(kubefeatures.LocalStorageCapacityIsolation) {
		includedMetrics[cadvisormetrics.DiskUsageMetrics] = struct{}{}
	}

```
- 创建cAdvisor container manager
```go
	duration := maxHousekeepingInterval
	housekeepingConfig := manager.HouskeepingConfig{
		Interval:     &duration,
		AllowDynamic: pointer.BoolPtr(allowDynamicHousekeeping),
	}

	// Create the cAdvisor container manager.
	m, err := manager.New(memory.New(statsCacheDuration, nil), sysFs, housekeepingConfig, includedMetrics, http.DefaultClient, cgroupRoots, "")
	if err != nil {
		return nil, err
	}
```

#### cAdvisor container manager分析
> 入参分析
- memoryCache 代表一个内存的缓存，maxAge为2分钟
- sysfs 代表底层的节点realfs对象
- houskeepingConfig 代表清理的配置
- includedMetrics 代表指标采集的大类
- collectorHTTPClient 代表http client
- cgroupRoots 代表cgroup根
- perfEventsFile 代表perf相关的文件
> New分析 内部就是准备manager所需的各个字段
- 根据挂载点信息获取fsinfo对象
```go
	context := fs.Context{}

	if err := container.InitializeFSContext(&context); err != nil {
		return nil, err
	}

	fsInfo, err := fs.NewFsInfo(context)
	if err != nil {
		return nil, err
	}
```
- 获取machineInfo，包含节点的机器信息
```go
	machineInfo, err := machine.Info(sysfs, fsInfo, inHostNamespace)
	if err != nil {
		return nil, err
	}
```
- 最终构造Manager对象
```go
	newManager := &manager{
		containers:                            make(map[namespacedContainerName]*containerData),
		quitChannels:                          make([]chan error, 0, 2),
		memoryCache:                           memoryCache,
		fsInfo:                                fsInfo,
		sysFs:                                 sysfs,
		cadvisorContainer:                     selfContainer,
		inHostNamespace:                       inHostNamespace,
		startupTime:                           time.Now(),
		maxHousekeepingInterval:               *houskeepingConfig.Interval,
		allowDynamicHousekeeping:              *houskeepingConfig.AllowDynamic,
		includedMetrics:                       includedMetricsSet,
		containerWatchers:                     []watcher.ContainerWatcher{},
		eventsChannel:                         eventsChannel,
		collectorHTTPClient:                   collectorHTTPClient,
		nvidiaManager:                         accelerators.NewNvidiaManager(includedMetricsSet),
		rawContainerCgroupPathPrefixWhiteList: rawContainerCgroupPathPrefixWhiteList,
	}
```


#### 回到kubelet中
- 最后返回cadvisorClient对象
```go
	return &cadvisorClient{
		imageFsInfoProvider: imageFsInfoProvider,
		rootPath:            rootPath,
		Manager:             m,
	}, nil
```


## kubelet中cadvisor的调用01 MachineInfo 
```go
machineInfo, err := klet.cadvisor.MachineInfo()
```
- 通过cadvisor.MachineInfo获取节点机器信息，底层调用 cadvisorManagerd的GetMachineInfo
```go
func (m *manager) GetMachineInfo() (*info.MachineInfo, error) {
	m.machineMu.RLock()
	defer m.machineMu.RUnlock()
	return m.machineInfo.Clone(), nil
}
```
- 然后通过clone获取到的节点各个字段
```go
func (m *MachineInfo) Clone() *MachineInfo {
	memoryByType := m.MemoryByType
	if len(m.MemoryByType) > 0 {
		memoryByType = make(map[string]*MemoryInfo)
		for memoryType, memoryInfo := range m.MemoryByType {
			memoryByType[memoryType] = memoryInfo
		}
	}
	diskMap := m.DiskMap
	if len(m.DiskMap) > 0 {
		diskMap = make(map[string]DiskInfo)
		for k, info := range m.DiskMap {
			diskMap[k] = info
		}
	}
	copy := MachineInfo{
		Timestamp:        m.Timestamp,
		NumCores:         m.NumCores,
		NumPhysicalCores: m.NumPhysicalCores,
		NumSockets:       m.NumSockets,
		CpuFrequency:     m.CpuFrequency,
		MemoryCapacity:   m.MemoryCapacity,
		MemoryByType:     memoryByType,
		NVMInfo:          m.NVMInfo,
		HugePages:        m.HugePages,
		MachineID:        m.MachineID,
		SystemUUID:       m.SystemUUID,
		BootID:           m.BootID,
		Filesystems:      m.Filesystems,
		DiskMap:          diskMap,
		NetworkDevices:   m.NetworkDevices,
		Topology:         m.Topology,
		CloudProvider:    m.CloudProvider,
		InstanceType:     m.InstanceType,
		InstanceID:       m.InstanceID,
	}
	return &copy
}
```

## kubelet中cadvisor的调用02 start
- 入口在 kubelet的初始化依赖模块函数initializeRuntimeDependentModules中
```go
func (kl *Kubelet) initializeRuntimeDependentModules() {
	if err := kl.cadvisor.Start(); err != nil {
		// Fail kubelet and rely on the babysitter to retry starting kubelet.
		klog.ErrorS(err, "Failed to start cAdvisor")
		os.Exit(1)
	}

```
- 底层就是cadvisorClient的Start
```go
func (cc *cadvisorClient) Start() error {
	return cc.Manager.Start()
}
```
### cadvisorManager 的start
- 通过初始化插件获取containerWatchers
```go
	m.containerWatchers = container.InitializePlugins(m, m.fsInfo, m.includedMetrics)

```
- InitializePlugins中遍历 plugins map，调用Register获得watcher
```go
func InitializePlugins(factory info.MachineInfoFactory, fsInfo fs.FsInfo, includedMetrics MetricSet) []watcher.ContainerWatcher {
	pluginsLock.Lock()
	defer pluginsLock.Unlock()

	containerWatchers := []watcher.ContainerWatcher{}
	for name, plugin := range plugins {
		watcher, err := plugin.Register(factory, fsInfo, includedMetrics)
		if err != nil {
			klog.V(5).Infof("Registration of the %s container factory failed: %v", name, err)
		}
		if watcher != nil {
			containerWatchers = append(containerWatchers, watcher)
		}
	}
	return containerWatchers
}
```
- 获取rawWatcher
```go
	err := raw.Register(m, m.fsInfo, m.includedMetrics, m.rawContainerCgroupPathPrefixWhiteList)
	if err != nil {
		klog.Errorf("Registration of the raw container factory failed: %v", err)
	}

	rawWatcher, err := raw.NewRawContainerWatcher()
	if err != nil {
		return err
	}
	m.containerWatchers = append(m.containerWatchers, rawWatcher)

```

#### 启动对oom的监听
```go
	// Watch for OOMs.
	err = m.watchForNewOoms()
	if err != nil {
		klog.Warningf("Could not configure a source for OOM detection, disabling OOM events: %v", err)
	}
```
- watchForNewOoms中首先新建kmsg log 解析器 ，解析/dev/kmsg中的内核日志
- 同时新建outStream chan 用作生产者和消费者之间的交互
```go
	klog.V(2).Infof("Started watching for new ooms in manager")
	outStream := make(chan *oomparser.OomInstance, 10)
	oomLog, err := oomparser.New()
	if err != nil {
		return err
	}
```
- 启动生成者，就是从内核日志解析容器oom的日志，这个在9.4分析过
    - 过程就是判断有没有invoked oom-killer:字段
    - 然后再用containerRegexp正则判断是容器进程的oom
```go
	go oomLog.StreamOoms(outStream)
func (p *OomParser) StreamOoms(outStream chan<- *OomInstance) {
	kmsgEntries := p.parser.Parse()
	defer p.parser.Close()

	for msg := range kmsgEntries {
		isOomMessage := checkIfStartOfOomMessages(msg.Message)
		if isOomMessage {
			oomCurrentInstance := &OomInstance{
				ContainerName:       "/",
				VictimContainerName: "/",
				TimeOfDeath:         msg.Timestamp,
			}
			for msg := range kmsgEntries {
				finished, err := getContainerName(msg.Message, oomCurrentInstance)
				if err != nil {
					klog.Errorf("%v", err)
				}
				if !finished {
					finished, err = getProcessNamePid(msg.Message, oomCurrentInstance)
					if err != nil {
						klog.Errorf("%v", err)
					}
				}
				if finished {
					oomCurrentInstance.TimeOfDeath = msg.Timestamp
					break
				}
			}
			outStream <- oomCurrentInstance
		}
	}
```
- 启动消费者产生oom 和oomKill event
```go
	go func() {
		for oomInstance := range outStream {
			// Surface OOM and OOM kill events.
			newEvent := &info.Event{
				ContainerName: oomInstance.ContainerName,
				Timestamp:     oomInstance.TimeOfDeath,
				EventType:     info.EventOom,
			}
			err := m.eventHandler.AddEvent(newEvent)
			if err != nil {
				klog.Errorf("failed to add OOM event for %q: %v", oomInstance.ContainerName, err)
			}
			klog.V(3).Infof("Created an OOM event in container %q at %v", oomInstance.ContainerName, oomInstance.TimeOfDeath)

			newEvent = &info.Event{
				ContainerName: oomInstance.VictimContainerName,
				Timestamp:     oomInstance.TimeOfDeath,
				EventType:     info.EventOomKill,
				EventData: info.EventData{
					OomKill: &info.OomKillEventData{
						Pid:         oomInstance.Pid,
						ProcessName: oomInstance.ProcessName,
					},
				},
			}
			err = m.eventHandler.AddEvent(newEvent)
			if err != nil {
				klog.Errorf("failed to add OOM kill event for %q: %v", oomInstance.ContainerName, err)
			}
		}
	}()
```

#### 启动对新容器的监听，添加相关的资源采集
```go
	// Watch for new container.
	quitWatcher := make(chan error)
	err = m.watchForNewContainers(quitWatcher)
	if err != nil {
		return err
	}
```
- 首先遍历 containerWatchers 调用他们的Start启动
```go
	watched := make([]watcher.ContainerWatcher, 0)
	for _, watcher := range m.containerWatchers {
		err := watcher.Start(m.eventsChannel)
		if err != nil {
			for _, w := range watched {
				stopErr := w.Stop()
				if stopErr != nil {
					klog.Warningf("Failed to stop wacher %v with error: %v", w, stopErr)
				}
			}
			return err
		}
		watched = append(watched, watcher)
	}

```
##### rawContainerWatcher 的start
- 入参分析 这里将m.eventsChannel 传入，所以watcher.Start 是生成者
- 遍历cgroups的顶级目录。调用watchDirectory查找里面的容器目录，并监听
```go
	watched := make([]string, 0)
	for _, cgroupPath := range w.cgroupPaths {
		_, err := w.watchDirectory(events, cgroupPath, "/")
		if err != nil {
			for _, watchedCgroupPath := range watched {
				_, removeErr := w.watcher.RemoveWatch("/", watchedCgroupPath)
				if removeErr != nil {
					klog.Warningf("Failed to remove inotify watch for %q with error: %v", watchedCgroupPath, removeErr)
				}
			}
			return err
		}
		watched = append(watched, cgroupPath)
	}
```
> watchDirectory 读取启动时的cgrops目录， 内部产生了ContainerEvent
- 读取目录中的子目录，然后找到subcontainerName，产生ContainerEvent发给events chan
```go
	entries, err := ioutil.ReadDir(dir)
	if err != nil {
		return alreadyWatching, err
	}
	for _, entry := range entries {
		if entry.IsDir() {
			entryPath := path.Join(dir, entry.Name())
			subcontainerName := path.Join(containerName, entry.Name())
			alreadyWatchingSubDir, err := w.watchDirectory(events, entryPath, subcontainerName)
			if err != nil {
				klog.Errorf("Failed to watch directory %q: %v", entryPath, err)
				if os.IsNotExist(err) {
					// The directory may have been removed before watching. Try to watch the other
					// subdirectories. (https://github.com/kubernetes/kubernetes/issues/28997)
					continue
				}
				return alreadyWatching, err
			}
			// since we already missed the creation event for this directory, publish an event here.
			if !alreadyWatchingSubDir {
				go func() {
					events <- watcher.ContainerEvent{
						EventType:   watcher.ContainerAdd,
						Name:        subcontainerName,
						WatchSource: watcher.Raw,
					}
				}()
			}
		}
	}
```

> 外部的processEvent 处理内核发来的inotify事件，处理cgroup目录变动
```go
	// Process the events received from the kernel.
	go func() {
		for {
			select {
			case event := <-w.watcher.Event():
				err := w.processEvent(event, events)
				if err != nil {
					klog.Warningf("Error while processing event (%+v): %v", event, err)
				}
			case err := <-w.watcher.Error():
				klog.Warningf("Error while watching %q: %v", "/", err)
			case <-w.stopWatcher:
				err := w.watcher.Close()
				if err == nil {
					w.stopWatcher <- err
					return
				}
			}
		}
	}()
```

> processEvent中
- 先根据inotify的类型，判断是容器cgroup目录创建还是删除
```go
	// Convert the inotify event type to a container create or delete.
	var eventType watcher.ContainerEventType
	switch {
	case (event.Mask & inotify.InCreate) > 0:
		eventType = watcher.ContainerAdd
	case (event.Mask & inotify.InDelete) > 0:
		eventType = watcher.ContainerDelete
	case (event.Mask & inotify.InMovedFrom) > 0:
		eventType = watcher.ContainerDelete
	case (event.Mask & inotify.InMovedTo) > 0:
		eventType = watcher.ContainerAdd
	default:
		// Ignore other events.
		return nil
	}
```
- 然后创建目录的就watchDirectory ，删除目录的就RemoveWatch
```go
	// Maintain the watch for the new or deleted container.
	switch eventType {
	case watcher.ContainerAdd:
		// New container was created, watch it.
		alreadyWatched, err := w.watchDirectory(events, event.Name, containerName)
		if err != nil {
			return err
		}

		// Only report container creation once.
		if alreadyWatched {
			return nil
		}
	case watcher.ContainerDelete:
		// Container was deleted, stop watching for it.
		lastWatched, err := w.watcher.RemoveWatch(containerName, event.Name)
		if err != nil {
			return err
		}

		// Only report container deletion once.
		if !lastWatched {
			return nil
		}
	default:
		return fmt.Errorf("unknown event type %v", eventType)
	}
```

##### 回到 watchForNewContainers的消费者中
- 处理由watcher生产者通过eventsChannel发来的ContainerEvent事件
    - 如果是ContainerAdd就调用createContainer新增资源采集
    - 如果是ContainerDelete就调用destroyContainer删除资源采集
- createContainer内部逻辑较为复杂就不在这里展开了
```go
	go func() {
		for {
			select {
			case event := <-m.eventsChannel:
				switch {
				case event.EventType == watcher.ContainerAdd:
					switch event.WatchSource {
					default:
						err = m.createContainer(event.Name, event.WatchSource)
					}
				case event.EventType == watcher.ContainerDelete:
					err = m.destroyContainer(event.Name)
				}
				if err != nil {
					klog.Warningf("Failed to process watch event %+v: %v", event, err)
				}
```
####  globalHousekeeping 垃圾清理


```go
	go m.globalHousekeeping(quitGlobalHousekeeping)

func (m *manager) globalHousekeeping(quit chan error) {
	// Long housekeeping is either 100ms or half of the housekeeping interval.
	longHousekeeping := 100 * time.Millisecond
	if *globalHousekeepingInterval/2 < longHousekeeping {
		longHousekeeping = *globalHousekeepingInterval / 2
	}

	ticker := time.NewTicker(*globalHousekeepingInterval)
	for {
		select {
		case t := <-ticker.C:
			start := time.Now()

			// Check for new containers.
			err := m.detectSubcontainers("/")
			if err != nil {
				klog.Errorf("Failed to detect containers: %s", err)
			}

			// Log if housekeeping took too long.
			duration := time.Since(start)
			if duration >= longHousekeeping {
				klog.V(3).Infof("Global Housekeeping(%d) took %s", t.Unix(), duration)
			}
		case <-quit:
			// Quit if asked to do so.
			quit <- nil
			klog.Infof("Exiting global housekeeping thread")
			return
		}
	}
}

```

#### updateMachineInfo 定时更新机器信息给kubelet等使用
```go
	go m.updateMachineInfo(quitUpdateMachineInfo)

func (m *manager) updateMachineInfo(quit chan error) {
	ticker := time.NewTicker(*updateMachineInfoInterval)
	for {
		select {
		case <-ticker.C:
			info, err := machine.Info(m.sysFs, m.fsInfo, m.inHostNamespace)
			if err != nil {
				klog.Errorf("Could not get machine info: %v", err)
				break
			}
			m.machineMu.Lock()
			m.machineInfo = *info
			m.machineMu.Unlock()
			klog.V(5).Infof("Update machine info: %+v", *info)
		case <-quit:
			ticker.Stop()
			quit <- nil
			return
		}
	}
}
```

# 本节重点总结
- cadvisor的作用是提供运行容器的资源使用情况和性能特征
- cadvisor采集容器的资源使用情况的的容器信息依据就是容器的cgroup目录
- cadvisor的两个生成者
    - watchDirectory 读取启动时的cgrops目录
    - processEvent 处理内核发来的inotify事件，处理cgroup目录变动
    - 两者共同作为生产者，产生ContainerEvent事件
- cadvisor的消费者处理由watcher生产者通过eventsChannel发来的ContainerEvent事件
    - 如果是ContainerAdd就调用createContainer新增资源采集
    - 如果是ContainerDelete就调用destroyContainer删除资源采集
    - 具体的采集流程就是查看容器进程/proc/<pid>/统计文件