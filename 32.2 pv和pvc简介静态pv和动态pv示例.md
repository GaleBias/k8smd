# 本节重点总结

> pv和pvc的关系示意图
> ![pv01.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/908/1636076186000/9e9f7e98620b433c823ab924259812f3.png)

> 生命周期

- PV和PVC之间的相互作用遵循这个生命周期:
- PV是集群中的资源。PVC是对这些资源的请求。

```shell
Provisioning --> Binding --> Using --> Releasing --> Recycling
供应-->绑定-->使用--> 释放--> 循环
```

# PV与PVC

> pv

- PV 的全称是：PersistentVolume（持久化卷），是对底层的共享存储的一种抽象
- PV 由管理员进行创建和配置，它和具体的底层的共享存储技术的实现方式有关
- 比如 Ceph、GlusterFS、NFS 等，都是通过插件机制完成与共享存储的对接。

> pvc

- PVC 的全称是：PersistentVolumeClaim（持久化卷声明）
- PVC 是用户存储的一种声明，PVC 和 Pod 比较类似，Pod 消耗的是节点，PVC 消耗的是 PV 资源，Pod 可以请求 CPU 和内存，而 PVC 可以请求特定的存储空间和访问模式
- 对于真正使用存储的用户不需要关心底层的存储实现细节，只需要直接使用 PVC 即可。

> pv和pvc的关系示意图
> ![pv01.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/908/1636076186000/cc63bc795fbd4b54a3529d741bdef82f.png)

> 生命周期

- PV是集群中的资源。PVC是对这些资源的请求。
- PV和PVC之间的相互作用遵循这个生命周期:

```shell
Provisioning --> Binding --> Using --> Releasing --> Recycling
供应-->绑定-->使用--> 释放--> 循环
```

## Provisioning 供应

- 这里有两种PV的提供方式:静态或者动态。
  - Static：集群管理员创建多个PV。 它们携带可供集群用户使用的真实存储的详细信息。 它们存在于Kubernetes API中，可用于消费。
  - Dynamic：当管理员创建的静态PV都不匹配用户的PersistentVolumeClaim时，集群可能会尝试为PVC动态配置卷
    - 此配置基于StorageClasses：PVC必须请求一个类，并且管理员必须已创建并配置该类才能进行动态配置。 要求该类的声明有效地为自己禁用动态配置

## Binding绑定

- 在动态配置的情况下，用户创建或已经创建了具有特定数量的存储请求和特定访问模式的PersistentVolumeClaim
- 主机中的控制回路监视新的PVC，找到匹配的PV（如果可能），并将它们绑定在一起
  - 如果为新的PVC动态配置PV，则循环将始终将该PV绑定到PVC
  - 否则，用户总是至少得到他们要求的内容，但是卷可能超出了要求。
  - 一旦绑定，PersistentVolumeClaim绑定是排他的，不管用于绑定它们的模式。

> 如果匹配的卷不存在

- 如果匹配的卷不存在，PVC将保持无限期。 随着匹配卷变得可用，PVC将被绑定
  - 例如，提供许多50Gi PV的集群将不匹配要求100Gi的PVC
  - 当集群中添加100Gi PV时，可以绑定PVC。

> 删除保护

- PVC 保护的目的是确保由 pod 正在使用的 PVC 不会从系统中移除，因为如果被移除的话可能会导致数据丢失
- 当启用PVC 保护 alpha 功能时，如果用户删除了一个 pod 正在使用的 PVC，则该 PVC 不会被立即删除
- PVC 的 删除将被推迟，直到 PVC 不再被任何 pod 使用。

## PV访问模式

> 每个 PV 都有一套自己的用来描述特定功能的访问模式

- ReadWriteOnce——该卷可以被单个节点以读/写模式挂载
- ReadOnlyMany——该卷可以被多个节点以只读模式挂载
- ReadWriteMany——该卷可以被多个节点以读/写模式挂载

## 回收策略Reclaiming

- Retain（保留）：手动回收
- Recycle（回收）：基本擦除（ rm -rf /thevolume/* ）
- Delete（删除）：关联的存储资产（例如 AWS EBS、GCE PD、Azure Disk 和 OpenStack Cinder 卷） 将被删除

## pv状态

> 卷可以处于以下的某种状态：

- Available（可用）：一块空闲资源还没有被任何声明绑定
- Bound（已绑定）：卷已经被声明绑定
- Released（已释放）：声明被删除，但是资源还未被集群重新声明
- Failed（失败）：该卷的自动回收失败 命令行会显示绑定到 PV 的 PVC 的名称

## 静态提供的pv示例

- 在node节点上创建 /data/pv作为pv的hostPath
- 创建pv

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: task-pv-volume
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/data/pv"
```

- 然后get pv得知,可以看到这个PV为10G，访问模式为RWO，卸载后保留，目前的STATUS为可用状态。

```shell
[root@k8s-master01 pv]# kubectl get pv task-pv-volume 
NAME             CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE
task-pv-volume   10Gi       RWO            Retain           Available           manual                  50s
[root@k8s-master01 pv]# 
```

- 创建PVC ，要求的资源要大于3G，storageClassName类型为manual，这里一定要跟创建PV时对应上。

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: task-pv-claim
spec:
  storageClassName: manual
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 3Gi
```

- 创建完pvc后get pv 和pvc，发现pv状态变为Bound了，而且CLAIM字段也对应我们创建的pvc

```shell
[root@k8s-master01 pv]# kubectl get pv task-pv-volume 
NAME             CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                   STORAGECLASS   REASON   AGE
task-pv-volume   10Gi       RWO            Retain           Bound    default/task-pv-claim   manual                  115s
```

- pvc状态

```shell
[root@k8s-master01 pv]# kubectl get pvc task-pv-claim
NAME            STATUS   VOLUME           CAPACITY   ACCESS MODES   STORAGECLASS   AGE
task-pv-claim   Bound    task-pv-volume   10Gi       RWO            manual         2m
[root@k8s-master01 pv]# 
```

- 在node上 /data/pv目录写入nginx 的index.html内容

```shell
echo "pv and pvc" > /data/pv/index.html
```

- 创建POD

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: task-pv-pod
spec:
  volumes:
    - name: task-pv-storage
      persistentVolumeClaim:
        claimName: task-pv-claim
  containers:
    - name: task-pv-container
      image: nginx:1.7.9
      ports:
        - containerPort: 80
          name: "http-server"
      volumeMounts:
        - mountPath: "/usr/share/nginx/html"
          name: task-pv-storage
```

- 然后curl pod ip，发现pod可以通过pvc 使用pv，底层使用的node上的hostPath

```shell
[root@k8s-master01 pv]# kubectl get pod task-pv-pod -o wide 
NAME          READY   STATUS    RESTARTS   AGE   IP              NODE         NOMINATED NODE   READINESS GATES
task-pv-pod   1/1     Running   0          22s   10.100.85.231   k8s-node01   <none>           <none>
[root@k8s-master01 pv]# curl  10.100.85.231
pv and pvc
[root@k8s-master01 pv]# 

```

## 使用nfs-Provisioner 动态提供pv

- K8S提供了PVC的方式进行存储的便利性，但是PV的创建还是要手工的，使用起来不是很方便
- 在k8s 1.4以后，kubernetes提供了一种更加方便的动态创建PV的方式，即StorageClass
- 使用StorageClass时无需预先创建固定大小的PV来等待使用者创建PVC使用，而是直接创建PVC即可使用。

### 步骤01 服务端安装nfs

- 我们这里选择k8s的master 节点为nfs的服务端
- 因为centos7自带了rpcbind，所以不用安装rpc服务，rpc监听在111端口，可以使用ss -tnulp | grep 111查看rpc服务是否自动启动，如果没有启动，就systemctl start rpcbind 启动rpc服务
- rpc在nfs服务器搭建过程中至关重要，因为rpc能够获得nfs服务器端的端口号等信息，nfs服务器端通过rpc获得这些信息后才能连接nfs服务器端。

```shell
[root@k8s-master01 statefulset]# ss -tnlp |grep rpc   
LISTEN     0      128          *:111                      *:*                   users:(("rpcbind",pid=3497,fd=4),("systemd",pid=1,fd=69))
LISTEN     0      128         :::111                     :::*                   users:(("rpcbind",pid=3497,fd=6),("systemd",pid=1,fd=71))
yum -y install nfs-utils
```

> 编辑/etc/exports ，添加以下内容

```shell
/data/nfs/k8s  *(rw,sync,no_root_squash)
```

- /data/k8s：是共享的数据目录
- *：表示任何人都有权限连接，当然也可以是一个网段，一个 IP，也可以是域名
- rw：读写的权限
- sync：表示文件同时写入硬盘和内存
- no_root_squash：当登录 NFS 主机使用共享目录的使用者是 root 时，其权限将被转换成为匿名使用者，通常它的 UID 与 GID，都会变成 nobody 身份

> 启动nfs服务

- 启动服务

```shell
systemctl start nfs ;systemctl enable nfs 
```

> 使用rpcinfo 检查 nfs的端口

```shell
root@k8s-master01 statefulset]# rpcinfo -p localhost
   program vers proto   port  service
    100000    4   tcp    111  portmapper
    100000    3   tcp    111  portmapper
    100000    2   tcp    111  portmapper
    100000    4   udp    111  portmapper
    100000    3   udp    111  portmapper
    100000    2   udp    111  portmapper
    100024    1   udp  35823  status
    100005    1   udp  20048  mountd
    100005    1   tcp  20048  mountd
    100024    1   tcp  49000  status
    100005    2   udp  20048  mountd
    100005    2   tcp  20048  mountd
    100005    3   udp  20048  mountd
    100005    3   tcp  20048  mountd
    100003    3   tcp   2049  nfs
    100003    4   tcp   2049  nfs
    100227    3   tcp   2049  nfs_acl
    100003    3   udp   2049  nfs
    100003    4   udp   2049  nfs
    100227    3   udp   2049  nfs_acl
    100021    1   udp  34801  nlockmgr
    100021    3   udp  34801  nlockmgr
    100021    4   udp  34801  nlockmgr
    100021    1   tcp  36864  nlockmgr
    100021    3   tcp  36864  nlockmgr
    100021    4   tcp  36864  nlockmgr
```

> 检查nfs配置

```shell
[root@k8s-master01 statefulset]# showmount -e localhost
Export list for localhost:
/data/nfs/k8s 
```

> 创建目录添加测试文件,更改权限

```shell
mkdir -pv /data/nfs/k8s
echo "nfs test" >>/data/nfs/k8s/nfs_test.txt

chown -R nfsnobody.nfsnobody /data/nfs/k8s
```

### 步骤02 客户端安装nfs

- 安装

```shell
 yum -y install nfs-utils 
```

- 使用showmount -e masterip 查看

```shell
[root@k8s-node01 ~]# showmount -e 172.20.70.205
Export list for 172.20.70.205:
/data/nfs/k8s *
```

- 挂载至本地/mnt目录，查看测试文件的内容，如果能看到内容说明正常

```shell
mount -t nfs 172.20.70.205:/data/nfs/k8s /mnt
cat /mnt/nfs_test.txt
nfs test
```

### 步骤3 安装nfs-client 自动配置程序

- 要使用 StorageClass，我们就得安装对应的自动配置程序，比如我们这里存储后端使用的是 nfs，那么我们就需要使用到一个 nfs-client 的自动配置程序，我们也叫它 Provisioner
- 这个程序使用我们已经配置好的 nfs 服务器，来自动创建持久卷，也就是自动帮我们创建 PV。
  - 自动创建的 PV 以${namespace}-${pvcName}-${pvName}这样的命名格式创建在 NFS 服务器上的共享数据目录中
  - 而当这个 PV 被回收后会以archieved-${namespace}-${pvcName}-${pvName}这样的命名格式存在 NFS 服务器上。

> 项目地址

- [nfs-subdir-external-provisioner](https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner)

#### 安装过程

- 首先将代码 clone下来

```shell
git clone  https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner.git
```

- 然后部署rbac鉴权相关

```shell
NS=$(kubectl config get-contexts|grep -e "^\*" |awk '{print $5}')
NAMESPACE=${NS:-default}
sed -i'' "s/namespace:.*/namespace: $NAMESPACE/g" ./deploy/rbac.yaml ./deploy/deployment.yaml
kubectl create -f deploy/rbac.yaml
```

- 然后就是部署nfs-subdir-external-provisioner应用，这里会遇到镜像拉取失败的问题，由于是k8s.gcr.io所以被墙了

```shell
image: k8s.gcr.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2
```

##### 利用阿里云构建海外项目的镜像

- 首先fork nfs-subdir-external-provisioner 项目到自己的github上，

```shell
https://github.com/ning1875/nfs-subdir-external-provisioner
```

> 这里以之前的kube-prometheus 为例

- 在阿里的个人账号上做容器镜像服务  地址https://cr.console.aliyun.com/cn-beijing/instance/repositories
- 在你自己的GitHub上fork你想要拉去镜像的仓库 ，比如ksm
- ![image.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/908/1631407698000/78b5ed813d9343b0b128d77960ee3e25.png)
- 到阿里云的容器镜像创建仓库，选公开
- ![image.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/908/1631407698000/c68653b1bdf449eb8c3e77f13598bd97.png)
- 绑定GitHub仓库
- ![image.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/908/1631407698000/1d63a835fcdb430196bccae14a266b92.png)
- 添加构建规则
- ![image.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/908/1631407698000/e8ba1f23f23c432ca104594ec7375dbc.png)
- 根据tag添加规则
- ![image.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/908/1631407698000/e73a437eb14045ebb2ef4c4a94751a97.png)
- 点击立即构建
- ![image.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/908/1631407698000/d84d3ddcd6a941568392a6b289dfa66a.png)
- 等待构建结果
- ![image.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/908/1631407698000/8624c81385e9417a8777e473082a3f80.png)
- 可以看到还是构建失败了，原因是DockerFile中是直接拷贝编译的二进制的，而编译的过程由make命令触发

```shell
FROM gcr.io/distroless/static:latest
LABEL maintainers="Kubernetes Authors"
LABEL description="NFS subdir external provisioner"
ARG binary=./bin/nfs-subdir-external-provisioner

COPY ${binary} /nfs-subdir-external-provisioner
ENTRYPOINT ["/nfs-subdir-external-provisioner"]
```

- 所以我们得手动将 go build的命令添加到 DockerFile中 ，比如我个提交 ：https://github.com/ning1875/nfs-subdir-external-provisioner/commit/7c0ebc8b618c47ca37fe6499e26ab8114f620445
- 然后编辑Deployment文件，将镜像替换为我们自己的

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nfs-client-provisioner
  labels:
    app: nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: default
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: nfs-client-provisioner
  template:
    metadata:
      labels:
        app: nfs-client-provisioner
    spec:
      serviceAccountName: nfs-client-provisioner
      containers:
        - name: nfs-client-provisioner
          #image: k8s.gcr.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2
          #image: quay.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2
          #image: quay.io/external_storage/nfs-client-provisioner:latest
          image: registry.cn-beijing.aliyuncs.com/ning1875_haiwai_image/nfs-subdir-external-provisioner:v4.0.2
          volumeMounts:
            - name: nfs-client-root
              mountPath: /persistentvolumes
          env:
            - name: PROVISIONER_NAME
              value: k8s-sigs.io/nfs-subdir-external-provisioner
            - name: NFS_SERVER
              value: 172.20.70.205
            - name: NFS_PATH
              value: /data/nfs/k8s
      volumes:
        - name: nfs-client-root
          nfs:
            server: 172.20.70.205
            path: /data/nfs/k8s
```

##### 替换nfs-client-provisioner中的nfs-server信息

- 填写nfs服务端地址和 nfs的挂载路径

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nfs-client-provisioner
  labels:
    app: nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: default
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: nfs-client-provisioner
  template:
    metadata:
      labels:
        app: nfs-client-provisioner
    spec:
      serviceAccountName: nfs-client-provisioner
      containers:
        - name: nfs-client-provisioner
          #image: k8s.gcr.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2
          #image: quay.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2
          #image: quay.io/external_storage/nfs-client-provisioner:latest
          image: registry.cn-beijing.aliyuncs.com/ning1875_haiwai_image/nfs-subdir-external-provisioner:v4.0.2
          volumeMounts:
            - name: nfs-client-root
              mountPath: /persistentvolumes
          env:
            - name: PROVISIONER_NAME
              value: k8s-sigs.io/nfs-subdir-external-provisioner
            - name: NFS_SERVER
              value: 172.20.70.205
            - name: NFS_PATH
              value: /data/nfs/k8s
      volumes:
        - name: nfs-client-root
          nfs:
            server: 172.20.70.205
            path: /data/nfs/k8s
```

- 然后apply 创建这个dep

```shell
[root@k8s-master01 deploy]# kubectl get pod
NAME                                      READY   STATUS      RESTARTS   AGE
nfs-client-provisioner-78577dfbf6-wht7j   1/1     Running     0          58m
```

- 然后查看nfs-client-provisioner的日志

```shell
[root@k8s-master01 nfs-subdir-external-provisioner-master]# kubectl logs nfs-client-provisioner-78577dfbf6-wht7j  -f 
I1018 20:13:04.006654       1 leaderelection.go:242] attempting to acquire leader lease  default/k8s-sigs.io-nfs-subdir-external-provisioner...
I1018 20:13:21.619515       1 leaderelection.go:252] successfully acquired lease default/k8s-sigs.io-nfs-subdir-external-provisioner
I1018 20:13:21.619760       1 controller.go:820] Starting provisioner controller k8s-sigs.io/nfs-subdir-external-provisioner_nfs-client-provisioner-78577dfbf6-wht7j_d90ece08-5c20-4514-b31b-f7d4711dea54!
I1018 20:13:21.619941       1 event.go:278] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"default", Name:"k8s-sigs.io-nfs-subdir-external-provisioner", UID:"f84f9471-32d5-42e3-9ee1-75b43815d0f6", APIVersion:"v1", ResourceVersion:"1226826", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' nfs-client-provisioner-78577dfbf6-wht7j_d90ece08-5c20-4514-b31b-f7d4711dea54 became leader
I1018 20:13:21.720089       1 controller.go:869] Started provisioner controller k8s-sigs.io/nfs-subdir-external-provisioner_nfs-client-provisioner-78577dfbf6-wht7j_d90ece08-5c20-4514-b31b-f7d4711dea54!


```

##### 部署  storage class

- 就是apply deploy/class.yaml

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: managed-nfs-storage
provisioner: k8s-sigs.io/nfs-subdir-external-provisioner # or choose another name, must match deployment's env PROVISIONER_NAME'
parameters:
  pathPattern: "${.PVC.namespace}/${.PVC.annotations.nfs.io/storage-path}" # waits for nfs.io/storage-path annotation, if not specified will accept as empty string.
  onDelete: delete
```

- 检查sc

```shell
[root@k8s-master01 deploy]# kubectl get sc 
NAME                   PROVISIONER                                   RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
managed-nfs-storage    k8s-sigs.io/nfs-subdir-external-provisioner   Delete          Immediate              false                  10s
```

##### 部署测试 pod

```shell
kubectl create -f deploy/test-claim.yaml -f deploy/test-pod.yaml
```

- 分析一下test-claim.yaml ，代表使用上面创建的managed-nfs-storage 的storageClass 创建一个静态的pvc

```yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: test-claim
spec:
  storageClassName: managed-nfs-storage
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 1Mi

```

- 分析一下test-pod.yaml ，里面通过上面的pvc使用 volume

```yaml
kind: Pod
apiVersion: v1
metadata:
  name: test-pod
spec:
  containers:
  - name: test-pod
    image: busybox:stable
    command:
      - "/bin/sh"
    args:
      - "-c"
      - "touch /mnt/SUCCESS && exit 0 || exit 1"
    volumeMounts:
      - name: nfs-pvc
        mountPath: "/mnt"
  restartPolicy: "Never"
  volumes:
    - name: nfs-pvc
      persistentVolumeClaim:
        claimName: test-claim

```

- 然后检查pv 和pvc ，可以看到 创建了一个名为test-claim的pvc
- 同时创建了一个pvc-299377f9-360d-4a5d-85e5-3946a686a8d8的pv

```shell
[root@k8s-master01 nfs-subdir-external-provisioner-master]# kubectl get pvc
NAME          STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS           AGE
grafana-pvc   Bound    grafana-pv                                 10Gi       RWO            grafana-storageclass   6d10h
test-claim    Bound    pvc-299377f9-360d-4a5d-85e5-3946a686a8d8   1Mi        RWX            managed-nfs-storage    2m46s
[root@k8s-master01 nfs-subdir-external-provisioner-master]# kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS        CLAIM                                      STORAGECLASS           REASON   AGE
grafana-pv                                 10Gi       RWO            Retain           Bound         default/grafana-pvc                        grafana-storageclass            6d10h
prometheus-lpv-0                           10Gi       RWO            Retain           Terminating   kube-system/prometheus-data-prometheus-0   prometheus-lpv                  7d1h
pvc-299377f9-360d-4a5d-85e5-3946a686a8d8   1Mi        RWX            Delete           Bound         default/test-claim                         managed-nfs-storage             2m47s
[root@k8s-master01 nfs-subdir-external-provisioner-master]# 
```

- 这时候我们去nfs存储上查看发现多了default/SUCCESS文件，这是test-pod创建的

```shell
[root@k8s-master01 nfs-subdir-external-provisioner-master]# ll /data/nfs/k8s/default/SUCCESS
-rw-r--r-- 1 root root 0 Oct 18 21:22 /data/nfs/k8s/default/SUCCESS
```

- 同时观察nfs-client-provisioner的日志，可以看到相关的pv和pvc创建过程

```shell
[root@k8s-master01 deploy]# kubectl logs nfs-client-provisioner-78577dfbf6-5vc8k  -f 
I1018 21:22:30.465733       1 controller.go:1317] provision "default/test-claim" class "managed-nfs-storage": started
I1018 21:22:30.475689       1 event.go:278] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"test-claim", UID:"299377f9-360d-4a5d-85e5-3946a686a8d8", APIVersion:"v1", ResourceVersion:"1236396", FieldPath:""}): type: 'Normal' reason: 'Provisioning' External provisioner is provisioning volume for claim "default/test-claim"
I1018 21:22:30.478079       1 controller.go:1420] provision "default/test-claim" class "managed-nfs-storage": volume "pvc-299377f9-360d-4a5d-85e5-3946a686a8d8" provisioned
I1018 21:22:30.478117       1 controller.go:1437] provision "default/test-claim" class "managed-nfs-storage": succeeded
I1018 21:22:30.478137       1 volume_store.go:212] Trying to save persistentvolume "pvc-299377f9-360d-4a5d-85e5-3946a686a8d8"
I1018 21:22:30.491054       1 volume_store.go:219] persistentvolume "pvc-299377f9-360d-4a5d-85e5-3946a686a8d8" saved
I1018 21:22:30.491376       1 event.go:278] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"test-claim", UID:"299377f9-360d-4a5d-85e5-3946a686a8d8", APIVersion:"v1", ResourceVersion:"1236396", FieldPath:""}): type: 'Normal' reason: 'ProvisioningSucceeded' Successfully provisioned volume pvc-299377f9-360d-4a5d-85e5-3946a686a8d8
```

- 这时候删掉 test-pod和test-claim

```shell
[root@k8s-master01 nfs-subdir-external-provisioner-master]# kubectl delete -f deploy/test-pod.yaml -f deploy/test-claim.yaml
pod "test-pod" deleted
persistentvolumeclaim "test-claim" deleted
```

- 发现nfs存储上的 default/SUCCESS文件已经被删除

```shell
[root@k8s-master01 nfs-subdir-external-provisioner-master]# ll /data/nfs/k8s/default/SUCCESS
ls: cannot access /data/nfs/k8s/default/SUCCESS: No such file or directory
[root@k8s-master01 nfs-subdir-external-provisioner-master]# ll /data/nfs/k8s/
total 4
-rw-r--r-- 1 nfsnobody nfsnobody 9 Oct 18 18:07 nfs_test.txt
```

- 同时可以发现日志打印了删除的过程

```shell
I1018 21:29:37.944297       1 controller.go:1450] delete "pvc-299377f9-360d-4a5d-85e5-3946a686a8d8": started
I1018 21:29:37.953587       1 controller.go:1478] delete "pvc-299377f9-360d-4a5d-85e5-3946a686a8d8": volume deleted
I1018 21:29:37.958011       1 controller.go:1524] delete "pvc-299377f9-360d-4a5d-85e5-3946a686a8d8": persistentvolume deleted
I1018 21:29:37.958033       1 controller.go:1526] delete "pvc-299377f9-360d-4a5d-85e5-3946a686a8d8": succeeded

```

- 到这里说明我们基于nfs的动态pv已经搭建完毕

# 本节重点总结

> pv和pvc的关系示意图
>
> ![pv01.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/908/1636076186000/c5a6916839454cd8a817336ac04aa000.png)

> 生命周期

- PV是集群中的资源。PVC是对这些资源的请求。
- PV和PVC之间的相互作用遵循这个生命周期:

```shell
Provisioning --> Binding --> Using --> Releasing --> Recycling
供应-->绑定-->使用--> 释放--> 循环
```