# 接上回

- 前面我们分析了syncJob中的流程，其中最重要的一块没有分析
- 就是job 如果不是failed 状态，根据 jobNeedsSync 判断是否要进行同步,若需要同步则调用 jm.manageJob 进行同步；
- 入口在

```go
		manageJobCalled := false
		if jobNeedsSync && job.DeletionTimestamp == nil {
			active, action, manageJobErr = jm.manageJob(&job, activePods, succeeded, succeededIndexes)
			manageJobCalled = true
		}
```

## 首先看 同步判断条件 jobNeedsSync的逻辑

- 可以看到就是用Expectations 机制判断的

```go
jobNeedsSync := jm.expectations.SatisfiedExpectations(key)

```

## expectations 机制分析

- 其实，控制器除了有 informer 的缓存外，还有一个本地缓存就是 expectations
- expectations 会记录 控制器 所有对象需要 add/del 的 pod 数量
- 若两者都为 0 则说明该 控制器所期望创建的 pod 或者删除的 pod 数已经被满足
- 若不满足则说明某次在 syncLoop 中创建或者删除 pod 时有失败的操作
- 则需要等待 expectations 过期后再次同步该控制器

> 总结 expectations 机制的目的就是减少不必要的 sync 操作。

### expectations 中用到的几个方法

- 位置 D:\go_path\src\github.com\kubernetes\kubernetes\pkg\controller\controller_utils.go

```go
type ControllerExpectationsInterface interface {
	GetExpectations(controllerKey string) (*ControlleeExpectations, bool, error)
    // 检查expectations 机制是否会满足
	SatisfiedExpectations(controllerKey string) bool
     // 删除该 key
	DeleteExpectations(controllerKey string)
	SetExpectations(controllerKey string, add, del int) error
    // 写入 key 需要 add 的 pod 数量
	ExpectCreations(controllerKey string, adds int) error
    // 写入 key 需要 del 的 pod 数量
	ExpectDeletions(controllerKey string, dels int) error
    // 创建了一个 pod 说明 expectations 中对应的 key add 期望值需要减少一个 pod， add -1
	CreationObserved(controllerKey string)
    // 删除了一个 pod 说明 expectations 中对应的 key del 期望值需要减少一个 pod， del - 1
	DeletionObserved(controllerKey string)
	RaiseExpectations(controllerKey string, add, del int)
	LowerExpectations(controllerKey string, add, del int)
}
```

#### 首先看下判断 jobNeedsSync用到的SatisfiedExpectations方法

- 我们观察下SatisfiedExpectations返回true的地方其实有三个
  - 该 key 在 ControllerExpectations 中不存在，即该对象是新创建的；
  - 满足exp.Fulfilled() ，该 key 在 ControllerExpectations 中的 adds 和 dels 都 <= 0，即调用 apiserver 的创建和删除接口没有失败过
  - 满足 exp.isExpired() ，该 key 在 ControllerExpectations 中已经超过 5min 没有更新了
- SatisfiedExpectations代码如下

```go
func (r *ControllerExpectations) SatisfiedExpectations(controllerKey string) bool {
	if exp, exists, err := r.GetExpectations(controllerKey); exists {
		if exp.Fulfilled() {
			klog.V(4).Infof("Controller expectations fulfilled %#v", exp)
			return true
		} else if exp.isExpired() {
			klog.V(4).Infof("Controller expectations expired %#v", exp)
			return true
		} else {
			klog.V(4).Infof("Controller still waiting on expectations %#v", exp)
			return false
		}
	} else if err != nil {
		klog.V(2).Infof("Error encountered while checking expectations %#v, forcing sync", err)
	} else {
		// When a new controller is created, it doesn't have expectations.
		// When it doesn't see expected watch events for > TTL, the expectations expire.
		//	- In this case it wakes up, creates/deletes controllees, and sets expectations again.
		// When it has satisfied expectations and no controllees need to be created/destroyed > TTL, the expectations expire.
		//	- In this case it continues without setting expectations till it needs to create/delete controllees.
		klog.V(4).Infof("Controller %v either never recorded expectations, or the ttl expired.", controllerKey)
	}
	// Trigger a sync if we either encountered and error (which shouldn't happen since we're
	// getting from local store) or this controller hasn't established expectations.
	return true
}

```

> Fulfilled 代表 add和del都为小于等于0

```go
// Fulfilled returns true if this expectation has been fulfilled.
func (e *ControlleeExpectations) Fulfilled() bool {
	// TODO: think about why this line being atomic doesn't matter
	return atomic.LoadInt64(&e.add) <= 0 && atomic.LoadInt64(&e.del) <= 0
}
```

> isExpired代表上次更新时间超过5分钟

```go
func (exp *ControlleeExpectations) isExpired() bool {
	return clock.RealClock{}.Since(exp.timestamp) > ExpectationsTimeout
}
```

#### expectations中的add和del何时被设置初始化的值

- 那么对应就是ExpectCreations方法的调用，追踪发现就是在manageJob中会调用
- 这里的diff就是目标添加的数目

```go
func (jm *Controller) manageJob(job *batch.Job, activePods []*v1.Pod, succeeded int32, succeededIndexes []interval) (int32, string, error) {
    	if active < wantActive {
    		diff := wantActive - active
    		if diff > int32(maxPodCreateDeletePerSync) {
    			diff = int32(maxPodCreateDeletePerSync)
    		}
  
    		jm.expectations.ExpectCreations(jobKey, int(diff))
}
```

#### 那么expectations中的add和del何时被调用

- 追踪可以发现 eventHandler 中可以看到在 addPod 中会调用 jm.expectations.CreationObserved 将其 add 值 -1

```go
	podInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
		AddFunc:    jm.addPod,
		UpdateFunc: jm.updatePod,
		DeleteFunc: jm.deletePod,
	})
func (jm *Controller) addPod(obj interface{}) {
    jm.expectations.CreationObserved(jobKey)
}
```

- 在deletePod中会调用jm.expectations.DeletionObserved 将其 del 值 -1

```go
func (jm *Controller) deletePod(obj interface{}) {
    jm.expectations.DeletionObserved(jobKey)
}
```

- 也就是说正常情况下 addPod 和deletePod回调正常执行，最后expectations 的add和del都应该为0
- 那么在 jobNeedsSync判断中就认为不需要同步了

## 正式分析manageJob

- 首先获取active个数，并行度，并获取jobKey

```go
	active := int32(len(activePods))
	parallelism := *job.Spec.Parallelism
	jobKey, err := controller.KeyFunc(job)
	if err != nil {
		utilruntime.HandleError(fmt.Errorf("Couldn't get key for job %#v: %v", job, err))
		return 0, metrics.JobSyncActionTracking, nil
	}
```

### 处理job挂起的逻辑

- 如果开启了SuspendJob特性，并且job的suspend为true的话代表job挂起
- 需要更新缓存中的del个数

```go

	if jobSuspended(job) {
		klog.V(4).InfoS("Deleting all active pods in suspended job", "job", klog.KObj(job), "active", active)
		podsToDelete := activePodsForRemoval(job, activePods, int(active))
		jm.expectations.ExpectDeletions(jobKey, len(podsToDelete))
		removed, err := jm.deleteJobPods(job, jobKey, podsToDelete)
		active -= removed
		return active, metrics.JobSyncActionPodsDeleted, err
	}

```

### 计算 wantActive

- wantActive 代表期望active的个数
- 如果没有配置Completions ，如果有succeeded的，wantActive就是active的否则就是并行度parallelism
- 如果配置了Completions，那么wantActive为 Completions减去已成功的，并且要求wantActive不大于并行度parallelism

```go
	wantActive := int32(0)
	if job.Spec.Completions == nil {
		// Job does not specify a number of completions.  Therefore, number active
		// should be equal to parallelism, unless the job has seen at least
		// once success, in which leave whatever is running, running.
		if succeeded > 0 {
			wantActive = active
		} else {
			wantActive = parallelism
		}
	} else {
		// Job specifies a specific number of completions.  Therefore, number
		// active should not ever exceed number of remaining completions.
		wantActive = *job.Spec.Completions - succeeded
		if wantActive > parallelism {
			wantActive = parallelism
		}
		if wantActive < 0 {
			wantActive = 0
		}
	}
```

### 删除多余的pod

![remove_pod.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/908/1636075745000/37f476718a024979ab030df8a1669536.png)

- rmAtLeast 代表至少要删除的个数
- podsToDelete就是在activePods过滤 要删除的pods
- 如果rmAtLeast为0那么podsToDelete也会为0

```go
	rmAtLeast := active - wantActive
	if rmAtLeast < 0 {
		rmAtLeast = 0
	}
	podsToDelete := activePodsForRemoval(job, activePods, int(rmAtLeast))
	if len(podsToDelete) > maxPodCreateDeletePerSync {
		podsToDelete = podsToDelete[:maxPodCreateDeletePerSync]
	}
```

- 如果有待删的，那么需要做几件事
  - 更新expectations缓存中的del个数
  - 调用deleteJobPods删除pod，就是用kubeclient调用apiserver删除etcd中的pod
  - 然后对应的kubelet就能收到信息，调用runtime删除

```go

	if len(podsToDelete) > 0 {
		jm.expectations.ExpectDeletions(jobKey, len(podsToDelete))
		klog.V(4).InfoS("Too many pods running for job", "job", klog.KObj(job), "deleted", len(podsToDelete), "target", parallelism)
		removed, err := jm.deleteJobPods(job, jobKey, podsToDelete)
		active -= removed
		// While it is possible for a Job to require both pod creations and
		// deletions at the same time (e.g. indexed Jobs with repeated indexes), we
		// restrict ourselves to either just pod deletion or pod creation in any
		// given sync cycle. Of these two, pod deletion takes precedence.
		return active, metrics.JobSyncActionPodsDeleted, err
	}

```

### 慢启动创建需要的pod

![.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/908/1636075745000/943d86bf3a6e42729be3f2424216911c.png)

- 首先计算要添加的数量为diff，同时用maxPodCreateDeletePerSync控制diff的上限
- 然后更新expectations缓存

```go
	if active < wantActive {
		diff := wantActive - active
		if diff > int32(maxPodCreateDeletePerSync) {
			diff = int32(maxPodCreateDeletePerSync)
		}

		jm.expectations.ExpectCreations(jobKey, int(diff))
```

- 创建WaitGroup 准备并发创建，同时还有一些 索引job的逻辑，我们可以先不看

```go
		errCh := make(chan error, diff)
		klog.V(4).Infof("Too few pods running job %q, need %d, creating %d", jobKey, wantActive, diff)

		wait := sync.WaitGroup{}

		var indexesToAdd []int
		if isIndexedJob(job) {
			indexesToAdd = firstPendingIndexes(activePods, succeededIndexes, int(diff), int(*job.Spec.Completions))
			diff = int32(len(indexesToAdd))
		}
		active += diff

		podTemplate := job.Spec.Template.DeepCopy()
		if isIndexedJob(job) {
			addCompletionIndexEnvVariables(podTemplate)
		}
		if trackingUncountedPods(job) {
			podTemplate.Finalizers = appendJobCompletionFinalizerIfNotFound(podTemplate.Finalizers)
		}
```

> 下面就是慢启动的创建过程

- 目的是为了防止大量创建pod出现相同的错误
- 创建的 pod 数依次为 1、2、4、8......，呈指数级增长，job 创建 pod 的方式与 rs 创建 pod 是类似的

```go
for batchSize := int32(integer.IntMin(int(diff), controller.SlowStartInitialBatchSize)); diff > 0; batchSize = integer.Int32Min(2*batchSize, diff) {
    diff -= batchSize	
}
```

> 创建的动作在 go func 匿名函数中

- 调用jm.podControl.CreatePodsWithGenerateName创建pod，底层就是更新apiserver

```go
go func() {
				generateName := podGenerateNameWithIndex(job.Name, completionIndex)
					err := jm.podControl.CreatePodsWithGenerateName(job.Namespace, template, job, metav1.NewControllerRef(job, controllerKind), generateName)
				

}
```

- 同时会判断返回错误是否是这个ns 要被清理的错误，如果是这个错误那么忽略

```go
	                if err != nil {
						if apierrors.HasStatusCause(err, v1.NamespaceTerminatingCause) {
							// If the namespace is being torn down, we can safely ignore
							// this error since all subsequent creations will fail.
							return
						}
					}
```

- 如果是其他类型错误就更新expectations缓存，add-1

```go
					if err != nil {
						defer utilruntime.HandleError(err)
						// Decrement the expected number of creates because the informer won't observe this pod
						klog.V(2).Infof("Failed creation, decrementing expectations for job %q/%q", job.Namespace, job.Name)
						jm.expectations.CreationObserved(jobKey)
						atomic.AddInt32(&active, -1)
						errCh <- err
					}
				}()
```

> 判断这个batch批次是否有创建失败的

- errorCount < len(errCh) 说明errch中有新的错误产生
- skippedPods > 0 说明还有剩余的
- 此时需要将剩余的skippedPods 记录在expectations 中

```go
			// any skipped pods that we never attempted to start shouldn't be expected.
			skippedPods := diff - batchSize
			if errorCount < len(errCh) && skippedPods > 0 {
				klog.V(2).Infof("Slow-start failure. Skipping creation of %d pods, decrementing expectations for job %q/%q", skippedPods, job.Namespace, job.Name)
				active -= skippedPods
				for i := int32(0); i < skippedPods; i++ {
					// Decrement the expected number of creates because the informer won't observe this pod
					jm.expectations.CreationObserved(jobKey)
				}
				// The skipped pods will be retried later. The next controller resync will
				// retry the slow start process.
				break
			}
```

# 本节重点总结：

![.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/908/1636075745000/a95310c5a8e646c29225c62cdd98f0a7.png)

- 慢启动创建需要的pod

  - 目的是为了防止大量创建pod出现相同的错误
  - 创建的 pod 数依次为 1、2、4、8......，呈指数级增长，job 创建 pod 的方式与 rs 创建 pod 是类似的
- 删除过多的pod
  ![remove_pod.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/908/1636075745000/fea3bc7625bd46478a2d845ee7885e56.png)

> expectations 机制分析

- 其实，控制器除了有 informer 的缓存外，还有一个本地缓存就是 expectations
- expectations 会记录 控制器 所有对象需要 add/del 的 pod 数量
- 若两者都为 0 则说明该 控制器所期望创建的 pod 或者删除的 pod 数已经被满足
- 若不满足则说明某次在 syncLoop 中创建或者删除 pod 时有失败的操作
- 则需要等待 expectations 过期后再次同步该控制器

> 总结 expectations 机制的目的就是减少不必要的 sync 操作。
>