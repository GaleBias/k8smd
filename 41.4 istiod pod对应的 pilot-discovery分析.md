
# istiod pod对应的 pilot-discovery二进制的作用
- pilot-discovery 扮演服务注册中心、Istio 控制平面到 sidecar 之间的桥梁作用。pilot-discovery 的主要功能如下：
    - 监控服务注册中心（如 Kubernetes）的服务注册情况。在 Kubernetes 环境下，会监控 service、endpoint、pod、node 等资源信息。
    - 监控 Istio 控制面信息变化，在 Kubernetes 环境下，会监控包括 RouteRule、 VirtualService、Gateway、EgressRule、ServiceEntry 等以 Kubernetes CRD 形式存在的 Istio 控制面配置信息。
    - 将上述两类信息合并组合为 sidecar 可以理解的（遵循 Envoy data plane api 的）配置信息，并将这些信息以 gRPC 协议提供给 sidecar。



# istiod pod怎么对应上的pilot-discovery二进制
- 这是一个通用的排查方法
- 首先describe pod对应的image 和args
```shell script
[root@k8s-master01 ~]# kubectl get pod -l app=istiod -n istio-system 
NAME                      READY   STATUS    RESTARTS   AGE
istiod-5847c59c69-ntk78   1/1     Running   0          2d23h
[root@k8s-master01 ~]# kubectl describe  pod -l app=istiod -n istio-system   
Name:         istiod-5847c59c69-ntk78
Namespace:    istio-system
Priority:     0
Node:         k8s-node01/172.20.70.215
Start Time:   Mon, 15 Nov 2021 22:53:12 +0800
Labels:       app=istiod
              install.operator.istio.io/owning-resource=unknown
              istio=pilot
              istio.io/rev=default
              operator.istio.io/component=Pilot
              pod-template-hash=5847c59c69
              sidecar.istio.io/inject=false
Annotations:  cni.projectcalico.org/podIP: 10.100.85.241/32
              cni.projectcalico.org/podIPs: 10.100.85.241/32
              prometheus.io/port: 15014
              prometheus.io/scrape: true
              sidecar.istio.io/inject: false
Status:       Running
IP:           10.100.85.241
IPs:
  IP:           10.100.85.241
Controlled By:  ReplicaSet/istiod-5847c59c69
Containers:
  discovery:
    Container ID:  containerd://2bc50dec8c2191bc39fde911fd020f9a8716730862d177bc65f7ce160122af65
    Image:         docker.io/istio/pilot:1.11.4
    Image ID:      docker.io/istio/pilot@sha256:c590783fc54aec5d3edb44e3f588be5431db9f0844d44c4314a896728cdbbf77
    Ports:         8080/TCP, 15010/TCP, 15017/TCP
    Host Ports:    0/TCP, 0/TCP, 0/TCP
    Args:
      discovery
      --monitoringAddr=:15014
      --log_output_level=default:info
      --domain
      cluster.local
      --keepaliveMaxServerConnectionAge
      30m
    State:          Running
      Started:      Mon, 15 Nov 2021 22:54:21 +0800
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:      10m
      memory:   100Mi
    Readiness:  http-get http://:8080/ready delay=1s timeout=5s period=3s #success=1 #failure=3
    Environment:
      REVISION:                                     default
      JWT_POLICY:                                   third-party-jwt
      PILOT_CERT_PROVIDER:                          istiod
      POD_NAME:                                     istiod-5847c59c69-ntk78 (v1:metadata.name)
      POD_NAMESPACE:                                istio-system (v1:metadata.namespace)
      SERVICE_ACCOUNT:                               (v1:spec.serviceAccountName)
      KUBECONFIG:                                   /var/run/secrets/remote/config
      ENABLE_LEGACY_FSGROUP_INJECTION:              false
      PILOT_TRACE_SAMPLING:                         100
      PILOT_ENABLE_PROTOCOL_SNIFFING_FOR_OUTBOUND:  true
      PILOT_ENABLE_PROTOCOL_SNIFFING_FOR_INBOUND:   true
      ISTIOD_ADDR:                                  istiod.istio-system.svc:15012
      PILOT_ENABLE_ANALYSIS:                        false
      CLUSTER_ID:                                   Kubernetes
    Mounts:
      /etc/cacerts from cacerts (ro)
      /var/run/secrets/istio-dns from local-certs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-t5f44 (ro)
      /var/run/secrets/remote from istio-kubeconfig (ro)
      /var/run/secrets/tokens from istio-token (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  local-certs:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  cacerts:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  cacerts
    Optional:    true
  istio-kubeconfig:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  istio-kubeconfig
    Optional:    true
  kube-api-access-t5f44:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>
```
- 得到镜像为 docker.io/istio/pilot 
- 参数为
```shell script
    Args:
      discovery
      --monitoringAddr=:15014
      --log_output_level=default:info
      --domain
      cluster.local
      --keepaliveMaxServerConnectionAge
      30m
```
## 追踪这个镜像
- 对应的DockerFile位置 D:\go_path\src\github.com\istio\istio\pilot\docker\Dockerfile.pilot
```shell script
# BASE_DISTRIBUTION is used to switch between the old base distribution and distroless base images
ARG BASE_DISTRIBUTION=debug

# Version is the base image version from the TLD Makefile
ARG BASE_VERSION=latest

# The following section is used as base image if BASE_DISTRIBUTION=debug
FROM gcr.io/istio-release/base:${BASE_VERSION} as debug

# The following section is used as base image if BASE_DISTRIBUTION=distroless
FROM gcr.io/istio-release/distroless:${BASE_VERSION} as distroless

# This will build the final image based on either debug or distroless from above
# hadolint ignore=DL3006
FROM ${BASE_DISTRIBUTION:-debug}

ARG TARGETARCH
COPY ${TARGETARCH:-amd64}/pilot-discovery /usr/local/bin/pilot-discovery

# Copy templates for bootstrap generation.
COPY envoy_bootstrap.json /var/lib/istio/envoy/envoy_bootstrap_tmpl.json
COPY gcp_envoy_bootstrap.json /var/lib/istio/envoy/gcp_envoy_bootstrap_tmpl.json

USER 1337:1337

ENTRYPOINT ["/usr/local/bin/pilot-discovery"]

```
- 通过 ENTRYPOINT 可以发现对应的二进制就是 pilot-discovery

#  pilot-discovery 主要流程分析
- 在看代码之前我们先带个疑问那就是 istio定义了很多crd 如 Gateway、VirtualService、DestinationRule等
- 那么istiod中应该有相关crd 的controller 调谐方法
- 同时istiod还负责 sidecar 注入的 准入控制插件
- 同时肯定有启动grpc-server的代码，ok带着疑问去看吧 


## 01  discovery的入口
- 根据容器启动的参数中的子命令 discovery可以找到 对应的cobra commond
- 位置 D:\go_path\src\github.com\istio\istio\pilot\cmd\pilot-discovery\app\cmd.go 
```go
func newDiscoveryCommand() *cobra.Command {
	return &cobra.Command{
		Use:   "discovery",
		Short: "Start Istio proxy discovery service.",
		Args:  cobra.ExactArgs(0),
		PreRunE: func(c *cobra.Command, args []string) error {
			if err := log.Configure(loggingOptions); err != nil {
				return err
			}
			if err := validateFlags(serverArgs); err != nil {
				return err
			}
			if err := serverArgs.Complete(); err != nil {
				return err
			}
			return nil
		},
		RunE: func(c *cobra.Command, args []string) error {
			cmd.PrintFlags(c.Flags())

			// Create the stop channel for all of the servers.
			stop := make(chan struct{})

			// Create the server for the discovery service.
			discoveryServer, err := bootstrap.NewServer(serverArgs)
			if err != nil {
				return fmt.Errorf("failed to create discovery service: %v", err)
			}

			// Start the server
			if err := discoveryServer.Start(stop); err != nil {
				return fmt.Errorf("failed to start discovery service: %v", err)
			}

			cmd.WaitSignal(stop)
			// Wait until we shut down. In theory this could block forever; in practice we will get
			// forcibly shut down after 30s in Kubernetes.
			discoveryServer.WaitUntilCompletion()
			return nil
		},
	}
}

```
- 在RunE中可以发现就是创建discoveryServer并启动


## 02  SidecarInjector 注入的准入控制插件 
- 相关代码在39.6讲解过了
```go
	var wh *inject.Webhook
	// common https server for webhooks (e.g. injection, validation)
	if s.kubeClient != nil {
		s.initSecureWebhookServer(args)
		wh, err = s.initSidecarInjector(args)
		if err != nil {
			return nil, fmt.Errorf("error initializing sidecar injector: %v", err)
		}
		if err := s.initConfigValidation(args); err != nil {
			return nil, fmt.Errorf("error initializing config validator: %v", err)
		}
	}

	whc := func() map[string]string {
		if wh != nil {
			return wh.Config.Templates
		}
		return map[string]string{}
	}

```

## 03 启动grpc-server的代码
- grpc-server初始化的入口在 initDiscoveryService，
```go
s.initDiscoveryService(args)
// initDiscoveryService intializes discovery server on plain text port.
func (s *Server) initDiscoveryService(args *PilotArgs) {
	log.Infof("starting discovery service")
	// Implement EnvoyXdsServer grace shutdown
	s.addStartFunc(func(stop <-chan struct{}) error {
		log.Infof("Starting ADS server")
		s.XDSServer.Start(stop)
		return nil
	})

	s.initGrpcServer(args.KeepaliveOptions)

	if args.ServerOptions.GRPCAddr != "" {
		s.grpcAddress = args.ServerOptions.GRPCAddr
	} else {
		// This happens only if the GRPC port (15010) is disabled. We will multiplex
		// it on the HTTP port. Does not impact the HTTPS gRPC or HTTPS.
		log.Info("multiplexing gRPC on http addr ", args.ServerOptions.HTTPAddr)
		s.MultiplexGRPC = true
	}
}

```
- 首先就是这里的addStartFunc，意思是在启动grpc之前启动 ADS server
- 这很好理解 grpc-server启动之后如果没有数据，那么请求会异常，ADS server代表填充数据的
- 一路追踪后可以发现就是将 对应的fn传入i.components中
```go
	// Implement EnvoyXdsServer grace shutdown
	s.addStartFunc(func(stop <-chan struct{}) error {
		log.Infof("Starting ADS server")
		s.XDSServer.Start(stop)
		return nil
	})
// addStartFunc appends a function to be run. These are run synchronously in order,
// so the function should start a go routine if it needs to do anything blocking
func (s *Server) addStartFunc(fn server.Component) {
	s.server.RunComponent(fn)
}
func (i *instance) RunComponent(t Component) {
	select {
	case <-i.done:
		log.Warnf("attempting to run a new component after the server was shutdown")
	default:
		i.components <- t
	}
}

func (i *instance) Start(stop <-chan struct{}) error {
	shutdown := func() {
		close(i.done)
	}

	// First, drain all startup tasks and immediately return if any fail.
	for startupDone := false; !startupDone; {
		select {
		case next := <-i.components:
			if err := next(stop); err != nil {
				// Startup error: terminate and return the error.
				shutdown()
				return err
			}
		default:
			// We've drained all of the initial tasks.
			// Break out of the loop and run asynchronously.
			startupDone = true
		}
	}

```
- 追踪i.components发现会在grpc-server的Start方法中启动grpc实例前调用 Start 遍历i.components的方法，启动
```go
// D:\go_path\src\github.com\istio\istio\pilot\pkg\bootstrap\server.go
func (s *Server) Start(stop <-chan struct{}) error {
	log.Infof("Starting Istiod Server with primary cluster %s", s.clusterID)

	if features.UnsafeFeaturesEnabled() {
		log.Warn("Server is starting with unsafe features enabled")
	}

	// Now start all of the components.
	if err := s.server.Start(stop); err != nil {
		return err
	}
```


## 04 crd 的controller 
- controller的初始化位置在NewServer中
```go
func NewServer(args *PilotArgs, initFuncs ...func(*Server)) (*Server, error) {
	if err := s.initControllers(args); err != nil {
		return nil, err
	}
}

```
- 可以看到在 initControllers中初始化了4个Controller
```go
// initControllers initializes the controllers.
func (s *Server) initControllers(args *PilotArgs) error {
	log.Info("initializing controllers")
	// 多集群的
	s.initMulticluster(args)
	// 证书的
	if err := s.initCertController(args); err != nil {
		return fmt.Errorf("error initializing certificate controller: %v", err)
	}
	// crd的
	if err := s.initConfigController(args); err != nil {
		return fmt.Errorf("error initializing config controller: %v", err)
	}
	// svc的
	if err := s.initServiceControllers(args); err != nil {
		return fmt.Errorf("error initializing service controllers: %v", err)
	}
	return nil
}

```
- 这里我们以crd的为例讲解一下

### initConfigController 解读
- 这里我们应该关注的是initK8SConfigStore，其余两种代表外部的注册中心和基于文件的
```go
func (s *Server) initConfigController(args *PilotArgs) error {
    	} else {
    		err2 := s.initK8SConfigStore(args)
    		if err2 != nil {
    			return err2
    		}
    	}

}
```
- 在initK8SConfigStore中重点是这个makeKubeConfigController，可以看到调用的是crd相关的
```go
func (s *Server) initK8SConfigStore(args *PilotArgs) error {
	if s.kubeClient == nil {
		return nil
	}
	configController, err := s.makeKubeConfigController(args)
	if err != nil {
		return err
	}
func (s *Server) makeKubeConfigController(args *PilotArgs) (model.ConfigStoreCache, error) {
	return crdclient.New(s.kubeClient, args.Revision, args.RegistryOptions.KubeOptions.DomainSuffix)
}

```
- 这个crdclient.New的位置在 D:\go_path\src\github.com\istio\istio\pilot\pkg\config\kube\crdclient\client.go
- 这里可以看到这个schemas就是istio相关crd的Schemas
```go
func New(client kube.Client, revision, domainSuffix string) (model.ConfigStoreCache, error) {
	schemas := collections.Pilot
	if features.EnableGatewayAPI {
		schemas = collections.PilotGatewayAPI
	}
	return NewForSchemas(context.Background(), client, revision, domainSuffix, schemas)
}

```
- 追踪一下可以看到这个 collections.Pilot位置在 D:\go_path\src\github.com\istio\istio\pkg\config\schema\collections\collections.gen.go
- 其中使用构造者模式把相关的crd都添加进来了
```go
	// Pilot contains only collections used by Pilot.
	Pilot = collection.NewSchemasBuilder().
		MustAdd(IstioExtensionsV1Alpha1Wasmplugins).
		MustAdd(IstioNetworkingV1Alpha3Destinationrules).
		MustAdd(IstioNetworkingV1Alpha3Envoyfilters).
		MustAdd(IstioNetworkingV1Alpha3Gateways).
		MustAdd(IstioNetworkingV1Alpha3Serviceentries).
		MustAdd(IstioNetworkingV1Alpha3Sidecars).
		MustAdd(IstioNetworkingV1Alpha3Virtualservices).
		MustAdd(IstioNetworkingV1Alpha3Workloadentries).
		MustAdd(IstioNetworkingV1Alpha3Workloadgroups).
		MustAdd(IstioNetworkingV1Beta1Proxyconfigs).
		MustAdd(IstioSecurityV1Beta1Authorizationpolicies).
		MustAdd(IstioSecurityV1Beta1Peerauthentications).
		MustAdd(IstioSecurityV1Beta1Requestauthentications).
		MustAdd(IstioTelemetryV1Alpha1Telemetries).
		Build()
```
- 比如在istio中常用的Gateway
```go
	// IstioNetworkingV1Alpha3Gateways describes the collection
	// istio/networking/v1alpha3/gateways
	IstioNetworkingV1Alpha3Gateways = collection.Builder{
		Name:         "istio/networking/v1alpha3/gateways",
		VariableName: "IstioNetworkingV1Alpha3Gateways",
		Disabled:     false,
		Resource: resource.Builder{
			Group:   "networking.istio.io",
			Kind:    "Gateway",
			Plural:  "gateways",
			Version: "v1alpha3",
			Proto:   "istio.networking.v1alpha3.Gateway", StatusProto: "istio.meta.v1alpha1.IstioStatus",
			ReflectType: reflect.TypeOf(&istioioapinetworkingv1alpha3.Gateway{}).Elem(), StatusType: reflect.TypeOf(&istioioapimetav1alpha1.IstioStatus{}).Elem(),
			ProtoPackage: "istio.io/api/networking/v1alpha3", StatusPackage: "istio.io/api/meta/v1alpha1",
			ClusterScoped: false,
			ValidateProto: validation.ValidateGateway,
		}.MustBuild(),
	}.MustBuild()
```
- 还有virtualservices
```go
	// IstioNetworkingV1Alpha3Virtualservices describes the collection
	// istio/networking/v1alpha3/virtualservices
	IstioNetworkingV1Alpha3Virtualservices = collection.Builder{
		Name:         "istio/networking/v1alpha3/virtualservices",
		VariableName: "IstioNetworkingV1Alpha3Virtualservices",
		Disabled:     false,
		Resource: resource.Builder{
			Group:   "networking.istio.io",
			Kind:    "VirtualService",
			Plural:  "virtualservices",
			Version: "v1alpha3",
			Proto:   "istio.networking.v1alpha3.VirtualService", StatusProto: "istio.meta.v1alpha1.IstioStatus",
			ReflectType: reflect.TypeOf(&istioioapinetworkingv1alpha3.VirtualService{}).Elem(), StatusType: reflect.TypeOf(&istioioapimetav1alpha1.IstioStatus{}).Elem(),
			ProtoPackage: "istio.io/api/networking/v1alpha3", StatusPackage: "istio.io/api/meta/v1alpha1",
			ClusterScoped: false,
			ValidateProto: validation.ValidateVirtualService,
		}.MustBuild(),
	}.MustBuild()
```
- 回到上面的crdclient.New中可以看到会调用1个NewForSchemas
- 在NewForSchemas中会构造一个crdClient对象，其中有相关的informer和 kinds等map
```go
func NewForSchemas(ctx context.Context, client kube.Client, revision, domainSuffix string, schemas collection.Schemas) (model.ConfigStoreCache, error) {
	schemasByCRDName := map[string]collection.Schema{}
	for _, s := range schemas.All() {
		// From the spec: "Its name MUST be in the format <.spec.name>.<.spec.group>."
		name := fmt.Sprintf("%s.%s", s.Resource().Plural(), s.Resource().Group())
		schemasByCRDName[name] = s
	}
	out := &Client{
		domainSuffix:     domainSuffix,
		schemas:          schemas,
		schemasByCRDName: schemasByCRDName,
		revision:         revision,
		queue:            queue.NewQueue(1 * time.Second),
		kinds:            map[config.GroupVersionKind]*cacheHandler{},
		handlers:         map[config.GroupVersionKind][]model.EventHandler{},
		client:           client,
		istioClient:      client.Istio(),
		gatewayAPIClient: client.GatewayAPI(),
		crdMetadataInformer: client.MetadataInformer().ForResource(collections.K8SApiextensionsK8SIoV1Customresourcedefinitions.Resource().
			GroupVersionResource()).Informer(),
		beginSync:   atomic.NewBool(false),
		initialSync: atomic.NewBool(false),
	}

```
- 并且会通过knownCRDs 请求apiserver 获取注册的crd的资源，返回known的map
```go
	known, err := knownCRDs(ctx, client.Ext())
	if err != nil {
		return nil, err
	}
// knownCRDs returns all CRDs present in the cluster, with retries
func knownCRDs(ctx context.Context, crdClient apiextensionsclient.Interface) (map[string]struct{}, error) {
	delay := time.Second
	maxDelay := time.Minute
	var res *crd.CustomResourceDefinitionList
	for {
		if err := ctx.Err(); err != nil {
			return nil, err
		}
		var err error
		res, err = crdClient.ApiextensionsV1().CustomResourceDefinitions().List(ctx, metav1.ListOptions{})
		if err == nil {
			break
		}
		scope.Errorf("failed to list CRDs: %v", err)
		time.Sleep(delay)
		delay *= 2
		if delay > maxDelay {
			delay = maxDelay
		}
	}

	mp := map[string]struct{}{}
	for _, r := range res.Items {
		mp[r.Name] = struct{}{}
	}
	return mp, nil
}

```
- 然后遍历crd调用handleCRDAdd处理，
- handleCRDAdd内部会创建对应resourceGVK的handler
- handleCRDAdd内部会启动对应的informer
```go
func handleCRDAdd(cl *Client, name string, stop <-chan struct{}) {
	scope.Debugf("adding CRD %q", name)
	s, f := cl.schemasByCRDName[name]
	if !f {
		scope.Debugf("added resource that we are not watching: %v", name)
		return
	}
	resourceGVK := s.Resource().GroupVersionKind()
	gvr := s.Resource().GroupVersionResource()

	cl.kindsMu.Lock()
	defer cl.kindsMu.Unlock()
	if _, f := cl.kinds[resourceGVK]; f {
		scope.Debugf("added resource that already exists: %v", resourceGVK)
		return
	}
	var i informers.GenericInformer
	var ifactory starter
	var err error
	if s.Resource().Group() == gvk.KubernetesGateway.Group {
		ifactory = cl.client.GatewayAPIInformer()
		i, err = cl.client.GatewayAPIInformer().ForResource(gvr)
	} else {
		ifactory = cl.client.IstioInformer()
		i, err = cl.client.IstioInformer().ForResource(gvr)
	}

	if err != nil {
		// Shouldn't happen
		scope.Errorf("failed to create informer for %v", resourceGVK)
		return
	}
	// 创建对应resourceGVK的handler
	cl.kinds[resourceGVK] = createCacheHandler(cl, s, i)
	if w, f := crdWatches[resourceGVK]; f {
		scope.Infof("notifying watchers %v was created", resourceGVK)
		close(w)
	}
	if stop != nil {
		// Start informer factory, only if stop is defined. In startup case, we will not start here as
		// we will start all factories once we are ready to initialize.
		// For dynamically added CRDs, we need to start immediately though
		ifactory.Start(stop)
	}
}

```
- 上面这个createCacheHandler中定义了对应crd的回调方法
```go
func createCacheHandler(cl *Client, schema collection.Schema, i informers.GenericInformer) *cacheHandler {
	scope.Debugf("registered CRD %v", schema.Resource().GroupVersionKind())
	h := &cacheHandler{
		client:   cl,
		schema:   schema,
		informer: i.Informer(),
	}
	h.lister = func(namespace string) cache.GenericNamespaceLister {
		if schema.Resource().IsClusterScoped() {
			return i.Lister()
		}
		return i.Lister().ByNamespace(namespace)
	}
	kind := schema.Resource().Kind()
	i.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
		AddFunc: func(obj interface{}) {
			incrementEvent(kind, "add")
			if !cl.beginSync.Load() {
				return
			}
			cl.queue.Push(func() error {
				return h.onEvent(nil, obj, model.EventAdd)
			})
		},
		UpdateFunc: func(old, cur interface{}) {
			incrementEvent(kind, "update")
			if !cl.beginSync.Load() {
				return
			}
			cl.queue.Push(func() error {
				return h.onEvent(old, cur, model.EventUpdate)
			})
		},
		DeleteFunc: func(obj interface{}) {
			incrementEvent(kind, "delete")
			if !cl.beginSync.Load() {
				return
			}
			cl.queue.Push(func() error {
				return h.onEvent(nil, obj, model.EventDelete)
			})
		},
	})
	return h
}

```
- 在回调方法内部可以看到就是向cl.queue push一个 h.onEvent的task，后续的调谐就是执行这个task
- 那么在onEvent中可以看到 是从h.client.handlers 这个map中根据GroupVersionKind获取到对应的func数组，遍历执行
```go
func (h *cacheHandler) onEvent(old interface{}, curr interface{}, event model.Event) error {

	// TODO we may consider passing a pointer to handlers instead of the value. While spec is a pointer, the meta will be copied
	for _, f := range h.client.handlers[h.schema.Resource().GroupVersionKind()] {
		f(oldConfig, currConfig, event)
	}
	return nil
}
```
- 那么追踪这个map中 对应的GroupVersionKind的方法数组的注册
- 对应的方法就是RegisterEventHandler，位置 D:\go_path\src\github.com\istio\istio\pilot\pkg\config\kube\crdclient\client.go
```go
func (cl *Client) RegisterEventHandler(kind config.GroupVersionKind, handler model.EventHandler) {
	cl.handlers[kind] = append(cl.handlers[kind], handler)
}
```

- 追踪RegisterEventHandler的调用方发现有ServiceEntry ，位置 D:\go_path\src\github.com\istio\istio\pilot\pkg\serviceregistry\serviceentry\servicediscovery.go
```go
	if configController != nil {
		if s.processServiceEntry {
			configController.RegisterEventHandler(gvk.ServiceEntry, s.serviceEntryHandler)
		}
		configController.RegisterEventHandler(gvk.WorkloadEntry, s.workloadEntryHandler)
		_ = configController.SetWatchErrorHandler(informermetric.ErrorHandlerForCluster(s.clusterID))
	}
```
- ServiceEntry的回调处理方法就是serviceEntryHandler
- 在serviceEntryHandler中可以看到有对应的更新操作
```go
// serviceEntryHandler defines the handler for service entries
func (s *ServiceEntryStore) serviceEntryHandler(old, curr config.Config, event model.Event) {
	cs := convertServices(curr)
	configsUpdated := map[model.ConfigKey]struct{}{}

	// If it is add/delete event we should always do a full push. If it is update event, we should do full push,
	// only when services have changed - otherwise, just push endpoint updates.
	var addedSvcs, deletedSvcs, updatedSvcs, unchangedSvcs []*model.Service

	switch event {
	case model.EventUpdate:
		os := convertServices(old)
		if selectorChanged(old, curr) {
			// Consider all services are updated.
			mark := make(map[host.Name]*model.Service, len(cs))
			for _, svc := range cs {
				mark[svc.Hostname] = svc
				updatedSvcs = append(updatedSvcs, svc)
			}
			for _, svc := range os {
				if _, f := mark[svc.Hostname]; !f {
					updatedSvcs = append(updatedSvcs, svc)
				}
			}
		} else {
			addedSvcs, deletedSvcs, updatedSvcs, unchangedSvcs = servicesDiff(os, cs)
		}
	case model.EventDelete:
		deletedSvcs = cs
	case model.EventAdd:
		addedSvcs = cs
	default:
		// this should not happen
		unchangedSvcs = cs
	}

	shard := model.ShardKeyFromRegistry(s)
	for _, svc := range addedSvcs {
		s.XdsUpdater.SvcUpdate(shard, string(svc.Hostname), svc.Attributes.Namespace, model.EventAdd)
		configsUpdated[makeConfigKey(svc)] = struct{}{}
	}

	for _, svc := range updatedSvcs {
		s.XdsUpdater.SvcUpdate(shard, string(svc.Hostname), svc.Attributes.Namespace, model.EventUpdate)
		configsUpdated[makeConfigKey(svc)] = struct{}{}
	}

	// If service entry is deleted, cleanup endpoint shards for services.
	for _, svc := range deletedSvcs {
		s.XdsUpdater.SvcUpdate(shard, string(svc.Hostname), svc.Attributes.Namespace, model.EventDelete)
		configsUpdated[makeConfigKey(svc)] = struct{}{}
	}
```