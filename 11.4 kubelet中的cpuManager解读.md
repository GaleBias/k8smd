# 本节重点总结 :

- kubelet使用cpuSet的要求

  - 首先要求是 Guaanteed 类型，Burstable 和BestEffort 不能设置cpu set，只能运行在cpu共享池中
  - 容器对 CPU 资源的限制值是一个大于或等于 1 的整数值，不能是小数，如果是小数的话也只能运行在cpu共享池中
  - 这种Guaanteed静态分配增强了 CPU 亲和性，减少了 CPU 密集的工作负载在节流时引起的上下文切换。
- cpuManager开启同步任务reconcileState

  - 对比容器的cpuset的状态，和上次不同就更新
- 容器创建的时候会allocateCPUs分配
- kubelet将cpu分为这几大块

  - 预留给system/kube的      饭店预留的座位
  - 分配给绑定核的Guaanteed 容器   包间
  - 用cfs调度的共享池    大厅、大堂

# k8s 和cpuSet

- 默认情况下，kubelet 使用 CFS 配额 来执行 Pod 的 CPU 约束
- 当节点上运行了很多 CPU 密集的 Pod 时，工作负载可能会迁移到不同的 CPU 核， 这取决于调度时 Pod 是否被扼制，以及哪些 CPU 核是可用的。
- 许多工作负载对这种迁移不敏感，因此无需任何干预即可正常工作。
- 然而，有些工作负载的性能明显地受到 CPU 缓存亲和性以及调度延迟的影响
- 对此，kubelet 提供了可选的 CPU 管理策略，来确定节点上的一些分配偏好。

> 发展历史

- 直到Kubernetes 1.8开始，Kubernetes提供了CPU Manager特性来支持cpuset的能力
- 从Kubernetes 1.10版本开始到1.12，该特性还是Beta版。
- CPU Manager是Kubelet CM中的一个模块，目标是通过给某些Containers绑定指定的cpus，达到绑定cpus的目标，从而提升这些cpu敏感型任务的性能。

> 使用手册

- 首先要求是 Guaanteed 类型，Burstable 和BestEffort 不能设置cpu set，只能运行在cpu共享池中
- 容器对 CPU 资源的限制值是一个大于或等于 1 的整数值，不能是小数，如果是小数的话也只能运行在cpu共享池中
- 这种Guaanteed静态分配增强了 CPU 亲和性，减少了 CPU 密集的工作负载在节流时引起的上下文切换。

# cpuManager 接口定义

- 位置 D:\go_path\src\github.com\kubernetes\kubernetes\pkg\kubelet\cm\cpumanager\cpu_manager.go

```go
// Manager interface provides methods for Kubelet to manage pod cpus.
type Manager interface {
	// Start is called during Kubelet initialization.
	Start(activePods ActivePodsFunc, sourcesReady config.SourcesReady, podStatusProvider status.PodStatusProvider, containerRuntime runtimeService, initialContainers containermap.ContainerMap) error

	// Called to trigger the allocation of CPUs to a container. This must be
	// called at some point prior to the AddContainer() call for a container,
	// e.g. at pod admission time.
	Allocate(pod *v1.Pod, container *v1.Container) error

	// AddContainer adds the mapping between container ID to pod UID and the container name
	// The mapping used to remove the CPU allocation during the container removal
	AddContainer(p *v1.Pod, c *v1.Container, containerID string)

	// RemoveContainer is called after Kubelet decides to kill or delete a
	// container. After this call, the CPU manager stops trying to reconcile
	// that container and any CPUs dedicated to the container are freed.
	RemoveContainer(containerID string) error

	// State returns a read-only interface to the internal CPU manager state.
	State() state.Reader

	// GetTopologyHints implements the topologymanager.HintProvider Interface
	// and is consulted to achieve NUMA aware resource alignment among this
	// and other resource controllers.
	GetTopologyHints(*v1.Pod, *v1.Container) map[string][]topologymanager.TopologyHint

	// GetCPUs implements the podresources.CPUsProvider interface to provide allocated
	// cpus for the container
	GetCPUs(podUID, containerName string) cpuset.CPUSet

	// GetPodTopologyHints implements the topologymanager.HintProvider Interface
	// and is consulted to achieve NUMA aware resource alignment per Pod
	// among this and other resource controllers.
	GetPodTopologyHints(pod *v1.Pod) map[string][]topologymanager.TopologyHint

	// GetAllocatableCPUs returns the assignable (not allocated) CPUs
	GetAllocatableCPUs() cpuset.CPUSet
}
```

# cpuManager 结构体定义

- 位置

```go
type manager struct {
	sync.Mutex
	policy Policy

	// reconcilePeriod is the duration between calls to reconcileState.
	reconcilePeriod time.Duration

	// state allows pluggable CPU assignment policies while sharing a common
	// representation of state for the system to inspect and reconcile.
	state state.State

	// lastUpdatedstate holds state for each container from the last time it was updated.
	lastUpdateState state.State

	// containerRuntime is the container runtime service interface needed
	// to make UpdateContainerResources() calls against the containers.
	containerRuntime runtimeService

	// activePods 是获取node上的活跃pod
	activePods ActivePodsFunc

	// podStatusProvider provides a method for obtaining pod statuses
	// and the containerID of their containers
	podStatusProvider status.PodStatusProvider

	// containerMap provides a mapping from (pod, container) -> containerID
	// for all containers a pod
	containerMap containermap.ContainerMap

	topology *topology.CPUTopology

	nodeAllocatableReservation v1.ResourceList

	// sourcesReady provides the readiness of kubelet configuration sources such as apiserver update readiness.
	// We use it to determine when we can purge inactive pods from checkpointed state.
	sourcesReady config.SourcesReady

	// stateFileDirectory holds the directory where the state file for checkpoints is held.
	stateFileDirectory string

	// allocatableCPUs is the set of online CPUs as reported by the system
	allocatableCPUs cpuset.CPUSet

	// pendingAdmissionPod contain the pod during the admission phase
	pendingAdmissionPod *v1.Pod
}

```

# cpuManager 的初始化

- 入口在containerManager 的初始化中，位置 D:\go_path\src\github.com\kubernetes\kubernetes\pkg\kubelet\cm\container_manager_linux.go
- 首先判断 是否开启了CPUManager 功能，调用cpumanager.NewManager创建

```go
	// Initialize CPU manager
	if utilfeature.DefaultFeatureGate.Enabled(kubefeatures.CPUManager) {
		cm.cpuManager, err = cpumanager.NewManager(
			nodeConfig.ExperimentalCPUManagerPolicy,
			nodeConfig.ExperimentalCPUManagerPolicyOptions,
			nodeConfig.ExperimentalCPUManagerReconcilePeriod,
			machineInfo,
			nodeConfig.NodeAllocatableConfig.ReservedSystemCPUs,
			cm.GetNodeAllocatableReservation(),
			nodeConfig.KubeletRootDir,
			cm.topologyManager,
		)
		if err != nil {
			klog.ErrorS(err, "Failed to initialize cpu manager")
			return nil, err
		}
		cm.topologyManager.AddHintProvider(cm.cpuManager)
	}
```

## cpumanager.NewManager分析

- 根据cpuPolicyName策略决定 启用哪个策略

```go
func NewManager(cpuPolicyName string, cpuPolicyOptions map[string]string, reconcilePeriod time.Duration, machineInfo *cadvisorapi.MachineInfo, specificCPUs cpuset.CPUSet, nodeAllocatableReservation v1.ResourceList, stateFileDirectory string, affinity topologymanager.Store) (Manager, error) {
	var topo *topology.CPUTopology
	var policy Policy
	var err error

	switch policyName(cpuPolicyName) {

	case PolicyNone:
		policy, err = NewNonePolicy(cpuPolicyOptions)
		if err != nil {
			return nil, fmt.Errorf("new none policy error: %w", err)
		}
```

- cpuPolicyName来自 --cpu-manager-policy给定的cpuManager策略，选项有none 和static，默认为none

### none 策略

- none 策略显式地启用现有的默认 CPU 亲和方案，不提供操作系统调度器默认行为之外的亲和性策略
- 通过 CFS 配额来实现 Guaranteed pods 和 Burstable pods 的 CPU 使用限制
- 可以看到policy_none的所有方法都是nil，位置 D:\go_path\src\github.com\kubernetes\kubernetes\pkg\kubelet\cm\cpumanager\policy_none.go

```go
type nonePolicy struct{}

var _ Policy = &nonePolicy{}

// PolicyNone name of none policy
const PolicyNone policyName = "none"

// NewNonePolicy returns a cpuset manager policy that does nothing
func NewNonePolicy(cpuPolicyOptions map[string]string) (Policy, error) {
	if len(cpuPolicyOptions) > 0 {
		return nil, fmt.Errorf("None policy: received unsupported options=%v", cpuPolicyOptions)
	}
	return &nonePolicy{}, nil
}

func (p *nonePolicy) Name() string {
	return string(PolicyNone)
}

func (p *nonePolicy) Start(s state.State) error {
	klog.InfoS("None policy: Start")
	return nil
}

func (p *nonePolicy) Allocate(s state.State, pod *v1.Pod, container *v1.Container) error {
	return nil
}

func (p *nonePolicy) RemoveContainer(s state.State, podUID string, containerName string) error {
	return nil
}

func (p *nonePolicy) GetTopologyHints(s state.State, pod *v1.Pod, container *v1.Container) map[string][]topologymanager.TopologyHint {
	return nil
}

func (p *nonePolicy) GetPodTopologyHints(s state.State, pod *v1.Pod) map[string][]topologymanager.TopologyHint {
	return nil
}

```

### static 策略

- static 策略针对具有整数型 CPU requests 的 Guaranteed Pod
- 它允许该类 Pod 中的容器访问节点上的独占 CPU 资源
- 这种独占性是使用 cpuset cgroup 控制器 来实现的。

> 源码解读

- 首先根据cadvisor提供的 machineInfo初始化 CPUTopology

```go
		topo, err = topology.Discover(machineInfo)
		if err != nil {
			return nil, err
		}
```

- 返回的结构为

```go
type CPUTopology struct {
	NumCPUs    int
	NumCores   int
	NumSockets int
	CPUDetails CPUDetails
}
```

- 获取为节点的预留的cpu资源，对应的命令行参数为 --reserved-cpus=1,2,3代表1-3个核是预留的

```go
		reservedCPUs, ok := nodeAllocatableReservation[v1.ResourceCPU]
		if !ok {
			// The static policy cannot initialize without this information.
			return nil, fmt.Errorf("[cpumanager] unable to determine reserved CPU resources for static policy")
		}
		if reservedCPUs.IsZero() {
			// The static policy requires this to be nonzero. Zero CPU reservation
			// would allow the shared pool to be completely exhausted. At that point
			// either we would violate our guarantee of exclusivity or need to evict
			// any pod that has at least one container that requires zero CPUs.
			// See the comments in policy_static.go for more details.
			return nil, fmt.Errorf("[cpumanager] the static policy requires systemreserved.cpu + kubereserved.cpu to be greater than zero")
		}

		// Take the ceiling of the reservation, since fractional CPUs cannot be
		// exclusively allocated.
		reservedCPUsFloat := float64(reservedCPUs.MilliValue()) / 1000
		numReservedCPUs := int(math.Ceil(reservedCPUsFloat))
```

- 然后初始化StaticPolicy

```go
		policy, err = NewStaticPolicy(topo, numReservedCPUs, specificCPUs, affinity, cpuPolicyOptions)
		if err != nil {
			return nil, fmt.Errorf("new static policy error: %w", err)
		}
```

# cpuManager 运行

- 入口在containerManager 的start中，位置 D:\go_path\src\github.com\kubernetes\kubernetes\pkg\kubelet\cm\container_manager_linux.go
- containerMap 是 containerID->(*v1.Pod, *v1.Container) 的map

```go
	// Initialize CPU manager
	if utilfeature.DefaultFeatureGate.Enabled(kubefeatures.CPUManager) {
		containerMap, err := buildContainerMapFromRuntime(runtimeService)
		if err != nil {
			return fmt.Errorf("failed to build map of initial containers from runtime: %v", err)
		}
		err = cm.cpuManager.Start(cpumanager.ActivePodsFunc(activePods), sourcesReady, podStatusProvider, runtimeService, containerMap)
		if err != nil {
			return fmt.Errorf("start cpu manager error: %v", err)
		}
	}
```

## cpuManager.Start

- 位置D:\go_path\src\github.com\kubernetes\kubernetes\pkg\kubelet\cm\cpumanager\cpu_manager.go
- 首先创建 CheckpointState ，目的是保存cpuset的状态，为了重启redo

```go
	stateImpl, err := state.NewCheckpointState(m.stateFileDirectory, cpuManagerStateFileName, m.policy.Name(), m.containerMap)
	if err != nil {
		klog.ErrorS(err, "Could not initialize checkpoint manager, please drain node and remove policy state file")
		return err
	}
	m.state = stateImpl

```

- 对应的目录为/var/lib/kubelet/cpu_manager_state ，

```shell
 cat /var/lib/kubelet/cpu_manager_state
{"policyName":"none","defaultCpuSet":"","checksum":1353318690}
```

- 启动对应的policy

```go
	err = m.policy.Start(m.state)
	if err != nil {
		klog.ErrorS(err, "Policy start error")
		return err
	}
```

- 对应的staticPolicy的start就是validateState校验state

```go
func (p *staticPolicy) Start(s state.State) error {
	if err := p.validateState(s); err != nil {
		klog.ErrorS(err, "Static policy invalid state, please drain node and remove policy state file")
		return err
	}
	return nil
}
```

### validateState 校验缓存的数据解析

- 首先根据checkpoint缓存读取已经分配的cpuset

```go
	tmpAssignments := s.GetCPUAssignments()
func (s *stateMemory) GetCPUAssignments() ContainerCPUAssignments {
	s.RLock()
	defer s.RUnlock()
	return s.assignments.Clone()
}
// ContainerCPUAssignments type used in cpu manager state
type ContainerCPUAssignments map[string]map[string]cpuset.CPUSet

// Clone returns a copy of ContainerCPUAssignments
func (as ContainerCPUAssignments) Clone() ContainerCPUAssignments {
	ret := make(ContainerCPUAssignments)
	for pod := range as {
		ret[pod] = make(map[string]cpuset.CPUSet)
		for container, cset := range as[pod] {
			ret[pod][container] = cset
		}
	}
	return ret
}

```

- 从上面的clone得知 stateMemory中的assignments是一个双层map，第一层是pod的id，第二层是 容器id对应cpuset的map

```go
type stateMemory struct {
	sync.RWMutex
	assignments   ContainerCPUAssignments
	defaultCPUSet cpuset.CPUSet
}
```

- 获取本机默认的cpuset

```go
tmpDefaultCPUset := s.GetDefaultCPUSet()
func (s *stateMemory) GetDefaultCPUSet() cpuset.CPUSet {
	s.RLock()
	defer s.RUnlock()

	return s.defaultCPUSet.Clone()
}
// Clone returns a copy of this CPU set.
func (s CPUSet) Clone() CPUSet {
	b := NewBuilder()
	for elem := range s.elems {
		b.Add(elem)
	}
	return b.Result()
}

```

- 从上面的clone可以知道 defaultCPUSet就是机器上可用的cpu核心
- 用cadvisor获取到的node cpu拓扑初始化defaultCPUSet

```go
	// Default cpuset cannot be empty when assignments exist
	if tmpDefaultCPUset.IsEmpty() {
		if len(tmpAssignments) != 0 {
			return fmt.Errorf("default cpuset cannot be empty")
		}
		// state is empty initialize
		allCPUs := p.topology.CPUDetails.CPUs()
		s.SetDefaultCPUSet(allCPUs)
		return nil
	}

```

- 检查预留的cpuset是否是 全部核的一部分，可能的原因是kube/system的预留cpu核发生了变化，这可能导致某些容器起不来

```go
	if !p.reserved.Intersection(tmpDefaultCPUset).Equals(p.reserved) {
		return fmt.Errorf("not all reserved cpus: \"%s\" are present in defaultCpuSet: \"%s\"",
			p.reserved.String(), tmpDefaultCPUset.String())
	}
```

- 检查之前配置的cpuset是否已经不再node可用的中了

```go
	// 2. Check if state for static policy is consistent
	for pod := range tmpAssignments {
		for container, cset := range tmpAssignments[pod] {
			// None of the cpu in DEFAULT cset should be in s.assignments
			if !tmpDefaultCPUset.Intersection(cset).IsEmpty() {
				return fmt.Errorf("pod: %s, container: %s cpuset: \"%s\" overlaps with default cpuset \"%s\"",
					pod, container, cset.String(), tmpDefaultCPUset.String())
			}
		}
	}
```

- 检查可用的cpuset和已经分配的合集是否和节点的所有cpu一致，可能发生的原因是某些cpu核心下架了，拔了一颗cpu么

```go
	totalKnownCPUs := tmpDefaultCPUset.Clone()
	tmpCPUSets := []cpuset.CPUSet{}
	for pod := range tmpAssignments {
		for _, cset := range tmpAssignments[pod] {
			tmpCPUSets = append(tmpCPUSets, cset)
		}
	}
	totalKnownCPUs = totalKnownCPUs.UnionAll(tmpCPUSets)
	if !totalKnownCPUs.Equals(p.topology.CPUDetails.CPUs()) {
		return fmt.Errorf("current set of available CPUs \"%s\" doesn't match with CPUs in state \"%s\"",
			p.topology.CPUDetails.CPUs().String(), totalKnownCPUs.String())
	}

```

### 周期执行同步任务reconcileState

```go
	// Periodically call m.reconcileState() to continue to keep the CPU sets of
	// all pods in sync with and guaranteed CPUs handed out among them.
	go wait.Until(func() { m.reconcileState() }, m.reconcilePeriod, wait.NeverStop)
```

> Reconcile循环处理宿主机上的activePods，主要做了3件事：

- 找到containerID
- 获取这个container需要绑定的cpuset
- 更新这个container

> 源码分析

- 开始遍历activePods，获取podstatus

```go
	for _, pod := range m.activePods() {
		pstatus, ok := m.podStatusProvider.GetPodStatus(pod.UID)
		if !ok {
			klog.V(4).InfoS("ReconcileState: skipping pod; status not found", "pod", klog.KObj(pod))
			failure = append(failure, reconciledContainer{pod.Name, "", ""})
			continue
		}
```

- 遍历pod的容器获取容器id，如果出错则跳过。把init容器也加入尽量

```go
		allContainers := pod.Spec.InitContainers
		allContainers = append(allContainers, pod.Spec.Containers...)
		for _, container := range allContainers {
			containerID, err := findContainerIDByName(&pstatus, container.Name)
			if err != nil {
				klog.V(4).InfoS("ReconcileState: skipping container; ID not found in pod status", "pod", klog.KObj(pod), "containerName", container.Name, "err", err)
				failure = append(failure, reconciledContainer{pod.Name, container.Name, ""})
				continue
			}
```

- 获取容器的状态

```go
		cstatus, err := findContainerStatusByName(&pstatus, container.Name)
			if err != nil {
				klog.V(4).InfoS("ReconcileState: skipping container; container status not found in pod status", "pod", klog.KObj(pod), "containerName", container.Name, "err", err)
				failure = append(failure, reconciledContainer{pod.Name, container.Name, ""})
				continue
			}
```

- 跳过waiting状态的容器

```go
			if cstatus.State.Waiting != nil ||
				(cstatus.State.Waiting == nil && cstatus.State.Running == nil && cstatus.State.Terminated == nil) {
				klog.V(4).InfoS("ReconcileState: skipping container; container still in the waiting state", "pod", klog.KObj(pod), "containerName", container.Name, "err", err)
				failure = append(failure, reconciledContainer{pod.Name, container.Name, ""})
				continue
			}

```

- 跳过Terminated状态的容器

```go
			if cstatus.State.Terminated != nil {
				// The container is terminated but we can't call m.RemoveContainer()
				// here because it could remove the allocated cpuset for the container
				// which may be in the process of being restarted.  That would result
				// in the container losing any exclusively-allocated CPUs that it
				// was allocated.
				_, _, err := m.containerMap.GetContainerRef(containerID)
				if err == nil {
					klog.V(4).InfoS("ReconcileState: ignoring terminated container", "pod", klog.KObj(pod), "containerID", containerID)
				}
				m.Unlock()
				continue
			}

```

- 获取容器的cpuset

```go

			cset := m.state.GetCPUSetOrDefault(string(pod.UID), container.Name)
			if cset.IsEmpty() {
				// NOTE: This should not happen outside of tests.
				klog.V(4).InfoS("ReconcileState: skipping container; assigned cpuset is empty", "pod", klog.KObj(pod), "containerName", container.Name)
				failure = append(failure, reconciledContainer{pod.Name, container.Name, containerID})
				continue
			}
```

- 对应的就是到stateMemory缓存中根据podUID, containerName去双层map中获取，如果没有说明容器没有被设置cpuSet则返回节点可用的cpuset

```go
func (s *stateMemory) GetCPUSetOrDefault(podUID string, containerName string) cpuset.CPUSet {
	if res, ok := s.GetCPUSet(podUID, containerName); ok {
		return res
	}
	return s.GetDefaultCPUSet()
}

```

- 同理去lastUpdateState上次的同步时的缓存中获取容器的cpuset

```go
			lcset := m.lastUpdateState.GetCPUSetOrDefault(string(pod.UID), container.Name)
```

- 如果两边不一致就更新lastUpdateState，并调用container runtime应用，

```go
			if !cset.Equals(lcset) {
				klog.V(4).InfoS("ReconcileState: updating container", "pod", klog.KObj(pod), "containerName", container.Name, "containerID", containerID, "cpuSet", cset)
				err = m.updateContainerCPUSet(containerID, cset)
				if err != nil {
					klog.ErrorS(err, "ReconcileState: failed to update container", "pod", klog.KObj(pod), "containerName", container.Name, "containerID", containerID, "cpuSet", cset)
					failure = append(failure, reconciledContainer{pod.Name, container.Name, containerID})
					continue
				}
				m.lastUpdateState.SetCPUSet(string(pod.UID), container.Name, cset)
			}
```

# 容器所需的cpuset是在哪里写入的

- 在上面的分析知道cpuset缓存有 GetCPUSet获取方法，那么一定有set写入的方法
- 位置 D:\go_path\src\github.com\kubernetes\kubernetes\pkg\kubelet\cm\cpumanager\state\state_mem.go

```go
type stateMemory struct {
	sync.RWMutex
	assignments   ContainerCPUAssignments
	defaultCPUSet cpuset.CPUSet
}

func (s *stateMemory) GetCPUSet(podUID string, containerName string) (cpuset.CPUSet, bool) {
	s.RLock()
	defer s.RUnlock()

	res, ok := s.assignments[podUID][containerName]
	return res.Clone(), ok
}

```

- SetCPUSet写入方法

```go
func (s *stateMemory) SetCPUSet(podUID string, containerName string, cset cpuset.CPUSet) {
	s.Lock()
	defer s.Unlock()

	if _, ok := s.assignments[podUID]; !ok {
		s.assignments[podUID] = make(map[string]cpuset.CPUSet)
	}

	s.assignments[podUID][containerName] = cset
	klog.InfoS("Updated desired CPUSet", "podUID", podUID, "containerName", containerName, "cpuSet", cset)
}

```

- 追踪SetCPUSet的调用发现staticPolicy中的Allocate调用了

```go
		// Allocate CPUs according to the NUMA affinity contained in the hint.
		cpuset, err := p.allocateCPUs(s, numCPUs, hint.NUMANodeAffinity, p.cpusToReuse[string(pod.UID)])
		if err != nil {
			klog.ErrorS(err, "Unable to allocate CPUs", "pod", klog.KObj(pod), "containerName", container.Name, "numCPUs", numCPUs)
			return err
		}
		s.SetCPUSet(string(pod.UID), container.Name, cpuset)
		p.updateCPUsToReuse(pod, container, cpuset)
```

## allocateCPUs代表分配cpuset

- 首先获取可用的cpu核心

```go
	allocatableCPUs := p.GetAllocatableCPUs(s).Union(reusableCPUs)

```

- 如果设置了numaAffinity，代表numa拓扑亲和性，使用这个策略![image](http://bingerambo.com/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/example-numa-system.png)![image](http://bingerambo.com/posts/2021/01/k8s-affinity-topology-feature%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/numa-hint-provider1.png)

```go
	// If there are aligned CPUs in numaAffinity, attempt to take those first.
	result := cpuset.NewCPUSet()
	if numaAffinity != nil {
		alignedCPUs := cpuset.NewCPUSet()
		for _, numaNodeID := range numaAffinity.GetBits() {
			alignedCPUs = alignedCPUs.Union(allocatableCPUs.Intersection(p.topology.CPUDetails.CPUsInNUMANodes(numaNodeID)))
		}

		numAlignedToAlloc := alignedCPUs.Size()
		if numCPUs < numAlignedToAlloc {
			numAlignedToAlloc = numCPUs
		}

		alignedCPUs, err := takeByTopology(p.topology, alignedCPUs, numAlignedToAlloc)
		if err != nil {
			return cpuset.NewCPUSet(), err
		}

		result = result.Union(alignedCPUs)
	}
```

### takeByTopology 代表cpu拓扑

- CPU Manager为满足条件的Container分配指定的CPUs时，会尽量按照CPU Topology来分配，也就是考虑CPU Affinity
- 按照如下的优先顺序进行CPUs选择：（Logic CPUs就是Hyperthreads）

> 特点如下

- 如果Container请求的Logic CPUs数量不小于单块CPU Socket中Logci CPUs数量，那么会优先把整块CPU Socket中的Logic CPUs分配给该Container。
- 如果Container剩余请求的Logic CPUs数量不小于单块物理CPU Core提供的Logic CPUs数量，那么会优先把整块物理CPU Core上的Logic CPUs分配给该Container。
- Container剩余请求的Logic CPUs则从按照如下规则排好序的Logic CPUs列表中选择：
  - 同一个socket上的可用核
  - 统一个core上的可用核
- 总结就是尽量在一个socket上分配

> 代码如下

```go
func takeByTopology(topo *topology.CPUTopology, availableCPUs cpuset.CPUSet, numCPUs int) (cpuset.CPUSet, error) {
	acc := newCPUAccumulator(topo, availableCPUs, numCPUs)
	if acc.isSatisfied() {
		return acc.result, nil
	}
	if acc.isFailed() {
		return cpuset.NewCPUSet(), fmt.Errorf("not enough cpus available to satisfy request")
	}

	// Algorithm: topology-aware best-fit
	// 1. Acquire whole sockets, if available and the container requires at
	//    least a socket's-worth of CPUs.
	acc.takeFullSockets()
	if acc.isSatisfied() {
		return acc.result, nil
	}

	// 2. Acquire whole cores, if available and the container requires at least
	//    a core's-worth of CPUs.
	acc.takeFullCores()
	if acc.isSatisfied() {
		return acc.result, nil
	}

	// 3. Acquire single threads, preferring to fill partially-allocated cores
	//    on the same sockets as the whole cores we have already taken in this
	//    allocation.
	acc.takeRemainingCPUs()
	if acc.isSatisfied() {
		return acc.result, nil
	}

	return cpuset.NewCPUSet(), fmt.Errorf("failed to allocate cpus")
}

```

# 本节重点总结 :

- kubelet使用cpuSet的要求

  - 首先要求是 Guaanteed 类型，Burstable 和BestEffort 不能设置cpu set，只能运行在cpu共享池中
  - 容器对 CPU 资源的限制值是一个大于或等于 1 的整数值，不能是小数，如果是小数的话也只能运行在cpu共享池中
  - 这种Guaanteed静态分配增强了 CPU 亲和性，减少了 CPU 密集的工作负载在节流时引起的上下文切换。
- cpuManager开启同步任务reconcileState

  - 对比容器的cpuset的状态，和上次不同就更新
- 容器创建的时候会allocateCPUs分配
- kubelet将cpu分为这几大块

  - 预留给system/kube的
  - 分配给绑定核的Guaanteed 容器
  - 用cfs调度的共享池