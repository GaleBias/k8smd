# 本节重点总结：

- 所以这就是滚动更新过程：
    - 先启动少量新版本
    - 然后缩掉少量旧版本
    - 再启动少量新版本
    - 然后再缩掉少量旧版本
    - 直到直到 newRS replicas 扩容到 10，oldRSs replicas 缩容至 0
    - 同时在过程中要保证总数的上限maxSurge和不可用的下限maxUnavailable



# 滚动更新
- deployment 的更新方式有两种：重新创建或者滚动更新
- 其中滚动更新是最常用的，下面就看看其具体的实现。
```go
func (dc *DeploymentController) syncDeployment(key string) error {
	switch d.Spec.Strategy.Type {
	case apps.RecreateDeploymentStrategyType:
		return dc.rolloutRecreate(d, rsList, podMap)
	case apps.RollingUpdateDeploymentStrategyType:
		return dc.rolloutRolling(d, rsList)
	}
	return fmt.Errorf("unexpected deployment strategy type: %s", d.Spec.Strategy.Type)
}
```

## 滚动更新源码
- 调用 getAllReplicaSetsAndSyncRevision 获取所有的 rs，若没有 newRS 则创建；
```go

// rolloutRolling implements the logic for rolling a new replica set.
func (dc *DeploymentController) rolloutRolling(d *apps.Deployment, rsList []*apps.ReplicaSet) error {
	newRS, oldRSs, err := dc.getAllReplicaSetsAndSyncRevision(d, rsList, true)
	if err != nil {
		return err
	}
	allRSs := append(oldRSs, newRS)


```
### 调用 reconcileNewReplicaSet 判断是否需要对 newRS 进行 scaleUp 操作；
```go
	// Scale up, if we can.
	scaledUp, err := dc.reconcileNewReplicaSet(allRSs, newRS, d)
	if err != nil {
		return err
	}

```
- reconcileNewReplicaSet中有几个判断
- 如果newRS的副本和dep一致说明已完成，不需要新增
```go
func (dc *DeploymentController) reconcileNewReplicaSet(allRSs []*apps.ReplicaSet, newRS *apps.ReplicaSet, deployment *apps.Deployment) (bool, error) {
	if *(newRS.Spec.Replicas) == *(deployment.Spec.Replicas) {
		// Scaling not required.
		return false, nil
	}
```
- 如果newRS的副本大于dep说明需要缩容
```go
	if *(newRS.Spec.Replicas) > *(deployment.Spec.Replicas) {
		// Scale down.
		scaled, _, err := dc.scaleReplicaSetAndRecordEvent(newRS, *(deployment.Spec.Replicas), deployment)
		return scaled, err
	}
```
- 如果需要 scale ，则更新 rs 的 annotation 以及 rs.Spec.Replicas
```go
	scaled, _, err := dc.scaleReplicaSetAndRecordEvent(newRS, newReplicasCount, deployment)
	return scaled, err
```

### 执行 scale down 操作
```go
	// Scale down, if we can.
	scaledDown, err := dc.reconcileOldReplicaSets(allRSs, controller.FilterActiveReplicaSets(oldRSs), newRS, d)
	if err != nil {
		return err
	}
	if scaledDown {
		// Update DeploymentStatus
		return dc.syncRolloutStatus(allRSs, newRS, d)
	}

```
#### reconcileOldReplicaSets判断 scale down操作
- 首先遍历oldRSs计算副本数量和oldPodsCount，如果oldPodsCount为0 ，说明旧版本已经全部停止了
```go
func (dc *DeploymentController) reconcileOldReplicaSets(allRSs []*apps.ReplicaSet, oldRSs []*apps.ReplicaSet, newRS *apps.ReplicaSet, deployment *apps.Deployment) (bool, error) {
	oldPodsCount := deploymentutil.GetReplicaCountForReplicaSets(oldRSs)
	if oldPodsCount == 0 {
		// Can't scale down further
		return false, nil
	}
```
-  遍历allRSs计算副本数量和allPodsCount
```go
	allPodsCount := deploymentutil.GetReplicaCountForReplicaSets(allRSs)
```
- 根据dep的副本数和unavailable数量或百分比计算 maxUnavailable数
- 比如 11个副本25%的不可用数量，那么向下取整就是 2个
```go
	maxUnavailable := deploymentutil.MaxUnavailable(*deployment)

```
- 计算最小可用数 比如 11个副本25%的不可用数量，那么最小可用数就是 11-2 =9个
```go
minAvailable := *(deployment.Spec.Replicas) - maxUnavailable
```

- 计算升级过程中最多可以宕掉几个
```go
	newRSUnavailablePodCount := *(newRS.Spec.Replicas) - newRS.Status.AvailableReplicas
	maxScaledDown := allPodsCount - minAvailable - newRSUnavailablePodCount
	if maxScaledDown <= 0 {
		return false, nil
	}
```

- 这里要清理一下不健康的rs，不然会一直阻塞dep更新到dep超时
```go
	oldRSs, cleanupCount, err := dc.cleanupUnhealthyReplicas(oldRSs, deployment, maxScaledDown)
	if err != nil {
		return false, nil
	}
```
- 这里就是缩掉旧版rs的逻辑，会返回缩容过程中down了几个scaledDownCount
```go
	allRSs = append(oldRSs, newRS)
	scaledDownCount, err := dc.scaleDownOldReplicaSetsForRollingUpdate(allRSs, oldRSs, deployment)
	if err != nil {
		return false, nil
	}
```
- 最后后计算总down数=清理不健康的个数+缩容过程中down了几个
```go

	totalScaledDown := cleanupCount + scaledDownCount
	return totalScaledDown > 0, nil
```
- 然后在外层会根据这个scaledDown返回值判断是否有必要更新dep
```go
	if scaledDown {
		// Update DeploymentStatus
		return dc.syncRolloutStatus(allRSs, newRS, d)
	}
```
##### scaleDownOldReplicaSetsForRollingUpdate缩旧解析
- 这里还是要判断下 当前存活的pod数是否低于最小存活数，double check
```go
func (dc *DeploymentController) scaleDownOldReplicaSetsForRollingUpdate(allRSs []*apps.ReplicaSet, oldRSs []*apps.ReplicaSet, deployment *apps.Deployment) (int32, error) {
	maxUnavailable := deploymentutil.MaxUnavailable(*deployment)

	// Check if we can scale down.
	minAvailable := *(deployment.Spec.Replicas) - maxUnavailable
	// Find the number of available pods.
	availablePodCount := deploymentutil.GetAvailableReplicaCountForReplicaSets(allRSs)
	if availablePodCount <= minAvailable {
		// Cannot scale down.
		return 0, nil
	}
	klog.V(4).Infof("Found %d available pods in deployment %s, scaling down old RSes", availablePodCount, deployment.Name)

```
- 然后将待 缩的RS按照创建时间排序，先新后旧
```go
	sort.Sort(controller.ReplicaSetsByCreationTimestamp(oldRSs))
```
- 然后就是遍历待 缩的RS，调用scaleReplicaSetAndRecordEvent缩容，同时还要判断不要缩过了
```go
	totalScaledDown := int32(0)
	totalScaleDownCount := availablePodCount - minAvailable
	for _, targetRS := range oldRSs {
		if totalScaledDown >= totalScaleDownCount {
			// No further scaling required.
			break
		}
		if *(targetRS.Spec.Replicas) == 0 {
			// cannot scale down this ReplicaSet.
			continue
		}
		// Scale down.
		scaleDownCount := int32(integer.IntMin(int(*(targetRS.Spec.Replicas)), int(totalScaleDownCount-totalScaledDown)))
		newReplicasCount := *(targetRS.Spec.Replicas) - scaleDownCount
		if newReplicasCount > *(targetRS.Spec.Replicas) {
			return 0, fmt.Errorf("when scaling down old RS, got invalid request to scale down %s/%s %d -> %d", targetRS.Namespace, targetRS.Name, *(targetRS.Spec.Replicas), newReplicasCount)
		}
		_, _, err := dc.scaleReplicaSetAndRecordEvent(targetRS, newReplicasCount, deployment)
		if err != nil {
			return totalScaledDown, err
		}

		totalScaledDown += scaleDownCount
	}

```

### 判断dep是否已完成更新 清理过期的 rs
- DeploymentComplete判断dep是否已完成更新，标志就是副本数量对，副本都可用，没有旧的pod在running了

```go
	if deploymentutil.DeploymentComplete(d, &d.Status) {
		if err := dc.cleanupDeployment(oldRSs, d); err != nil {
			return err
		}
	}
func DeploymentComplete(deployment *apps.Deployment, newStatus *apps.DeploymentStatus) bool {
	return newStatus.UpdatedReplicas == *(deployment.Spec.Replicas) &&
		newStatus.Replicas == *(deployment.Spec.Replicas) &&
		newStatus.AvailableReplicas == *(deployment.Spec.Replicas) &&
		newStatus.ObservedGeneration >= deployment.Generation
}
```
#### cleanupDeployment 清理操作
- deployment.Spec.RevisionHistoryLimit 的值清理 oldRSs
- 如果没配置就返回
```go
func (dc *DeploymentController) cleanupDeployment(oldRSs []*apps.ReplicaSet, deployment *apps.Deployment) error {
	if !deploymentutil.HasRevisionHistoryLimit(deployment) {
		return nil
	}

```
- 计算要清理的num diff
```go
	// Avoid deleting replica set with deletion timestamp set
	aliveFilter := func(rs *apps.ReplicaSet) bool {
		return rs != nil && rs.ObjectMeta.DeletionTimestamp == nil
	}
	cleanableRSes := controller.FilterReplicaSets(oldRSs, aliveFilter)

	diff := int32(len(cleanableRSes)) - *deployment.Spec.RevisionHistoryLimit
	if diff <= 0 {
		return nil
	}
```
- 按照版本号排序
```go
	sort.Sort(deploymentutil.ReplicaSetsByRevision(cleanableRSes))
	klog.V(4).Infof("Looking to cleanup old replica sets for deployment %q", deployment.Name)

```

- 遍历diff个然后清理，可以看到清理的动作就是调用apiserver 删除rs
```go

	for i := int32(0); i < diff; i++ {
		rs := cleanableRSes[i]
		// Avoid delete replica set with non-zero replica counts
		if rs.Status.Replicas != 0 || *(rs.Spec.Replicas) != 0 || rs.Generation > rs.Status.ObservedGeneration || rs.DeletionTimestamp != nil {
			continue
		}
		klog.V(4).Infof("Trying to cleanup replica set %q for deployment %q", rs.Name, deployment.Name)
		if err := dc.client.AppsV1().ReplicaSets(rs.Namespace).Delete(context.TODO(), rs.Name, metav1.DeleteOptions{}); err != nil && !errors.IsNotFound(err) {
			// Return error instead of aggregating and continuing DELETEs on the theory
			// that we may be overloading the api server.
			return err
		}
	}
```

### 还剩下syncRolloutStatus 
- 在rolloutRolling中可以看到多次调用 syncRolloutStatus 
- 计算newStatus 
```go
func (dc *DeploymentController) syncRolloutStatus(allRSs []*apps.ReplicaSet, newRS *apps.ReplicaSet, d *apps.Deployment) error {
	newStatus := calculateStatus(allRSs, newRS, d)

```
- 计算dep的condition，并将condition更新到newStatus中，以Complete状态为例
```go
		switch {
		case util.DeploymentComplete(d, &newStatus):
			// Update the deployment conditions with a message for the new replica set that
			// was successfully deployed. If the condition already exists, we ignore this update.
			msg := fmt.Sprintf("Deployment %q has successfully progressed.", d.Name)
			if newRS != nil {
				msg = fmt.Sprintf("ReplicaSet %q has successfully progressed.", newRS.Name)
			}
			condition := util.NewDeploymentCondition(apps.DeploymentProgressing, v1.ConditionTrue, util.NewRSAvailableReason, msg)
			util.SetDeploymentCondition(&newStatus, *condition)

```
- 最后更新dep的状态到apiserver中
```go
	newDeployment := d
	newDeployment.Status = newStatus
	_, err := dc.client.AppsV1().Deployments(newDeployment.Namespace).UpdateStatus(context.TODO(), newDeployment, metav1.UpdateOptions{})
	return err
```

## 滚动更新的实践
- 下面我们结合滚动更新的实际操作来回顾一下上面的代码流程
> 首先我们准备一个10副本的nginx 1.8的dep
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment-roll01
  labels:
    app: nginx-001
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx-001
  template:
    metadata:
      labels:
        app: nginx-001
    spec:
      containers:
      - name: nginx
        image: nginx:1.8
        ports:
        - containerPort: 80
``` 
- 应用它，然后等待10个副本都ready
```shell script

[root@k8s-master01 dep]# kubectl get deployment nginx-deployment-roll01 
NAME                      READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment-roll01   3/3     3            3           20s
```
> 然后更新 nginx-deployment 的镜像，默认使用滚动更新的方式
```shell script
kubectl set image deploy/nginx-deployment-roll01 nginx=nginx:1.9.3
```
- 此时通过源码可知会计算该 deployment 的 maxSurge、maxUnavailable 和 maxAvailable 的值，分别为 3、2 和 13，计算方法如下所示：
```shell script
# 向上取整为 3
maxSurge = replicas * deployment.spec.strategy.rollingUpdate.maxSurge(25%)= 2.5

# 向下取整为 2
maxUnavailable = replicas * deployment.spec.strategy.rollingUpdate.maxUnavailable(25%)= 2.5

maxAvailable = replicas(10) + MaxSurge（3） = 13
```
- 如上面代码所说，更新时首先创建 newRS，然后为其设定 replicas，计算 newRS replicas 值的方法在NewRSNewReplicas 中
    - 此时计算出 replicas 结果为 3，然后更新 deployment 的 annotation创建 events本次 syncLoop 完成
    - 等到下一个 syncLoop 时，所有 rs 的 replicas 已经达到最大值 10 + 3 = 13，此时需要 scale down oldRSs 了
- scale down 的数量是通过以下公式得到的：
```shell script
# 13 = 10 + 3
allPodsCount := deploymentutil.GetReplicaCountForReplicaSets(allRSs)

# 8 = 10 - 2
minAvailable := *(deployment.Spec.Replicas) - maxUnavailable

# ???
newRSUnavailablePodCount := *(newRS.Spec.Replicas) - newRS.Status.AvailableReplicas

# 13 - 8 - ???
maxScaledDown := allPodsCount - minAvailable - newRSUnavailablePodCount
```
- allPodsCount 是 allRSs 的 replicas 之和此时为 13，minAvailable 为 8 
- newRSUnavailablePodCount 此时不确定，但是值在 [0,3] 中
- 此时假设 newRS 的三个 pod 还处于 containerCreating 状态,则newRSUnavailablePodCount 为 3
- 根据以上公式计算所知 maxScaledDown 为 2，则 oldRS 需要 scale down 2 个 pod，其 replicas 需要改为 8，此时该 syncLoop 完成。
- 下一个 syncLoop 时在 scaleUp 处计算得知 scaleUpCount = maxTotalPods - currentPodCount，13-3-8=2， ， 此时 newRS 需要更新 replicase 增加 2。
- 以此轮询直到 newRS replicas 扩容到 10，oldRSs replicas 缩容至 0。
- 所以这就是滚动更新过程：
    - 先启动少量新版本
    - 然后缩掉少量旧版本
    - 再启动少量新版本
    - 然后再缩掉少量旧版本
    - 直到直到 newRS replicas 扩容到 10，oldRSs replicas 缩容至 0。
    - 同时在过程中要保证总数的上限maxSurge和不可用的下限maxUnavailable
    
> 我们使用 watch观察 dep驱动rs 滚动更新的过程


- 58477c5d5d代表新版本 ，64f9d8d55d代表旧版本


```shell script
[root@k8s-master01 dep]# kubectl get rs -l app=nginx-001 -w
NAME                                 DESIRED   CURRENT   READY   AGE
nginx-deployment-roll01-64f9d8d55d   10        10        10      74s
nginx-deployment-roll01-58477c5d5d   3         0         0       0s
nginx-deployment-roll01-64f9d8d55d   8         10        10      79s
nginx-deployment-roll01-58477c5d5d   3         0         0       0s
nginx-deployment-roll01-58477c5d5d   3         3         0       0s
nginx-deployment-roll01-64f9d8d55d   8         10        10      79s
nginx-deployment-roll01-64f9d8d55d   8         8         8       79s
nginx-deployment-roll01-58477c5d5d   5         3         0       0s
nginx-deployment-roll01-58477c5d5d   5         3         0       0s
nginx-deployment-roll01-58477c5d5d   5         5         0       0s
nginx-deployment-roll01-58477c5d5d   5         5         1       4s
nginx-deployment-roll01-64f9d8d55d   7         8         8       83s
nginx-deployment-roll01-58477c5d5d   6         5         1       4s
nginx-deployment-roll01-64f9d8d55d   7         8         8       83s
nginx-deployment-roll01-64f9d8d55d   7         7         7       83s
nginx-deployment-roll01-58477c5d5d   6         5         1       4s
nginx-deployment-roll01-58477c5d5d   6         6         1       4s
nginx-deployment-roll01-58477c5d5d   6         6         2       5s
nginx-deployment-roll01-64f9d8d55d   6         7         7       84s
nginx-deployment-roll01-64f9d8d55d   6         7         7       84s
nginx-deployment-roll01-58477c5d5d   7         6         2       5s
nginx-deployment-roll01-64f9d8d55d   6         6         6       84s
nginx-deployment-roll01-58477c5d5d   7         6         2       5s
nginx-deployment-roll01-58477c5d5d   7         7         2       5s
nginx-deployment-roll01-58477c5d5d   7         7         3       5s
nginx-deployment-roll01-64f9d8d55d   5         6         6       84s
nginx-deployment-roll01-58477c5d5d   8         7         3       5s
nginx-deployment-roll01-64f9d8d55d   5         6         6       84s
nginx-deployment-roll01-64f9d8d55d   5         5         5       84s
nginx-deployment-roll01-58477c5d5d   8         7         3       5s
nginx-deployment-roll01-58477c5d5d   8         8         3       5s
nginx-deployment-roll01-58477c5d5d   8         8         4       6s
nginx-deployment-roll01-64f9d8d55d   4         5         5       85s
nginx-deployment-roll01-58477c5d5d   9         8         4       6s
nginx-deployment-roll01-64f9d8d55d   4         5         5       85s
nginx-deployment-roll01-64f9d8d55d   4         4         4       85s
nginx-deployment-roll01-58477c5d5d   9         8         4       6s
nginx-deployment-roll01-58477c5d5d   9         9         4       6s
nginx-deployment-roll01-58477c5d5d   9         9         5       6s
nginx-deployment-roll01-64f9d8d55d   3         4         4       85s
nginx-deployment-roll01-58477c5d5d   10        9         5       6s
nginx-deployment-roll01-64f9d8d55d   3         4         4       85s
nginx-deployment-roll01-64f9d8d55d   3         3         3       86s
nginx-deployment-roll01-58477c5d5d   10        9         5       7s
nginx-deployment-roll01-58477c5d5d   10        10        5       7s
nginx-deployment-roll01-58477c5d5d   10        10        6       12s
nginx-deployment-roll01-64f9d8d55d   2         3         3       91s
nginx-deployment-roll01-64f9d8d55d   2         3         3       91s
nginx-deployment-roll01-64f9d8d55d   2         2         2       91s
nginx-deployment-roll01-58477c5d5d   10        10        7       13s
nginx-deployment-roll01-64f9d8d55d   1         2         2       92s
nginx-deployment-roll01-64f9d8d55d   1         2         2       92s
nginx-deployment-roll01-64f9d8d55d   1         1         1       92s
nginx-deployment-roll01-58477c5d5d   10        10        8       13s
nginx-deployment-roll01-64f9d8d55d   0         1         1       92s
nginx-deployment-roll01-64f9d8d55d   0         1         1       92s
nginx-deployment-roll01-64f9d8d55d   0         0         0       92s
nginx-deployment-roll01-58477c5d5d   10        10        9       14s
nginx-deployment-roll01-58477c5d5d   10        10        10      14s

```

