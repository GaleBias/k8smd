# 本节重点总结

> StatefulSets简介

- StatefulSet 是用来管理有状态应用的工作负载 API 对象。

> 什么的场景使用  StatefulSets

- StatefulSets 对于需要满足以下一个或多个需求的应用程序很有价值：
- 稳定的、唯一的网络标识符。
- 稳定的、持久的存储。
- 有序的、优雅的部署和缩放。
- 有序的、自动的滚动更新。

> 动态pv和静态pv

- 静态PV 就是我要使用的一个 PVC 的话就必须手动去创建一个 PV，在pv需求比较大时效率比较低
- 同时使用方对于存储的要求不能直观的从pv上获取
- 但是通过 PVC 请求到一定的存储空间也很有可能不足以满足应用对于存储设备的各种需求，而且不同的应用程序对于存储性能的要求可能也不尽相同，比如读写速度、并发性能等
  - 为了解决这一问题，Kubernetes 又为我们引入了一个新的资源对：StorageClass
  - 通过 StorageClass 的定义，管理员可以将存储资源定义为某种类型的资源，比如快速存储、慢速存储等
  - 用户根据 StorageClass 的描述就可以非常直观的知道各种存储资源的具体特性了，这样就可以根据应用的特性去申请合适的存储资源了。

> 使用nfs-Provisioner 动态提供pv

- 安装nfs服务端和客户端
- 安装nfs-client 自动配置程序

# StatefulSets简介

- StatefulSet 是用来管理有状态应用的工作负载 API 对象。
- statefulset 旨在与有状态的应用及分布式系统一起使用，statefulset 中的每个 pod 拥有一个唯一的身份标识，并且所有 pod 名都是按照 {0..N-1} 的顺序进行编号

## 什么的场景使用  StatefulSets

> StatefulSets 对于需要满足以下一个或多个需求的应用程序很有价值：

- 稳定的、唯一的网络标识符。
- 稳定的、持久的存储。
- 有序的、优雅的部署和缩放。
- 有序的、自动的滚动更新。

## StatefulSets的限制

- 给定 Pod 的存储必须由 PersistentVolume 驱动 基于所请求的 storage class 来提供，或者由管理员预先提供。
- 删除或者收缩 StatefulSet 并不会删除它关联的存储卷。 这样做是为了保证数据安全，它通常比自动清除 StatefulSet 所有相关的资源更有价值。
- StatefulSet 当前需要无头服务 来负责 Pod 的网络标识。你需要负责创建此服务。
- 当删除 StatefulSets 时，StatefulSet 不提供任何终止 Pod 的保证。 为了实现 StatefulSet 中的 Pod 可以有序地且体面地终止，可以在删除之前将 StatefulSet 缩放为 0。
- 在默认 Pod 管理策略(OrderedReady) 时使用 滚动更新，可能进入需要人工干预 才能修复的损坏状态。

# StatefulSet功能演示

## 新建 StatefulSet

- 下面的yaml文件来自k8s官网，可以看到创建了一个无头service和StatefulSet
- 其中StatefulSet是创建2个副本的pod运行nginx-slim容器
- 同时使用了volumeClaimTemplates pvc模板，申请pv

```yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  ports:
  - port: 80
    name: web
  clusterIP: None
  selector:
    app: nginx
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  serviceName: "nginx"
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.8
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates:
  - metadata:
      name: www
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 1Gi
```

- 我们创建这个sts，发现sts没有正常运行，所属的pod处于Pending状态，

```shell
[root@k8s-master01 nfs-subdir-external-provisioner-master]# kubectl get sts
NAME   READY   AGE
web    0/2     77m
[root@k8s-master01 nfs-subdir-external-provisioner-master]# kubectl get pod
NAME                                      READY   STATUS      RESTARTS   AGE
web-0                                     0/1     Pending     0          77m

[root@k8s-master01 nfs-subdir-external-provisioner-master]# kubectl describe pod web-0 
Name:           web-0
Namespace:      default
Priority:       0
Node:           <none>
Labels:         app=nginx
                controller-revision-hash=web-b46f789c4
                statefulset.kubernetes.io/pod-name=web-0
Annotations:    <none>
Status:         Pending
IP:         
IPs:            <none>
Controlled By:  StatefulSet/web
Containers:
  nginx:
    Image:        k8s.gcr.io/nginx-slim:0.8
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /usr/share/nginx/html from www (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-v8sdt (ro)
Conditions:
  Type           Status
  PodScheduled   False 
Volumes:
  www:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  www-web-0
    ReadOnly:   false
  kube-api-access-v8sdt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  78m   default-scheduler  0/2 nodes are available: 2 pod has unbound immediate PersistentVolumeClaims.
  Warning  FailedScheduling  58m   default-scheduler  0/2 nodes are available: 2 pod has unbound immediate PersistentVolumeClaims.
  Warning  FailedScheduling  33m   default-scheduler  0/2 nodes are available: 2 persistentvolumeclaim "www-web-0" not found.
  Warning  FailedScheduling  32m   default-scheduler  0/2 nodes are available: 2 persistentvolumeclaim "www-web-0" not found.
```

- 然后describe pending的pod发现处于 等待pvc的状态
- 原因是使用了动态pv的volumeClaimTemplates 需要storageClass支持

## 动态pv和静态pv

- 静态PV 就是我要使用的一个 PVC 的话就必须手动去创建一个 PV，在pv需求比较大时效率比较低
- 同时使用方对于存储的要求不能直观的从pv上获取
- 但是通过 PVC 请求到一定的存储空间也很有可能不足以满足应用对于存储设备的各种需求，而且不同的应用程序对于存储性能的要求可能也不尽相同，比如读写速度、并发性能等
  - 为了解决这一问题，Kubernetes 又为我们引入了一个新的资源对：StorageClass
  - 通过 StorageClass 的定义，管理员可以将存储资源定义为某种类型的资源，比如快速存储、慢速存储等
  - 用户根据 StorageClass 的描述就可以非常直观的知道各种存储资源的具体特性了，这样就可以根据应用的特性去申请合适的存储资源了。

## 使用nfs-Provisioner 动态提供pv

### 步骤01 服务端安装nfs

- 我们这里选择k8s的master 节点为nfs的服务端
- 因为centos7自带了rpcbind，所以不用安装rpc服务，rpc监听在111端口，可以使用ss -tnulp | grep 111查看rpc服务是否自动启动，如果没有启动，就systemctl start rpcbind 启动rpc服务
- rpc在nfs服务器搭建过程中至关重要，因为rpc能够获得nfs服务器端的端口号等信息，nfs服务器端通过rpc获得这些信息后才能连接nfs服务器端。
- 安装包为 yum -y install nfs-utils

```shell
[root@k8s-master01 statefulset]# ss -tnlp |grep rpc   
LISTEN     0      128          *:111                      *:*                   users:(("rpcbind",pid=3497,fd=4),("systemd",pid=1,fd=69))
LISTEN     0      128         :::111                     :::*                   users:(("rpcbind",pid=3497,fd=6),("systemd",pid=1,fd=71))
yum -y install nfs-utils
```

> 编辑/etc/exports ，添加以下内容

```shell
/data/nfs/k8s  *(rw,sync,no_root_squash)
```

- /data/nfs/k8s：是共享的数据目录
- *：表示任何人都有权限连接，当然也可以是一个网段，一个 IP，也可以是域名
- rw：读写的权限
- sync：表示文件同时写入硬盘和内存
- no_root_squash：当登录 NFS 主机使用共享目录的使用者是 root 时，其权限将被转换成为匿名使用者，通常它的 UID 与 GID，都会变成 nobody 身份

> 启动nfs服务

- 启动服务

```shell
systemctl start nfs ;systemctl enable nfs 
```

> 使用rpcinfo 检查 nfs的端口

```shell
root@k8s-master01 statefulset]# rpcinfo -p localhost
   program vers proto   port  service
    100000    4   tcp    111  portmapper
    100000    3   tcp    111  portmapper
    100000    2   tcp    111  portmapper
    100000    4   udp    111  portmapper
    100000    3   udp    111  portmapper
    100000    2   udp    111  portmapper
    100024    1   udp  35823  status
    100005    1   udp  20048  mountd
    100005    1   tcp  20048  mountd
    100024    1   tcp  49000  status
    100005    2   udp  20048  mountd
    100005    2   tcp  20048  mountd
    100005    3   udp  20048  mountd
    100005    3   tcp  20048  mountd
    100003    3   tcp   2049  nfs
    100003    4   tcp   2049  nfs
    100227    3   tcp   2049  nfs_acl
    100003    3   udp   2049  nfs
    100003    4   udp   2049  nfs
    100227    3   udp   2049  nfs_acl
    100021    1   udp  34801  nlockmgr
    100021    3   udp  34801  nlockmgr
    100021    4   udp  34801  nlockmgr
    100021    1   tcp  36864  nlockmgr
    100021    3   tcp  36864  nlockmgr
    100021    4   tcp  36864  nlockmgr
```

> 检查nfs配置

```shell
[root@k8s-master01 statefulset]# showmount -e localhost
Export list for localhost:
/data/nfs/k8s 
```

> 创建目录添加测试文件,更改权限

```shell
mkdir -pv /data/nfs/k8s
echo "nfs test" >>/data/nfs/k8s/nfs_test.txt

chown -R nfsnobody.nfsnobody /data/nfs/k8s
```

### 步骤02 客户端安装nfs

- 安装

```shell
 yum -y install nfs-utils 
```

- 使用showmount -e masterip 查看

```shell
[root@k8s-node01 ~]# showmount -e 172.20.70.205
Export list for 172.20.70.205:
/data/nfs/k8s *
```

- 挂载至本地/mnt目录，查看测试文件的内容，如果能看到内容说明正常

```shell
mount -t nfs 172.20.70.205:/data/nfs/k8s /mnt
cat /mnt/nfs_test.txt
nfs test
```

### 步骤3 安装nfs-client 自动配置程序

- 要使用 StorageClass，我们就得安装对应的自动配置程序，比如我们这里存储后端使用的是 nfs，那么我们就需要使用到一个 nfs-client 的自动配置程序，我们也叫它 Provisioner
- 这个程序使用我们已经配置好的 nfs 服务器，来自动创建持久卷，也就是自动帮我们创建 PV。
  - 自动创建的 PV 以${namespace}-${pvcName}-${pvName}这样的命名格式创建在 NFS 服务器上的共享数据目录中
  - 而当这个 PV 被回收后会以archieved-${namespace}-${pvcName}-${pvName}这样的命名格式存在 NFS 服务器上。

> 项目地址

- [nfs-subdir-external-provisioner](https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner)

#### 安装过程

- 首先将代码 clone下来

```shell
git clone  https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner.git
```

- 然后部署rbac鉴权相关

```shell
NS=$(kubectl config get-contexts|grep -e "^\*" |awk '{print $5}')
NAMESPACE=${NS:-default}
sed -i'' "s/namespace:.*/namespace: $NAMESPACE/g" ./deploy/rbac.yaml ./deploy/deployment.yaml
kubectl create -f deploy/rbac.yaml
```

- 然后就是部署nfs-subdir-external-provisioner应用，这里会遇到镜像拉取失败的问题，由于是k8s.gcr.io所以被墙了

```shell
image: k8s.gcr.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2
```

##### 利用阿里云构建海外项目的镜像

- 首先fork nfs-subdir-external-provisioner 项目到自己的github上，

```shell
https://github.com/ning1875/nfs-subdir-external-provisioner
```

> 这里以之前的kube-prometheus 为例

- 在阿里的个人账号上做容器镜像服务  地址https://cr.console.aliyun.com/cn-beijing/instance/repositories
- 在你自己的GitHub上fork你想要拉去镜像的仓库 ，比如ksm
- ![image.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/908/1631407698000/78b5ed813d9343b0b128d77960ee3e25.png)
- 到阿里云的容器镜像创建仓库，选公开
- ![image.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/908/1631407698000/c68653b1bdf449eb8c3e77f13598bd97.png)
- 绑定GitHub仓库
- ![image.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/908/1631407698000/1d63a835fcdb430196bccae14a266b92.png)
- 添加构建规则
- ![image.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/908/1631407698000/e8ba1f23f23c432ca104594ec7375dbc.png)
- 根据tag添加规则
- ![image.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/908/1631407698000/e73a437eb14045ebb2ef4c4a94751a97.png)
- 点击立即构建
- ![image.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/908/1631407698000/d84d3ddcd6a941568392a6b289dfa66a.png)
- 等待构建结果
- ![image.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/908/1631407698000/8624c81385e9417a8777e473082a3f80.png)
- 可以看到还是构建失败了，原因是DockerFile中是直接拷贝编译的二进制的，而编译的过程由make命令触发

```shell
FROM gcr.io/distroless/static:latest
LABEL maintainers="Kubernetes Authors"
LABEL description="NFS subdir external provisioner"
ARG binary=./bin/nfs-subdir-external-provisioner

COPY ${binary} /nfs-subdir-external-provisioner
ENTRYPOINT ["/nfs-subdir-external-provisioner"]
```

- 所以我们得手动将 go build的命令添加到 DockerFile中 ，比如我个提交 ：https://github.com/ning1875/nfs-subdir-external-provisioner/commit/7c0ebc8b618c47ca37fe6499e26ab8114f620445
- 然后编辑Deployment文件，将镜像替换为我们自己的

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nfs-client-provisioner
  labels:
    app: nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: default
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: nfs-client-provisioner
  template:
    metadata:
      labels:
        app: nfs-client-provisioner
    spec:
      serviceAccountName: nfs-client-provisioner
      containers:
        - name: nfs-client-provisioner
          #image: k8s.gcr.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2
          #image: quay.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2
          #image: quay.io/external_storage/nfs-client-provisioner:latest
          image: registry.cn-beijing.aliyuncs.com/ning1875_haiwai_image/nfs-subdir-external-provisioner:v4.0.2
          volumeMounts:
            - name: nfs-client-root
              mountPath: /persistentvolumes
          env:
            - name: PROVISIONER_NAME
              value: k8s-sigs.io/nfs-subdir-external-provisioner
            - name: NFS_SERVER
              value: 172.20.70.205
            - name: NFS_PATH
              value: /data/nfs/k8s
      volumes:
        - name: nfs-client-root
          nfs:
            server: 172.20.70.205
            path: /data/nfs/k8s
```

##### 替换nfs-client-provisioner中的nfs-server信息

- 填写nfs服务端地址和 nfs的挂载路径

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nfs-client-provisioner
  labels:
    app: nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: default
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: nfs-client-provisioner
  template:
    metadata:
      labels:
        app: nfs-client-provisioner
    spec:
      serviceAccountName: nfs-client-provisioner
      containers:
        - name: nfs-client-provisioner
          #image: k8s.gcr.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2
          #image: quay.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2
          #image: quay.io/external_storage/nfs-client-provisioner:latest
          image: registry.cn-beijing.aliyuncs.com/ning1875_haiwai_image/nfs-subdir-external-provisioner:v4.0.2
          volumeMounts:
            - name: nfs-client-root
              mountPath: /persistentvolumes
          env:
            - name: PROVISIONER_NAME
              value: k8s-sigs.io/nfs-subdir-external-provisioner
            - name: NFS_SERVER
              value: 172.20.70.205
            - name: NFS_PATH
              value: /data/nfs/k8s
      volumes:
        - name: nfs-client-root
          nfs:
            server: 172.20.70.205
            path: /data/nfs/k8s
```

- 然后apply 创建这个dep

```shell
[root@k8s-master01 deploy]# kubectl get pod
NAME                                      READY   STATUS      RESTARTS   AGE
nfs-client-provisioner-78577dfbf6-wht7j   1/1     Running     0          58m
```

- 然后查看nfs-client-provisioner的日志

```shell
[root@k8s-master01 nfs-subdir-external-provisioner-master]# kubectl logs nfs-client-provisioner-78577dfbf6-wht7j  -f 
I1018 20:13:04.006654       1 leaderelection.go:242] attempting to acquire leader lease  default/k8s-sigs.io-nfs-subdir-external-provisioner...
I1018 20:13:21.619515       1 leaderelection.go:252] successfully acquired lease default/k8s-sigs.io-nfs-subdir-external-provisioner
I1018 20:13:21.619760       1 controller.go:820] Starting provisioner controller k8s-sigs.io/nfs-subdir-external-provisioner_nfs-client-provisioner-78577dfbf6-wht7j_d90ece08-5c20-4514-b31b-f7d4711dea54!
I1018 20:13:21.619941       1 event.go:278] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"default", Name:"k8s-sigs.io-nfs-subdir-external-provisioner", UID:"f84f9471-32d5-42e3-9ee1-75b43815d0f6", APIVersion:"v1", ResourceVersion:"1226826", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' nfs-client-provisioner-78577dfbf6-wht7j_d90ece08-5c20-4514-b31b-f7d4711dea54 became leader
I1018 20:13:21.720089       1 controller.go:869] Started provisioner controller k8s-sigs.io/nfs-subdir-external-provisioner_nfs-client-provisioner-78577dfbf6-wht7j_d90ece08-5c20-4514-b31b-f7d4711dea54!


```

##### 部署  storage class

- 就是apply deploy/class.yaml

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: managed-nfs-storage
provisioner: k8s-sigs.io/nfs-subdir-external-provisioner # or choose another name, must match deployment's env PROVISIONER_NAME'
parameters:
  pathPattern: "${.PVC.namespace}/${.PVC.annotations.nfs.io/storage-path}" # waits for nfs.io/storage-path annotation, if not specified will accept as empty string.
  onDelete: delete
```

- 检查sc

```shell
[root@k8s-master01 deploy]# kubectl get sc 
NAME                   PROVISIONER                                   RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
managed-nfs-storage    k8s-sigs.io/nfs-subdir-external-provisioner   Delete          Immediate              false                  10s
```

##### 部署测试 pod

```shell
kubectl create -f deploy/test-claim.yaml -f deploy/test-pod.yaml
```

- 分析一下test-claim.yaml ，代表使用上面创建的managed-nfs-storage 的storageClass 创建一个静态的pvc

```yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: test-claim
spec:
  storageClassName: managed-nfs-storage
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 1Mi

```

- 分析一下test-pod.yaml ，里面通过上面的pvc使用 volume

```yaml
kind: Pod
apiVersion: v1
metadata:
  name: test-pod
spec:
  containers:
  - name: test-pod
    image: busybox:stable
    command:
      - "/bin/sh"
    args:
      - "-c"
      - "touch /mnt/SUCCESS && exit 0 || exit 1"
    volumeMounts:
      - name: nfs-pvc
        mountPath: "/mnt"
  restartPolicy: "Never"
  volumes:
    - name: nfs-pvc
      persistentVolumeClaim:
        claimName: test-claim

```

- 然后检查pv 和pvc ，可以看到 创建了一个名为test-claim的pvc
- 同时创建了一个pvc-299377f9-360d-4a5d-85e5-3946a686a8d8的pv

```shell
[root@k8s-master01 nfs-subdir-external-provisioner-master]# kubectl get pvc
NAME          STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS           AGE
grafana-pvc   Bound    grafana-pv                                 10Gi       RWO            grafana-storageclass   6d10h
test-claim    Bound    pvc-299377f9-360d-4a5d-85e5-3946a686a8d8   1Mi        RWX            managed-nfs-storage    2m46s
[root@k8s-master01 nfs-subdir-external-provisioner-master]# kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS        CLAIM                                      STORAGECLASS           REASON   AGE
grafana-pv                                 10Gi       RWO            Retain           Bound         default/grafana-pvc                        grafana-storageclass            6d10h
prometheus-lpv-0                           10Gi       RWO            Retain           Terminating   kube-system/prometheus-data-prometheus-0   prometheus-lpv                  7d1h
pvc-299377f9-360d-4a5d-85e5-3946a686a8d8   1Mi        RWX            Delete           Bound         default/test-claim                         managed-nfs-storage             2m47s
[root@k8s-master01 nfs-subdir-external-provisioner-master]# 
```

- 这时候我们去nfs存储上查看发现多了default/SUCCESS文件，这是test-pod创建的

```shell
[root@k8s-master01 nfs-subdir-external-provisioner-master]# ll /data/nfs/k8s/default/SUCCESS
-rw-r--r-- 1 root root 0 Oct 18 21:22 /data/nfs/k8s/default/SUCCESS
```

- 同时观察nfs-client-provisioner的日志，可以看到相关的pv和pvc创建过程

```shell
[root@k8s-master01 deploy]# kubectl logs nfs-client-provisioner-78577dfbf6-5vc8k  -f 
I1018 21:22:30.465733       1 controller.go:1317] provision "default/test-claim" class "managed-nfs-storage": started
I1018 21:22:30.475689       1 event.go:278] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"test-claim", UID:"299377f9-360d-4a5d-85e5-3946a686a8d8", APIVersion:"v1", ResourceVersion:"1236396", FieldPath:""}): type: 'Normal' reason: 'Provisioning' External provisioner is provisioning volume for claim "default/test-claim"
I1018 21:22:30.478079       1 controller.go:1420] provision "default/test-claim" class "managed-nfs-storage": volume "pvc-299377f9-360d-4a5d-85e5-3946a686a8d8" provisioned
I1018 21:22:30.478117       1 controller.go:1437] provision "default/test-claim" class "managed-nfs-storage": succeeded
I1018 21:22:30.478137       1 volume_store.go:212] Trying to save persistentvolume "pvc-299377f9-360d-4a5d-85e5-3946a686a8d8"
I1018 21:22:30.491054       1 volume_store.go:219] persistentvolume "pvc-299377f9-360d-4a5d-85e5-3946a686a8d8" saved
I1018 21:22:30.491376       1 event.go:278] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"test-claim", UID:"299377f9-360d-4a5d-85e5-3946a686a8d8", APIVersion:"v1", ResourceVersion:"1236396", FieldPath:""}): type: 'Normal' reason: 'ProvisioningSucceeded' Successfully provisioned volume pvc-299377f9-360d-4a5d-85e5-3946a686a8d8
```

- 这时候删掉 test-pod和test-claim

```shell
[root@k8s-master01 nfs-subdir-external-provisioner-master]# kubectl delete -f deploy/test-pod.yaml -f deploy/test-claim.yaml
pod "test-pod" deleted
persistentvolumeclaim "test-claim" deleted
```

- 发现nfs存储上的 default/SUCCESS文件已经被删除

```shell
[root@k8s-master01 nfs-subdir-external-provisioner-master]# ll /data/nfs/k8s/default/SUCCESS
ls: cannot access /data/nfs/k8s/default/SUCCESS: No such file or directory
[root@k8s-master01 nfs-subdir-external-provisioner-master]# ll /data/nfs/k8s/
total 4
-rw-r--r-- 1 nfsnobody nfsnobody 9 Oct 18 18:07 nfs_test.txt
```

- 同时可以发现日志打印了删除的过程

```shell
I1018 21:29:37.944297       1 controller.go:1450] delete "pvc-299377f9-360d-4a5d-85e5-3946a686a8d8": started
I1018 21:29:37.953587       1 controller.go:1478] delete "pvc-299377f9-360d-4a5d-85e5-3946a686a8d8": volume deleted
I1018 21:29:37.958011       1 controller.go:1524] delete "pvc-299377f9-360d-4a5d-85e5-3946a686a8d8": persistentvolume deleted
I1018 21:29:37.958033       1 controller.go:1526] delete "pvc-299377f9-360d-4a5d-85e5-3946a686a8d8": succeeded

```

- 到这里说明我们基于nfs的动态pv已经搭建完毕

# 本节重点总结

> StatefulSets简介

- StatefulSet 是用来管理有状态应用的工作负载 API 对象。

> 什么的场景使用  StatefulSets

- StatefulSets 对于需要满足以下一个或多个需求的应用程序很有价值：
- 稳定的、唯一的网络标识符。
- 稳定的、持久的存储。
- 有序的、优雅的部署和缩放。
- 有序的、自动的滚动更新。

> 动态pv和静态pv

- 静态PV 就是我要使用的一个 PVC 的话就必须手动去创建一个 PV，在pv需求比较大时效率比较低
- 同时使用方对于存储的要求不能直观的从pv上获取
- 但是通过 PVC 请求到一定的存储空间也很有可能不足以满足应用对于存储设备的各种需求，而且不同的应用程序对于存储性能的要求可能也不尽相同，比如读写速度、并发性能等
  - 为了解决这一问题，Kubernetes 又为我们引入了一个新的资源对：StorageClass
  - 通过 StorageClass 的定义，管理员可以将存储资源定义为某种类型的资源，比如快速存储、慢速存储等
  - 用户根据 StorageClass 的描述就可以非常直观的知道各种存储资源的具体特性了，这样就可以根据应用的特性去申请合适的存储资源了。

> 使用nfs-Provisioner 动态提供pv

- 安装nfs服务端和客户端
- 安装nfs-client 自动配置程序