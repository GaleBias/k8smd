
#  本节重点总结 : 
- statusManager作用
    - statusManager 的主要功能是将 pod 状态信息同步到 apiserver
    - statusManager 并不会主动监控 pod 的状态，而是提供接口供其他 manager 进行调用
    - 等于是把podManger中负责同步podstatus的部分抽取出来了，看数据结构有podManager
- kubelet的syncPod中通过SetPodStatus调用 statusManager更新pod状态
    - 内部通过队列传递给处理程序，程序判断是否要更新
    - 底层调用client 向apiserver发送patch更新

# statusManager作用

- statusManager 的主要功能是将 pod 状态信息同步到 apiserver
- statusManager 并不会主动监控 pod 的状态，而是提供接口供其他 manager 进行调用
- 等于是把podManger中负责同步podstatus的部分抽取出来了，看数据结构有podManager

# statusManager 接口
- 位置 D:\go_path\src\github.com\kubernetes\kubernetes\pkg\kubelet\status\status_manager.go
```go
// Manager is the Source of truth for kubelet pod status, and should be kept up-to-date with
// the latest v1.PodStatus. It also syncs updates back to the API server.
type Manager interface {
	PodStatusProvider

	// Start the API server status sync loop.
	Start()

	// SetPodStatus  设置pod.status，同时触发一次sync动作
	SetPodStatus(pod *v1.Pod, status v1.PodStatus)

	// SetContainerReadiness 检查pod的containerStatus是否ready，不是触发sync
	SetContainerReadiness(podUID types.UID, containerID kubecontainer.ContainerID, ready bool)

	// SetContainerStartup  设置pod的containerStatus是否完成startUp，不是触发sync
	SetContainerStartup(podUID types.UID, containerID kubecontainer.ContainerID, started bool)

	// TerminatePod 设置pod的container和init containers的状态为terminated
	TerminatePod(pod *v1.Pod)

	// RemoveOrphanedStatuses删除statusManager中缓存的无用数据
	RemoveOrphanedStatuses(podUIDs map[types.UID]bool)
}
```
  

# statusManager结构体定义
```go
type manager struct {
    // 连接kube-apiserver的client
    kubeClient clientset.Interface
    // kubelet的组件，维护内存中pod数据
    podManager kubepod.Manager
    // 相当于tatus manager的缓存，保存的是podUID和状态的对应关系
    podStatuses      map[types.UID]versionedPodStatus
    // 互斥锁，更新podStatus时用
    podStatusesLock  sync.RWMutex
    // 接收更新podStatus的管道，收到消息就触发一次sync动作
    podStatusChannel chan podStatusSyncRequest
    // 维护mirror pod的status的版本号，更新一次加1，互斥访问
    apiStatusVersions map[kubetypes.MirrorPodUID]uint64
    // 删除pod的接口
    podDeletionSafety PodDeletionSafetyProvider
}
```

# pod状态实例字段
```shell script
[root@k8s-node01 volumes]# kubectl describe pod nginx-pod-test 
Name:         nginx-pod-test
Namespace:    default
Priority:     0
Node:         k8s-node01/172.20.70.215
Start Time:   Wed, 15 Sep 2021 19:13:20 +0800
Labels:       <none>
Annotations:  cni.projectcalico.org/podIP: 10.100.85.220/32
              cni.projectcalico.org/podIPs: 10.100.85.220/32
Status:       Pending
IP:           10.100.85.220
IPs:
  IP:  10.100.85.220
Containers:
  nginx:
    Container ID:   
    Image:          nginxa:1.8
    Image ID:       
    Port:           <none>
    Host Port:      <none>
    State:          Waiting
      Reason:       ImagePullBackOff
    Ready:          False
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-k46nh (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  default-token-k46nh:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-k46nh
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason   Age                    From     Message
  ----     ------   ----                   ----     -------
  Normal   Pulling  45m (x2487 over 8d)    kubelet  Pulling image "nginxa:1.8"
  Warning  Failed   6m4s (x56213 over 8d)  kubelet  Error: ImagePullBackOff
  Normal   BackOff  62s (x56235 over 8d)   kubelet  Back-off pulling image "nginxa:1.8"
```


# 初始化
- 入口在 kubelet的New中
```go
	klet.statusManager = status.NewManager(klet.kubeClient, klet.podManager, klet)

```
- NewManager
```go
func NewManager(kubeClient clientset.Interface, podManager kubepod.Manager, podDeletionSafety PodDeletionSafetyProvider) Manager {
	return &manager{
		kubeClient:        kubeClient,
		podManager:        podManager,
		podStatuses:       make(map[types.UID]versionedPodStatus),
		podStatusChannel:  make(chan podStatusSyncRequest, 1000), // Buffer up to 1000 statuses
		apiStatusVersions: make(map[kubetypes.MirrorPodUID]uint64),
		podDeletionSafety: podDeletionSafety,
	}
}
```

# Start运行
- 可以看到for里面监听两个io
    - 如果podStatusChannel有请求就调用syncPod同步一个
    - 如果syncPeriod时间到了，默认10秒钟就syncBatch批量同步
- 代码如下
```go
func (m *manager) Start() {
	// Don't start the status manager if we don't have a client. This will happen
	// on the master, where the kubelet is responsible for bootstrapping the pods
	// of the master components.
	if m.kubeClient == nil {
		klog.InfoS("Kubernetes client is nil, not starting status manager")
		return
	}

	klog.InfoS("Starting to sync pod status with apiserver")
	//lint:ignore SA1015 Ticker can link since this is only called once and doesn't handle termination.
	syncTicker := time.Tick(syncPeriod)
	// syncPod and syncBatch share the same go routine to avoid sync races.
	go wait.Forever(func() {
		for {
			select {
			case syncRequest := <-m.podStatusChannel:
				klog.V(5).InfoS("Status Manager: syncing pod with status from podStatusChannel",
					"podUID", syncRequest.podUID,
					"statusVersion", syncRequest.status.version,
					"status", syncRequest.status.status)
				m.syncPod(syncRequest.podUID, syncRequest.status)
			case <-syncTicker:
				klog.V(5).InfoS("Status Manager: syncing batch")
				// remove any entries in the status channel since the batch will handle them
				for i := len(m.podStatusChannel); i > 0; i-- {
					<-m.podStatusChannel
				}
				m.syncBatch()
			}
		}
	}, 0)
}
``` 



## syncPod同步一个解析
-  判断要不要更新，不需要更新直接return
```go
	if !m.needsUpdate(uid, status) {
		klog.V(1).InfoS("Status for pod is up-to-date; skipping", "podUID", uid)
		return
	}
```
- 获取pod当前的实例
```go
	// TODO: make me easier to express from client code
	pod, err := m.kubeClient.CoreV1().Pods(status.podNamespace).Get(context.TODO(), status.podName, metav1.GetOptions{})
	if errors.IsNotFound(err) {
		klog.V(3).InfoS("Pod does not exist on the server",
			"podUID", uid,
			"pod", klog.KRef(status.podNamespace, status.podName))
		// If the Pod is deleted the status will be cleared in
		// RemoveOrphanedStatuses, so we just ignore the update here.
		return
	}
```

- 拿到pod的真实UID，对于mirror pod，获取静态pod的id，其他pod就是转换id的类型
```go
	translatedUID := m.podManager.TranslatePodUID(pod.UID)

```
- 确认pod是否刚刚重建过，是就return
```go
	// Type convert original uid just for the purpose of comparison.
	if len(translatedUID) > 0 && translatedUID != kubetypes.ResolvedPodUID(uid) {
		klog.V(2).InfoS("Pod was deleted and then recreated, skipping status update",
			"pod", klog.KObj(pod),
			"oldPodUID", uid,
			"podUID", translatedUID)
		m.deletePodStatus(uid)
		return
	}
```
- 检查通过，调用patch接口更新状态
```go
	oldStatus := pod.Status.DeepCopy()
	newPod, patchBytes, unchanged, err := statusutil.PatchPodStatus(m.kubeClient, pod.Namespace, pod.Name, pod.UID, *oldStatus, mergePodStatus(*oldStatus, status.status))
	klog.V(3).InfoS("Patch status for pod", "pod", klog.KObj(pod), "patch", string(patchBytes))

	if err != nil {
		klog.InfoS("Failed to update status for pod", "pod", klog.KObj(pod), "err", err)
		return
	}
	if unchanged {
		klog.V(3).InfoS("Status for pod is up-to-date", "pod", klog.KObj(pod), "statusVersion", status.version)
	} else {
		klog.V(3).InfoS("Status for pod updated successfully", "pod", klog.KObj(pod), "statusVersion", status.version, "status", status.status)
		pod = newPod
	}

```

- 判断status是否可以删除，删除pod并清理statusManager保存该pod的缓存数据
```go
	// We don't handle graceful deletion of mirror pods.
	if m.canBeDeleted(pod, status.status) {
		deleteOptions := metav1.DeleteOptions{
			GracePeriodSeconds: new(int64),
			// Use the pod UID as the precondition for deletion to prevent deleting a
			// newly created pod with the same name and namespace.
			Preconditions: metav1.NewUIDPreconditions(string(pod.UID)),
		}
		err = m.kubeClient.CoreV1().Pods(pod.Namespace).Delete(context.TODO(), pod.Name, deleteOptions)
		if err != nil {
			klog.InfoS("Failed to delete status for pod", "pod", klog.KObj(pod), "err", err)
			return
		}
		klog.V(3).InfoS("Pod fully terminated and removed from etcd", "pod", klog.KObj(pod))
		m.deletePodStatus(uid)
	}
```

### 开头的needsUpdate解析
- mirror pod 的版本不对或者不存在就更新
- podManager不存在不需要更新
- 如果能被删除就更新
```go
// needsUpdate returns whether the status is stale for the given pod UID.
// This method is not thread safe, and must only be accessed by the sync thread.
func (m *manager) needsUpdate(uid types.UID, status versionedPodStatus) bool {
	latest, ok := m.apiStatusVersions[kubetypes.MirrorPodUID(uid)]
	if !ok || latest < status.version {
		return true
	}
	pod, ok := m.podManager.GetPodByUID(uid)
	if !ok {
		return false
	}
	return m.canBeDeleted(pod, status.status)
}

```
#### canBeDeleted解析
- 主要在PodResourcesAreReclaimed中判断pod是否可以被安全的删除
```go
func (m *manager) canBeDeleted(pod *v1.Pod, status v1.PodStatus) bool {
	if pod.DeletionTimestamp == nil || kubetypes.IsMirrorPod(pod) {
		return false
	}
	return m.podDeletionSafety.PodResourcesAreReclaimed(pod, status)
}

```
> PodResourcesAreReclaimed中的几个判定条件
- pod terminated了但是有些容器还在running
- pod terminated了但是有些容器还未报告
- pod terminated了但是有些volume还未被清理
- pod terminated了但是pod cgroup sandbox 还未清理
- 
```go
func (kl *Kubelet) PodResourcesAreReclaimed(pod *v1.Pod, status v1.PodStatus) bool {
	if kl.podWorkers.CouldHaveRunningContainers(pod.UID) {
		// We shouldn't delete pods that still have running containers
		klog.V(3).InfoS("Pod is terminated, but some containers are still running", "pod", klog.KObj(pod))
		return false
	}
	if count := countRunningContainerStatus(status); count > 0 {
		// We shouldn't delete pods until the reported pod status contains no more running containers (the previous
		// check ensures no more status can be generated, this check verifies we have seen enough of the status)
		klog.V(3).InfoS("Pod is terminated, but some container status has not yet been reported", "pod", klog.KObj(pod), "running", count)
		return false
	}
	if kl.podVolumesExist(pod.UID) && !kl.keepTerminatedPodVolumes {
		// We shouldn't delete pods whose volumes have not been cleaned up if we are not keeping terminated pod volumes
		klog.V(3).InfoS("Pod is terminated, but some volumes have not been cleaned up", "pod", klog.KObj(pod))
		return false
	}
	if kl.kubeletConfiguration.CgroupsPerQOS {
		pcm := kl.containerManager.NewPodContainerManager()
		if pcm.Exists(pod) {
			klog.V(3).InfoS("Pod is terminated, but pod cgroup sandbox has not been cleaned up", "pod", klog.KObj(pod))
			return false
		}
	}

	// Note: we leave pod containers to be reclaimed in the background since dockershim requires the
	// container for retrieving logs and we want to make sure logs are available until the pod is
	// physically deleted.

	klog.V(3).InfoS("Pod is terminated and all resources are reclaimed", "pod", klog.KObj(pod))
	return true
}

```

## syncBatch匹配解析
- 遍历podStatuses缓存，调用 needsUpdate判断是否需要更新
```go
		for uid, status := range m.podStatuses {
			syncedUID := kubetypes.MirrorPodUID(uid)
			if mirrorUID, ok := podToMirror[kubetypes.ResolvedPodUID(uid)]; ok {
				if mirrorUID == "" {
					klog.V(5).InfoS("Static pod does not have a corresponding mirror pod; skipping",
						"podUID", uid,
						"pod", klog.KRef(status.podNamespace, status.podName))
					continue
				}
				syncedUID = mirrorUID
			}
			if m.needsUpdate(types.UID(syncedUID), status) {
				updatedStatuses = append(updatedStatuses, podStatusSyncRequest{uid, status})
```
- 遍历要更新pod 的updatedStatuses调用syncPod更新
```go
	for _, update := range updatedStatuses {
		klog.V(5).InfoS("Status Manager: syncPod in syncbatch", "podUID", update.podUID)
		m.syncPod(update.podUID, update.status)
	}
```

### needsReconcile协调判断
- 主要检查pod.status.conditions内容是否一致
```go
func (m *manager) needsReconcile(uid types.UID, status v1.PodStatus) bool {
	// The pod could be a static pod, so we should translate first.
	pod, ok := m.podManager.GetPodByUID(uid)
	if !ok {
		klog.V(4).InfoS("Pod has been deleted, no need to reconcile", "podUID", string(uid))
		return false
	}
	// If the pod is a static pod, we should check its mirror pod, because only status in mirror pod is meaningful to us.
	if kubetypes.IsStaticPod(pod) {
		mirrorPod, ok := m.podManager.GetMirrorPodByPod(pod)
		if !ok {
			klog.V(4).InfoS("Static pod has no corresponding mirror pod, no need to reconcile", "pod", klog.KObj(pod))
			return false
		}
		pod = mirrorPod
	}

	podStatus := pod.Status.DeepCopy()
	normalizeStatus(pod, podStatus)

	if isPodStatusByKubeletEqual(podStatus, &status) {
		// If the status from the source is the same with the cached status,
		// reconcile is not needed. Just return.
		return false
	}
	klog.V(3).InfoS("Pod status is inconsistent with cached status for pod, a reconciliation should be triggered",
		"pod", klog.KObj(pod),
		"statusDiff", diff.ObjectDiff(podStatus, &status))

	return true
}

```
> isPodStatusByKubeletEqual判断 Condition
```go
func isPodStatusByKubeletEqual(oldStatus, status *v1.PodStatus) bool {
	oldCopy := oldStatus.DeepCopy()
	for _, c := range status.Conditions {
		if kubetypes.PodConditionByKubelet(c.Type) {
			_, oc := podutil.GetPodCondition(oldCopy, c.Type)
			if oc == nil || oc.Status != c.Status || oc.Message != c.Message || oc.Reason != c.Reason {
				return false
			}
		}
	}
	oldCopy.Conditions = status.Conditions
	return apiequality.Semantic.DeepEqual(oldCopy, status)
}
```



# statusManager如何被外部调用
- 观察接口方法知道是SetPodStatus触发的
## 在kubelet的syncPod中
- 会调用 SetPodStatus
```go
	kl.statusManager.SetPodStatus(pod, apiPodStatus)

```


## SetPodStatus
- 给定pod设置pod status
```go

func (m *manager) SetPodStatus(pod *v1.Pod, status v1.PodStatus) {
	m.podStatusesLock.Lock()
	defer m.podStatusesLock.Unlock()

	// Make sure we're caching a deep copy.
	status = *status.DeepCopy()

	// Force a status update if deletion timestamp is set. This is necessary
	// because if the pod is in the non-running state, the pod worker still
	// needs to be able to trigger an update and/or deletion.
	m.updateStatusInternal(pod, status, pod.DeletionTimestamp != nil)
}

```


### updateStatusInternal解读
- 从缓存中取出old status
```go
	var oldStatus v1.PodStatus
	cachedStatus, isCached := m.podStatuses[pod.UID]
	if isCached {
		oldStatus = cachedStatus.status
	} else if mirrorPod, ok := m.podManager.GetMirrorPodByPod(pod); ok {
		oldStatus = mirrorPod.Status
	} else {
		oldStatus = pod.Status
	}

```

- 检查pod的containers和init containers不会从teminated状态转化到非terminated
```go
	// Check for illegal state transition in containers
	if err := checkContainerStateTransition(oldStatus.ContainerStatuses, status.ContainerStatuses, pod.Spec.RestartPolicy); err != nil {
		klog.ErrorS(err, "Status update on pod aborted", "pod", klog.KObj(pod))
		return false
	}
	if err := checkContainerStateTransition(oldStatus.InitContainerStatuses, status.InitContainerStatuses, pod.Spec.RestartPolicy); err != nil {
		klog.ErrorS(err, "Status update on pod aborted", "pod", klog.KObj(pod))
		return false
	}

```

- 更新操作时间
```go
	// Set ContainersReadyCondition.LastTransitionTime.
	updateLastTransitionTime(&status, &oldStatus, v1.ContainersReady)

	// Set ReadyCondition.LastTransitionTime.
	updateLastTransitionTime(&status, &oldStatus, v1.PodReady)

	// Set InitializedCondition.LastTransitionTime.
	updateLastTransitionTime(&status, &oldStatus, v1.PodInitialized)

	// Set PodScheduledCondition.LastTransitionTime.
	updateLastTransitionTime(&status, &oldStatus, v1.PodScheduled)
```

- 组装对象 ，发送信号，触发syncPod动作
```go

	newStatus := versionedPodStatus{
		status:       status,
		version:      cachedStatus.version + 1,
		podName:      pod.Name,
		podNamespace: pod.Namespace,
	}
	m.podStatuses[pod.UID] = newStatus

	select {
	case m.podStatusChannel <- podStatusSyncRequest{pod.UID, newStatus}:
		klog.V(5).InfoS("Status Manager: adding pod with new status to podStatusChannel",
			"pod", klog.KObj(pod),
			"podUID", pod.UID,
			"statusVersion", newStatus.version,
			"status", newStatus.status)
		return true
	default:
		// Let the periodic syncBatch handle the update if the channel is full.
		// We can't block, since we hold the mutex lock.
		klog.V(4).InfoS("Skipping the status update for pod for now because the channel is full",
			"pod", klog.KObj(pod),
			"status", status)
		return false
	}
```



#  本节重点总结 : 
- statusManager作用
    - statusManager 的主要功能是将 pod 状态信息同步到 apiserver
    - statusManager 并不会主动监控 pod 的状态，而是提供接口供其他 manager 进行调用
    - 等于是把podManger中负责同步podstatus的部分抽取出来了，看数据结构有podManager
- kubelet的syncPod中会调用 SetPodStatus
    - 内部通过队列传递给处理程序，程序判断是否要更新
    - 底层调用client 向apiserver发送patch更新