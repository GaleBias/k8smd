
# 本节重点总结 : 
- 创建容器调用cgroupManager 创建cgroups，传入的就是容器配置的资源request
- 分布创建3种qos的pod 验证他们的cgroup目录
- 针对三种qos的pod cgroup目录有所区别，以cpu为例
    - /sys/fs/cgroup/cpu/kubepods.slice 代表 pod qos顶级目录
    -  /sys/fs/cgroup/cpu/kubepods-besteffort.slice 代表 besteffort类型的pod 
    -  /sys/fs/cgroup/cpu/kubepods-burstable.slice 代表 burstable类型的pod 
    -  /sys/fs/cgroup/cpu/kubepods-pod4115089e_088b_4fb1_8884_b3f163efa48c.slice 代表 guaranteed类型的pod ，每个pod以他们的id结尾
- pod清理时先kill 对应pod cgroup的pid 再调用底层删除cgroup，即删除目录

# cgroupManager 创建容器cgroup应用

## podContainerManager的EnsureExists解析
- 追踪cgroupManager.Create可以看到在 podContainerManager的EnsureExists中，位置 D:\go_path\src\github.com\kubernetes\kubernetes\pkg\kubelet\cm\pod_container_manager_linux.go

### 首先获取pod容器的cgroup name
```go
	podContainerName, _ := m.GetPodContainerName(pod)
```
- 底层使用GetPodContainerName获取，首先获取pod的 qos
```go
	podQOS := v1qos.GetPodQOS(pod)
```
- 然后根据qos不同确定父级cgroup目录 parentContainer
```go
	var parentContainer CgroupName
	switch podQOS {
	case v1.PodQOSGuaranteed:
		parentContainer = m.qosContainersInfo.Guaranteed
	case v1.PodQOSBurstable:
		parentContainer = m.qosContainersInfo.Burstable
	case v1.PodQOSBestEffort:
		parentContainer = m.qosContainersInfo.BestEffort
	}
```

- 比如 burstable 类型的pod parentContainer就是  {"kubepods", "burstable", "pod1234-abcd-5678-efgh"}

- 再把容器的id 拼接进去
```go
	// Get the absolute path of the cgroup
	cgroupName := NewCgroupName(parentContainer, podContainer)
	// Get the literal cgroupfs name
	cgroupfsName := m.cgroupManager.Name(cgroupName)

```
- 最后的podContainerName形如
```shell script
/sys/fs/cgroup/systemd/kubepods.slice/kubepods-pod4115089e_088b_4fb1_8884_b3f163efa48c.slice/cri-containerd-52b928e34ae91d177696a45aeba9aba96de9ce08964d02670a8a01a43afbfb29.scope
```

### 用拿到的podContainerName 创建 pod cgroup
- 先判断一下容器是否存在
```go
	alreadyExists := m.Exists(pod)
```
- 不存在就创建
- 判断是否要启用 MemoryQoS，v1.22加入的新特性，含义是cgroup v2 内存控制器在 pod / 容器上启用内存保护和使用限制。
```go
		enforceMemoryQoS := false
		if utilfeature.DefaultFeatureGate.Enabled(kubefeatures.MemoryQoS) &&
			libcontainercgroups.IsCgroup2UnifiedMode() {
			enforceMemoryQoS = true
		}
```

- 创建 CgroupConfig
```go
		// Create the pod container
		containerConfig := &CgroupConfig{
			Name:               podContainerName,
			ResourceParameters: ResourceConfigForPod(pod, m.enforceCPULimits, m.cpuCFSQuotaPeriod, enforceMemoryQoS),
		}
```
- 调用ResourceConfigForPod ，传入pod的request配置解析后返回 cgroup的信息
- 最终调用cgroupManager create创建容器 的cgroups
```go
		if err := m.cgroupManager.Create(containerConfig); err != nil {
			return fmt.Errorf("failed to create container for %v : %v", podContainerName, err)
		}
```

## EnsureExists是 Kubelet的syncpod时调用的
- 位置 D:\go_path\src\github.com\kubernetes\kubernetes\pkg\kubelet\kubelet.go
- 从注释就可以看出，有两种情况不会创建或更新cgroups
    - 如果pod被kill了 不会创建或更新 cgroups 
    - 如果RestartPolicy是RestartPolicyNever
- 代码如下
```go
		// Create and Update pod's Cgroups
		// Don't create cgroups for run once pod if it was killed above
		// The current policy is not to restart the run once pods when
		// the kubelet is restarted with the new flag as run once pods are
		// expected to run only once and if the kubelet is restarted then
		// they are not expected to run again.
		// We don't create and apply updates to cgroup if its a run once pod and was killed above
		if !(podKilled && pod.Spec.RestartPolicy == v1.RestartPolicyNever) {
			if !pcm.Exists(pod) {
				if err := kl.containerManager.UpdateQOSCgroups(); err != nil {
					klog.V(2).InfoS("Failed to update QoS cgroups while syncing pod", "pod", klog.KObj(pod), "err", err)
				}
				if err := pcm.EnsureExists(pod); err != nil {
					kl.recorder.Eventf(pod, v1.EventTypeWarning, events.FailedToCreatePodContainer, "unable to ensure pod container exists: %v", err)
					return fmt.Errorf("failed to ensure that the pod: %v cgroups exist and are correctly applied: %v", pod.UID, err)
				}
			}
		}
```



# systemd cgroup v1 subsystem 创建目录解析
- 首先创建一个 qos=guaranteed的 pod
```shell script
cat <<EOF | kubectl create -f -
apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod-guaranteed
spec:
  containers:
  - name: nginx
    image: nginx:1.8
    resources:
        requests:
          cpu: 100m
          memory: 100Mi
        limits:
          cpu: 100m
          memory: 100Mi
EOF


```

- 创建一个 qos=besteffort的 pod

```shell script
cat <<EOF | kubectl create -f -
apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod-besteffort
spec:
  containers:
  - name: nginx
    image: nginx:1.8
EOF
```

- 创建一个 qos=burstable的 pod

```shell script
cat <<EOF | kubectl create -f -
apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod-burstable
spec:
  containers:
  - name: nginx
    image: nginx:1.8
    resources:
        requests:
          cpu: 100m
          memory: 100Mi
        limits:
          cpu: 200m
          memory: 200Mi
EOF
```

> 到node上查看cgroup的目录 
- /sys/fs/cgroup/systemd/kubepods.slice 为systemd驱动的cgroup pod 目录
```shell script
ll /sys/fs/cgroup/systemd/kubepods.slice
total 0
-rw-r--r--  1 root root 0 Aug 20 11:05 cgroup.clone_children
--w--w--w-  1 root root 0 Aug 20 11:05 cgroup.event_control
-rw-r--r--  1 root root 0 Aug 20 11:05 cgroup.procs
drwxr-xr-x 21 root root 0 Sep 10 16:34 kubepods-besteffort.slice
drwxr-xr-x 21 root root 0 Sep  6 11:51 kubepods-burstable.slice
drwxr-xr-x  4 root root 0 Sep 27 11:38 kubepods-pod4115089e_088b_4fb1_8884_b3f163efa48c.slice
-rw-r--r--  1 root root 0 Aug 20 11:05 notify_on_release
-rw-r--r--  1 root root 0 Aug 20 11:05 tasks
```
- 其中有三个文件夹
    - kubepods-pod<podid> 代表guaranteed的pod ，每个pod一个，以他们的podid为结尾
    - kubepods-besteffort.slice 代表besteffort的pod
    - kubepods-burstable.slice 代表 burstable的pod
- 每个pod下面有具体的 cgroups配置
- 比如查看对应guaranteed的cpu cgroup配置 
```shell script
[root@k8s-node01 cri-containerd-52b928e34ae91d177696a45aeba9aba96de9ce08964d02670a8a01a43afbfb29.scope]# ll /sys/fs/cgroup/cpu/kubepods.slice/kubepods-pod4115089e_088b_4fb1_8884_b3f163efa48c.slice/cri-containerd-52b928e34ae91d177696a45aeba9aba96de9ce08964d02670a8a01a43afbfb29.scope
total 0
-rw-r--r-- 1 root root 0 Sep 27 11:38 cgroup.clone_children
--w--w--w- 1 root root 0 Sep 27 11:38 cgroup.event_control
-rw-r--r-- 1 root root 0 Sep 27 11:38 cgroup.procs
-r--r--r-- 1 root root 0 Sep 27 11:38 cpuacct.stat
-rw-r--r-- 1 root root 0 Sep 27 11:38 cpuacct.usage
-r--r--r-- 1 root root 0 Sep 27 11:38 cpuacct.usage_percpu
-rw-r--r-- 1 root root 0 Sep 27 11:38 cpu.cfs_period_us
-rw-r--r-- 1 root root 0 Sep 27 11:38 cpu.cfs_quota_us
-rw-r--r-- 1 root root 0 Sep 27 11:38 cpu.rt_period_us
-rw-r--r-- 1 root root 0 Sep 27 11:38 cpu.rt_runtime_us
-rw-r--r-- 1 root root 0 Sep 27 11:38 cpu.shares
-r--r--r-- 1 root root 0 Sep 27 11:38 cpu.stat
-rw-r--r-- 1 root root 0 Sep 27 11:38 notify_on_release
-rw-r--r-- 1 root root 0 Sep 27 11:38 tasks


```

- 对应的cpu限制为, cpu.cfs_period_us = 默认值是100000us(100ms) 代表1个核 即0.1s的cpu时间
```shell script
cat cpu.cfs_period_us 
100000

cat cpu.cfs_quota_us 
10000
cat cpu.shares
102
```
- cpu.cfs_quota_us /cpu.cfs_period_us  =0.1 代表使用0.1个核和我们limit中配置的 100m一致
- 而 cpu.shares也被设置为1024 的0.1也就是 102 ，cpu.share默认值是1024


# systemd cgroup v1 cgroup 目录解析
- 使用  systemd-cgls 可以打印cgroup的详情
- 可以发现k8s pod相关的都在kubepods.slice 下面
- kubepods-pod<podid> 代表guaranteed的pod 
- besteffort的在kubepods-besteffort.slice下面
- burstable的在kubepods-burstable.slice下面
```shell script
# 能够发现
├─1 /usr/lib/systemd/systemd --switched-root --system --deserialize 21
├─kubepods.slice
│ ├─kubepods-pod4115089e_088b_4fb1_8884_b3f163efa48c.slice
│ │ ├─cri-containerd-52b928e34ae91d177696a45aeba9aba96de9ce08964d02670a8a01a43afbfb29.scope
│ │ │ ├─2486 nginx: master process nginx -g daemon off
│ │ │ └─2535 nginx: worker proces
│ │ └─cri-containerd-23fb835658872a84de508af948ffc2ad1d9dc791b8bffb05c0adc8a21346dacd.scope
│ │   └─2441 /pause


│ ├─kubepods-besteffort.slice
│ │ ├─kubepods-besteffort-pod4850828c_35eb_4cfb_98b0_ecd6129535c3.slice
│ │ │ ├─cri-containerd-90562142a7f3aa69b4a5a43efe13d641acbfac13f63a24c827067c58f3280da3.scope
│ │ │ │ ├─759 nginx: master process nginx -g daemon off
│ │ │ │ └─773 nginx: worker proces
│ │ │ └─cri-containerd-4a6227e975e72f3be06ce59db210fae75c621b7772b3187afbf6eecb284a1ebd.scope
│ │ │   └─722 /pause
│ │ ├─kubepods-besteffort-poda2ae6671_a866_42bc_96aa_4be42e62ee5b.slice
│ │ │ ├─cri-containerd-a95b6cdddae38c7eecbfb93638f05dd4ee9d4ad4238e04bef5992de90b63946e.scope
│ │ │ │ ├─16638 nginx: master process nginx -g daemon off
│ │ │ │ └─16651 nginx: worker proces
│ │ │ └─cri-containerd-90f7d6eadfa6a961bcf63ff76c4e54cf1d7b96dcaf5c4aee31a8295f4a1b33aa.scope
│ │ │   └─16604 /pause


│ └─kubepods-burstable.slice
│   ├─kubepods-burstable-podc9595f0f_3d44_4049_a5eb_62d245071752.slice
│   │ ├─cri-containerd-6b5a91869fc4a00f39ad9c41a9e92b2674d10bcb4fe54a00dbec040aa1e1fbd7.scope
│   │ │ ├─1046 nginx: master process nginx -g daemon off
│   │ │ └─1059 nginx: worker proces
│   │ └─cri-containerd-edb5e7b382d872d57279117b1d2fdf391677b3af708ee4042a2622a50a3ebede.scope
│   │   └─1009 /pause
│   ├─kubepods-burstable-pod937e07bc_5cea_4e3d_83ac_a2e68e072340.slice
│   │ ├─cri-containerd-757d7cddae4c141f5af7c5578aadfb3d49950b19ebdf20cf933dd18ac2ce1a31.scope
│   │ │ └─3925 /bin/prometheus --web.console.templates=/etc/prometheus/consoles --web.console.libraries=/etc/prometheus/console_libraries --config.file=/etc/prometheus/config_out/prometheus.env.yaml --storage.tsdb.path=/promet
│   │ ├─cri-containerd-e232b4ffb5043e10e6c6d98dc1c7365241d621c5d29e859c61e296d2ff731f1b.scope
│   │ │ └─3665 /bin/prometheus-config-reloader --listen-address=:8080 --reload-url=http://localhost:9090/-/reload --config-file=/etc/prometheus/config/prometheus.yaml.gz --config-envsubst-file=/etc/prometheus/config_out/promet
│   │ └─cri-containerd-7f34948138f71db4fc3521de8c2fcb76094ddc2ceb5b6f61a1064d98d99147e0.scope
│   │   └─3316 /pause
│   ├─kubepods-burstable-pod8898c8f2_1ea7_412f_8a25_ce98a8ca47c2.slice
│   │ ├─cri-containerd-05569aac10982a3147c61eacf6f6ff6210746480ffc7831d231daa38fc7fa187.scope
│   │ │ └─3908 /bin/prometheus --web.console.templates=/etc/prometheus/consoles --web.console.libraries=/etc/prometheus/console_libraries --config.file=/etc/prometheus/config_out/prometheus.env.yaml --storage.tsdb.path=/promet
│   │ ├─cri-containerd-2918ed019bf61309737e1c1d5994a87ef347bd7bde57c093aaf0f46894e79365.scope
│   │ │ └─3672 /bin/prometheus-config-reloader --listen-address=:8080 --reload-url=http://localhost:9090/-/reload --config-file=/etc/prometheus/config/prometheus.yaml.gz --config-envsubst-file=/etc/prometheus/config_out/promet
│   │ └─cri-containerd-650ecc3e0300b30822e37ae55d556f15a16bd30fd60b3c8d8f929d4706f5d692.scope
│   │   └─3348 /pause
```



# 删除pod时 cgroupManager 删除容器cgroup
- 入口在 kubelet 的大循环syncLoopIteration中
- 其中监听了housekeepingCh chan，代表定时执行清理任务
```go
	case <-housekeepingCh:
		if !kl.sourcesReady.AllReady() {
			// If the sources aren't ready or volume manager has not yet synced the states,
			// skip housekeeping, as we may accidentally delete pods from unready sources.
			klog.V(4).InfoS("SyncLoop (housekeeping, skipped): sources aren't ready yet")
		} else {
			start := time.Now()
			klog.V(4).InfoS("SyncLoop (housekeeping)")
			if err := handler.HandlePodCleanups(); err != nil {
				klog.ErrorS(err, "Failed cleaning pods")
			}
			duration := time.Since(start)
			if duration > housekeepingWarningDuration {
				klog.ErrorS(fmt.Errorf("housekeeping took too long"), "Housekeeping took longer than 15s", "seconds", duration.Seconds())
			}
			klog.V(4).InfoS("SyncLoop (housekeeping) end")
		}
	}
```


## HandlePodCleanups 解析
- HandlePodCleanups执行一系列清理工作，包括
    - 终止podworker
    - 杀死不需要的pod
    - 并删除孤立卷和pod
  
- 其中和cgroup相关的是调用 cleanupOrphanedPodCgroups清理
```go
	// Remove any cgroups in the hierarchy for pods that are definitely no longer
	// running (not in the container runtime).
	if kl.cgroupsPerQOS {
		pcm := kl.containerManager.NewPodContainerManager()
		klog.V(3).InfoS("Clean up orphaned pod cgroups")
		kl.cleanupOrphanedPodCgroups(pcm, cgroupPods, possiblyRunningPods)
	}
```

## cleanupOrphanedPodCgroups清理
- 遍历pod 判断pod的volume 不存在，pod不是running 才调用PodContainerManager的Destroy清理
```go
// cleanupOrphanedPodCgroups removes cgroups that should no longer exist.
// it reconciles the cached state of cgroupPods with the specified list of runningPods
func (kl *Kubelet) cleanupOrphanedPodCgroups(pcm cm.PodContainerManager, cgroupPods map[types.UID]cm.CgroupName, possiblyRunningPods map[types.UID]sets.Empty) {
	// Iterate over all the found pods to verify if they should be running
	for uid, val := range cgroupPods {
		// if the pod is in the running set, its not a candidate for cleanup
		if _, ok := possiblyRunningPods[uid]; ok {
			continue
		}

		// If volumes have not been unmounted/detached, do not delete the cgroup
		// so any memory backed volumes don't have their charges propagated to the
		// parent croup.  If the volumes still exist, reduce the cpu shares for any
		// process in the cgroup to the minimum value while we wait.  if the kubelet
		// is configured to keep terminated volumes, we will delete the cgroup and not block.
		if podVolumesExist := kl.podVolumesExist(uid); podVolumesExist && !kl.keepTerminatedPodVolumes {
			klog.V(3).InfoS("Orphaned pod found, but volumes not yet removed.  Reducing cpu to minimum", "podUID", uid)
			if err := pcm.ReduceCPULimits(val); err != nil {
				klog.InfoS("Failed to reduce cpu time for pod pending volume cleanup", "podUID", uid, "err", err)
			}
			continue
		}
		klog.V(3).InfoS("Orphaned pod found, removing pod cgroups", "podUID", uid)
		// Destroy all cgroups of pod that should not be running,
		// by first killing all the attached processes to these cgroups.
		// We ignore errors thrown by the method, as the housekeeping loop would
		// again try to delete these unwanted pod cgroups
		go pcm.Destroy(val)
	}
}
```

## podContainerManager Destroy解析

### 在清理前先 kill 这个从cgroup 所有的进程pid
```go
	// Try killing all the processes attached to the pod cgroup
	if err := m.tryKillingCgroupProcesses(podCgroup); err != nil {
		klog.InfoS("Failed to kill all the processes attached to cgroup", "cgroupName", podCgroup, "err", err)
		return fmt.Errorf("failed to kill all the processes attached to the %v cgroups : %v", podCgroup, err)
	}

```
- 先通过 cgroupManager的Pid方法获取 pid
```go
	pidsToKill := m.cgroupManager.Pids(podCgroup)
	// No pids charged to the terminated pod cgroup return
	if len(pidsToKill) == 0 {
		return nil
	}

```
### cgroupManager的Pid方法
- 新建一个待kill 的pid map
```go
	// Get a list of processes that we need to kill
	pidsToKill := sets.NewInt()
```
- 遍历 subsystems目录，调用getCgroupProcs获取 pid 
```go
	for _, val := range m.subsystems.MountPoints {
		dir := path.Join(val, cgroupFsName)
		_, err := os.Stat(dir)
		if os.IsNotExist(err) {
			// The subsystem pod cgroup is already deleted
			// do nothing, continue
			continue
		}
		// Get a list of pids that are still charged to the pod's cgroup
		pids, err = getCgroupProcs(dir)
		if err != nil {
			continue
		}
```
- getCgroupProcs底层 查看是subsystem目录下的cgroup.procs拿到对应的进程pid
```go
func getCgroupProcs(dir string) ([]int, error) {
	procsFile := filepath.Join(dir, "cgroup.procs")
	f, err := os.Open(procsFile)
	if err != nil {
		if os.IsNotExist(err) {
			// The procsFile does not exist, So no pids attached to this directory
			return []int{}, nil
		}
		return nil, err
	}
	defer f.Close()

	s := bufio.NewScanner(f)
	out := []int{}
	for s.Scan() {
		if t := s.Text(); t != "" {
			pid, err := strconv.Atoi(t)
			if err != nil {
				return nil, fmt.Errorf("unexpected line in %v; could not convert to pid: %v", procsFile, err)
			}
			out = append(out, pid)
		}
	}
	return out, nil
}
```

- 然后调用用 visit和walk方法递归遍历内层目录 ，追加pid列表
```go
		// WalkFunc which is called for each file and directory in the pod cgroup dir
		visitor := func(path string, info os.FileInfo, err error) error {
			if err != nil {
				klog.V(4).InfoS("Cgroup manager encountered error scanning cgroup path", "path", path, "err", err)
				return filepath.SkipDir
			}
			if !info.IsDir() {
				return nil
			}
			pids, err = getCgroupProcs(path)
			if err != nil {
				klog.V(4).InfoS("Cgroup manager encountered error getting procs for cgroup path", "path", path, "err", err)
				return filepath.SkipDir
			}
			pidsToKill.Insert(pids...)
			return nil
		}
		// Walk through the pod cgroup directory to check if
		// container cgroups haven't been GCed yet. Get attached processes to
		// all such unwanted containers under the pod cgroup
		if err = filepath.Walk(dir, visitor); err != nil {
			klog.V(4).InfoS("Cgroup manager encountered error scanning pids for directory", "path", dir, "err", err)
		}
```

> cgroup.procs目录实例 ，ps 对应的pid 可以看到就是nginx 进程
```shell script
[root@k8s-node01 cri-containerd-52b928e34ae91d177696a45aeba9aba96de9ce08964d02670a8a01a43afbfb29.scope]# pwd
/sys/fs/cgroup/systemd/kubepods.slice/kubepods-pod4115089e_088b_4fb1_8884_b3f163efa48c.slice/cri-containerd-52b928e34ae91d177696a45aeba9aba96de9ce08964d02670a8a01a43afbfb29.scope
[root@k8s-node01 cri-containerd-52b928e34ae91d177696a45aeba9aba96de9ce08964d02670a8a01a43afbfb29.scope]# cat cgroup.procs 
2486
2535
[root@k8s-node01 cri-containerd-52b928e34ae91d177696a45aeba9aba96de9ce08964d02670a8a01a43afbfb29.scope]# ps -ef |grep 2486|grep -v grep 
root      2486  2418  0 11:38 ?        00:00:00 nginx: master process nginx -g daemon off;
104       2535  2486  0 11:38 ?        00:00:00 nginx: worker process
[root@k8s-node01 cri-containerd-52b928e34ae91d177696a45aeba9aba96de9ce08964d02670a8a01a43afbfb29.scope]# ps -ef |grep 2535|grep -v grep     
104       2535  2486  0 11:38 ?        00:00:00 nginx: worker process
```


### 带上retry kill pid
```go
	removed := map[int]bool{}
	for i := 0; i < 5; i++ {
		if i != 0 {
			klog.V(3).InfoS("Attempt failed to kill all unwanted process from cgroup, retrying", "attempt", i, "cgroupName", podCgroup)
		}
		errlist = []error{}
		for _, pid := range pidsToKill {
			if _, ok := removed[pid]; ok {
				continue
			}
			klog.V(3).InfoS("Attempting to kill process from cgroup", "pid", pid, "cgroupName", podCgroup)
			if err := m.killOnePid(pid); err != nil {
				klog.V(3).InfoS("Failed to kill process from cgroup", "pid", pid, "cgroupName", podCgroup, "err", err)
				errlist = append(errlist, err)
			} else {
				removed[pid] = true
			}
		}
		if len(errlist) == 0 {
			klog.V(3).InfoS("Successfully killed all unwanted processes from cgroup", "cgroupName", podCgroup)
			return nil
		}
	}
``` 

### 然后调用 cgroupManager Destroy清理
```go
	// Now its safe to remove the pod's cgroup
	containerConfig := &CgroupConfig{
		Name:               podCgroup,
		ResourceParameters: &ResourceConfig{},
	}
	if err := m.cgroupManager.Destroy(containerConfig); err != nil {
		klog.InfoS("Failed to delete cgroup paths", "cgroupName", podCgroup, "err", err)
		return fmt.Errorf("failed to delete cgroup paths for %v : %v", podCgroup, err)
	}
```
- 追踪底层调用的 比如 systemd cgroup v1 的RemovePaths清理目录 ，位置D:\go_path\src\github.com\kubernetes\kubernetes\vendor\github.com\opencontainers\runc\libcontainer\cgroups\utils.go
```go
func RemovePaths(paths map[string]string) (err error) {
	const retries = 5
	delay := 10 * time.Millisecond
	for i := 0; i < retries; i++ {
		if i != 0 {
			time.Sleep(delay)
			delay *= 2
		}
		for s, p := range paths {
			if err := RemovePath(p); err != nil {
				// do not log intermediate iterations
				switch i {
				case 0:
					logrus.WithError(err).Warnf("Failed to remove cgroup (will retry)")
				case retries - 1:
					logrus.WithError(err).Error("Failed to remove cgroup")
				}
			}
			_, err := os.Stat(p)
			// We need this strange way of checking cgroups existence because
			// RemoveAll almost always returns error, even on already removed
			// cgroups
			if os.IsNotExist(err) {
				delete(paths, s)
			}
		}
		if len(paths) == 0 {
			//nolint:ineffassign,staticcheck // done to help garbage collecting: opencontainers/runc#2506
			paths = make(map[string]string)
			return nil
		}
	}
	return fmt.Errorf("Failed to remove paths: %v", paths)
}
```



# 本节重点总结 : 
- 创建容器调用cgroupManager 创建cgroups，传入的就是容器配置的资源request
- 分布创建3种qos的pod 验证他们的cgroup目录
- 针对三种qos的pod cgroup目录有所区别，以cpu为例
    - /sys/fs/cgroup/cpu/kubepods.slice 代表 pod qos顶级目录
    -  /sys/fs/cgroup/cpu/kubepods-besteffort.slice 代表 besteffort类型的pod 
    -  /sys/fs/cgroup/cpu/kubepods-burstable.slice 代表 burstable类型的pod 
    -  /sys/fs/cgroup/cpu/kubepods-pod4115089e_088b_4fb1_8884_b3f163efa48c.slice 代表 guaranteed类型的pod ，每个pod以他们的id结尾
- pod清理时先kill 对应pod cgroup的pid 再调用底层删除cgroup，即删除目录