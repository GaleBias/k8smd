# 本节重点总结:

- TopologyManager是创建容器时的5大准入控制器之一
- 通过遍历cpu、device、memory三个资源管理器进行拓扑查询
  - 如果有报错证明 资源不能按照提议的拓扑分配，那么admit失败
- 以cpuManager和deviceManager为例解读了 generateCPUTopologyHints的过程
- cpu generateCPUTopologyHints的过程![topology_gethit_cpu.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/908/1634434370000/1d912004249f4fc0b1ddf09f0941ef0d.png)
- merge的流程![topology_merge.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/908/1634434370000/6cbb5c878e204b8b897c2da61fa1b6c7.png)

# TopologyManager 初始化

- 入口在containerManager的初始化的地方，位置 D:\go_path\src\github.com\kubernetes\kubernetes\pkg\kubelet\cm\container_manager_linux.go
- 如果开启了TopologyManager特性就进行初始化

```go
	if utilfeature.DefaultFeatureGate.Enabled(kubefeatures.TopologyManager) {
		cm.topologyManager, err = topologymanager.NewManager(
			machineInfo.Topology,
			nodeConfig.ExperimentalTopologyManagerPolicy,
			nodeConfig.ExperimentalTopologyManagerScope,
		)

		if err != nil {
			return nil, err
		}

	} 
```

- 入参分析
  - machineInfo.Topology 代表cadvisor提供的节点numa节点拓扑信息
  - ExperimentalTopologyManagerPolicy 代表TopologyManager的四种分配策略

    - none (默认)
    - best-effort 没找到最优的拓扑建议时节点也会接纳这个pod
    - restricted 没找到最优的拓扑建议时节点会拒绝这个pod
    - single-numa-node 要求找到的最优的拓扑建议只包含1个numa节点，如果没有那么节点会拒绝这个pod
  - ExperimentalTopologyManagerScope 代表拓扑管理器的作用域

    - container是默认的
    - pod

## topologymanager.NewManager

- 首先根据cadvisor提供的节点numa 拓扑获取 numa 节点id

```go
	var numaNodes []int
	for _, node := range topology {
		numaNodes = append(numaNodes, node.Id)
	}
```

- 对numaid 数量上限进行判断，现在最多支持8个

```go

	if topologyPolicyName != PolicyNone && len(numaNodes) > maxAllowableNUMANodes {
		return nil, fmt.Errorf("unsupported on machines with more than %v NUMA Nodes", maxAllowableNUMANodes)
	}
```

- 根据传入的策略进行policy的初始化

```go
	var policy Policy
	switch topologyPolicyName {

	case PolicyNone:
		policy = NewNonePolicy()

	case PolicyBestEffort:
		policy = NewBestEffortPolicy(numaNodes)

	case PolicyRestricted:
		policy = NewRestrictedPolicy(numaNodes)

	case PolicySingleNumaNode:
		policy = NewSingleNumaNodePolicy(numaNodes)

	default:
		return nil, fmt.Errorf("unknown policy: \"%s\"", topologyPolicyName)
	}
```

- 然后使用policy 根据作用域初始化scope

```go
	var scope Scope
	switch topologyScopeName {

	case containerTopologyScope:
		scope = NewContainerScope(policy)

	case podTopologyScope:
		scope = NewPodScope(policy)

	default:
		return nil, fmt.Errorf("unknown scope: \"%s\"", topologyScopeName)
	}

	manager := &manager{
		scope: scope,
	}

```

# TopologyManager 调用

- 在 memoryManager、cpuManager和deviceManager的New中都传入的topologyManager

```go
        // memoryManager
		cm.memoryManager, err = memorymanager.NewManager(
			nodeConfig.ExperimentalMemoryManagerPolicy,
			machineInfo,
			cm.GetNodeAllocatableReservation(),
			nodeConfig.ExperimentalMemoryManagerReservedMemory,
			nodeConfig.KubeletRootDir,
			cm.topologyManager,
		)
        // cpuManager
		cm.cpuManager, err = cpumanager.NewManager(
			nodeConfig.ExperimentalCPUManagerPolicy,
			nodeConfig.ExperimentalCPUManagerPolicyOptions,
			nodeConfig.ExperimentalCPUManagerReconcilePeriod,
			machineInfo,
			nodeConfig.NodeAllocatableConfig.ReservedSystemCPUs,
			cm.GetNodeAllocatableReservation(),
			nodeConfig.KubeletRootDir,
			cm.topologyManager,
		)
        // deviceManager
	if devicePluginEnabled {
		cm.deviceManager, err = devicemanager.NewManagerImpl(machineInfo.Topology, cm.topologyManager)
		cm.topologyManager.AddHintProvider(cm.deviceManager)
```

- 追踪containerManager 的topologyManager可以发现在 NewContainerManager中将 3种HintProvider添加

```go
cm.topologyManager.AddHintProvider(cm.deviceManager)
cm.topologyManager.AddHintProvider(cm.cpuManager)
cm.topologyManager.AddHintProvider(cm.memoryManager)
```

- AddHintProvider底层就是将HintProvider添加到hintProviders数组中

```go
func (s *scope) AddHintProvider(h HintProvider) {
	s.hintProviders = append(s.hintProviders, h)
}
type scope struct {
	mutex sync.Mutex
	name  string
	// Mapping of a Pods mapping of Containers and their TopologyHints
	// Indexed by PodUID to ContainerName
	podTopologyHints podTopologyHints
	// The list of components registered with the Manager
	hintProviders []HintProvider
	// Topology Manager Policy
	policy Policy
	// Mapping of (PodUid, ContainerName) to ContainerID for Adding/Removing Pods from PodTopologyHints mapping
	podMap containermap.ContainerMap
}

```

# 从kubelet Addpod的循环说起

- 在syncLoopIteration监听 配置的chan 中，对应的Add动作会调用HandlePodAdditions

```go
		switch u.Op {
		case kubetypes.ADD:
			klog.V(2).InfoS("SyncLoop ADD", "source", u.Source, "pods", format.Pods(u.Pods))
			// After restarting, kubelet will get all existing pods through
			// ADD as if they are new pods. These pods will then go through the
			// admission process and *may* be rejected. This can be resolved
			// once we have checkpointing.
			handler.HandlePodAdditions(u.Pods)
```

- 在HandlePodAdditions中会调用canAdmitPod对每个pod做准入检查，如果没过则会rejectPod

```go
func (kl *Kubelet) HandlePodAdditions(pods []*v1.Pod) {
	for _, pod := range pods {
					// Check if we can admit the pod; if not, reject it.
        			if ok, reason, message := kl.canAdmitPod(activePods, pod); !ok {
        				kl.rejectPod(pod, reason, message)
        				continue
        			}
	}
}
```

- 在准入检查的canAdmitPod中会遍历kubelet的admitHandlers ，然后调用 podAdmitHandler.Admit做

```go
func (kl *Kubelet) canAdmitPod(pods []*v1.Pod, pod *v1.Pod) (bool, string, string) {
	// the kubelet will invoke each pod admit handler in sequence
	// if any handler rejects, the pod is rejected.
	// TODO: move out of disk check into a pod admitter
	// TODO: out of resource eviction should have a pod admitter call-out
	attrs := &lifecycle.PodAdmitAttributes{Pod: pod, OtherPods: pods}
	for _, podAdmitHandler := range kl.admitHandlers {
		if result := podAdmitHandler.Admit(attrs); !result.Admit {
			return false, result.Reason, result.Message
		}
	}

	return true, "", ""
}
```

- 对应的5个admitHandlers在kubelet的初始化中有体现

```go
    klet.admitHandlers.AddPodAdmitHandler(evictionAdmitHandler)
    klet.admitHandlers.AddPodAdmitHandler(sysctlsAllowlist)
    klet.admitHandlers.AddPodAdmitHandler(klet.containerManager.GetAllocateResourcesPodAdmitHandler())
	klet.admitHandlers.AddPodAdmitHandler(lifecycle.NewPredicateAdmitHandler(klet.getNodeAnyWay, criticalPodAdmissionHandler, klet.containerManager.UpdatePluginResources))
	klet.admitHandlers.AddPodAdmitHandler(shutdownAdmitHandler)

```

- 在klet.containerManager.GetAllocateResourcesPodAdmitHandler中可以找到我们的topologyManager 对应的AdmitHandler

```go
func (cm *containerManagerImpl) GetAllocateResourcesPodAdmitHandler() lifecycle.PodAdmitHandler {
	if utilfeature.DefaultFeatureGate.Enabled(kubefeatures.TopologyManager) {
		return cm.topologyManager
	}
	// TODO: we need to think about a better way to do this. This will work for
	// now so long as we have only the cpuManager and deviceManager relying on
	// allocations here. However, going forward it is not generalized enough to
	// work as we add more and more hint providers that the TopologyManager
	// needs to call Allocate() on (that may not be directly intstantiated
	// inside this component).
	return &resourceAllocator{cm.cpuManager, cm.memoryManager, cm.deviceManager}
}
```

- 所以canAdmitPod会调用 topologyManager的Admit方法对pod进行准入检测

## container作用域下的Admit方法解析

- 位置 D:\go_path\src\github.com\kubernetes\kubernetes\pkg\kubelet\cm\topologymanager\scope_container.go
- 先判断policy是非none

```go
	// Exception - Policy : none
	if s.policy.Name() == PolicyNone {
		return s.admitPolicyNone(pod)
	}

```

- 遍历pod中的init容器和其他容器，调用 获取拓扑结果和准入Admit结果

```go
func (s *containerScope) Admit(pod *v1.Pod) lifecycle.PodAdmitResult {
	for _, container := range append(pod.Spec.InitContainers, pod.Spec.Containers...) {
		bestHint, admit := s.calculateAffinity(pod, &container)
		klog.InfoS("Best TopologyHint", "bestHint", bestHint, "pod", klog.KObj(pod), "containerName", container.Name)

}
```

- 如果admit返回的false，则返回

```go
		if !admit {
			return admission.GetPodAdmitResult(&TopologyAffinityError{})
		}
// TopologyAffinityError represents an resource alignment error
type TopologyAffinityError struct{}

func (e TopologyAffinityError) Error() string {
	return "Resources cannot be allocated with Topology locality"
}

func (e TopologyAffinityError) Type() string {
	return ErrorTopologyAffinity
}
```

- 如果admit返回true，则更新拓扑信息并分配资源

```go
		klog.InfoS("Topology Affinity", "bestHint", bestHint, "pod", klog.KObj(pod), "containerName", container.Name)
		s.setTopologyHints(string(pod.UID), container.Name, bestHint)

		err := s.allocateAlignedResources(pod, &container)
		if err != nil {
			return admission.GetPodAdmitResult(err)
		}
```

### calculateAffinity 判定分析

- 这个函数看起来很简单，只有几行代码

```go
func (s *containerScope) calculateAffinity(pod *v1.Pod, container *v1.Container) (TopologyHint, bool) {
	providersHints := s.accumulateProvidersHints(pod, container)
	bestHint, admit := s.policy.Merge(providersHints)
	klog.InfoS("ContainerTopologyHint", "bestHint", bestHint)
	return bestHint, admit
}
```

- providersHints代表调用accumulateProvidersHints获取pod的TopologyHint
  - 内部就是遍历hintProviders
  - 调用他们的GetTopologyHints方法
- accumulateProvidersHints如下

```go
func (s *containerScope) accumulateProvidersHints(pod *v1.Pod, container *v1.Container) []map[string][]TopologyHint {
	var providersHints []map[string][]TopologyHint

	for _, provider := range s.hintProviders {
		// Get the TopologyHints for a Container from a provider.
		hints := provider.GetTopologyHints(pod, container)
		providersHints = append(providersHints, hints)
		klog.InfoS("TopologyHints", "hints", hints, "pod", klog.KObj(pod), "containerName", container.Name)
	}
	return providersHints
}

```

- 点击HintProvider接口中的GetTopologyHints方法实现可以看到 对应的3个资源Manager 都实现了对应的方法
- 下面以cpuManager为例查看对应的GetTopologyHints方法

### cpuManager的GetTopologyHints

- 首先获取guaranteed pod请求的cpu

```go
	// Get a count of how many guaranteed CPUs have been requested by Pod.
	requested := p.podGuaranteedCPUs(pod)

```

- 对应的podGuaranteedCPUs逻辑为
  - 遍历pod init容器，找到其中cpu request的最大值
  - 遍历pod的业务容器，对cpu request 进行sum
  - 然后返回 sum 和max 的最大值
- 代码如下

```go
func (p *staticPolicy) podGuaranteedCPUs(pod *v1.Pod) int {
	// The maximum of requested CPUs by init containers.
	requestedByInitContainers := 0
	for _, container := range pod.Spec.InitContainers {
		if _, ok := container.Resources.Requests[v1.ResourceCPU]; !ok {
			continue
		}
		requestedCPU := p.guaranteedCPUs(pod, &container)
		if requestedCPU > requestedByInitContainers {
			requestedByInitContainers = requestedCPU
		}
	}
	// The sum of requested CPUs by app containers.
	requestedByAppContainers := 0
	for _, container := range pod.Spec.Containers {
		if _, ok := container.Resources.Requests[v1.ResourceCPU]; !ok {
			continue
		}
		requestedByAppContainers += p.guaranteedCPUs(pod, &container)
	}

	if requestedByInitContainers > requestedByAppContainers {
		return requestedByInitContainers
	}
	return requestedByAppContainers
}

```

- 遍历init容器和业务容器，在缓存中查找分配的结果缓存，如果有就求和最后对比这次的request

```go
	assignedCPUs := cpuset.NewCPUSet()
	for _, container := range append(pod.Spec.InitContainers, pod.Spec.Containers...) {
		requestedByContainer := p.guaranteedCPUs(pod, &container)
		// Short circuit to regenerate the same hints if there are already
		// guaranteed CPUs allocated to the Container. This might happen after a
		// kubelet restart, for example.
		if allocated, exists := s.GetCPUSet(string(pod.UID), container.Name); exists {
			if allocated.Size() != requestedByContainer {
				klog.ErrorS(nil, "CPUs already allocated to container with different number than request", "pod", klog.KObj(pod), "containerName", container.Name, "allocatedSize", requested, "requestedByContainer", requestedByContainer, "allocatedSize", allocated.Size())
				// An empty list of hints will be treated as a preference that cannot be satisfied.
				// In definition of hints this is equal to: TopologyHint[NUMANodeAffinity: nil, Preferred: false].
				// For all but the best-effort policy, the Topology Manager will throw a pod-admission error.
				return map[string][]topologymanager.TopologyHint{
					string(v1.ResourceCPU): {},
				}
			}
			// A set of CPUs already assigned to containers in this pod
			assignedCPUs = assignedCPUs.Union(allocated)
		}
	}
	if assignedCPUs.Size() == requested {
		klog.InfoS("Regenerating TopologyHints for CPUs already allocated", "pod", klog.KObj(pod))
		return map[string][]topologymanager.TopologyHint{
			string(v1.ResourceCPU): p.generateCPUTopologyHints(assignedCPUs, cpuset.CPUSet{}, requested),
		}
	}

```

- 获取节点上可分配的cpu 默认的cpuset - 预留的

```go
	// Get a list of available CPUs.
	available := p.GetAllocatableCPUs(s)
// GetAllocatableCPUs returns the set of unassigned CPUs minus the reserved set.
func (p *staticPolicy) GetAllocatableCPUs(s state.State) cpuset.CPUSet {
	return s.GetDefaultCPUSet().Difference(p.reserved)
}
```

- 从reuseCpuMap中获取 init容器reuse的cpu，如果是新的容器reusable则为空

```go
	// Get a list of reusable CPUs (e.g. CPUs reused from initContainers).
	// It should be an empty CPUSet for a newly created pod.
	reusable := p.cpusToReuse[string(pod.UID)]
```

- 产生TopologyHint最重要的函数 generateCPUTopologyHints

```go
	// Generate hints.
	cpuHints := p.generateCPUTopologyHints(available, reusable, requested)
	klog.InfoS("TopologyHints generated", "pod", klog.KObj(pod), "cpuHints", cpuHints)

```

#### generateCPUTopologyHints解读

- 初始值为k8s节点上所有NUMA节点的个数。

```go
	// Initialize minAffinitySize to include all NUMA Nodes.
	minAffinitySize := p.topology.CPUDetails.NUMANodes().Size()

```

- bitmask.IterateBitMasks这个函数用于将k8s节点上所有的NUMA节点求组合，然后通过回调函数处理这个组合。

```go
bitmask.IterateBitMasks(p.topology.CPUDetails.NUMANodes().ToSlice(), func(mask bitmask.BitMask) {
```

- IterateBitMasks内部实现类似冒泡排序，使用两层的for循环处理所有的可能性

```go
// IterateBitMasks iterates all possible masks from a list of bits,
// issuing a callback on each mask.
func IterateBitMasks(bits []int, callback func(BitMask)) {
	var iterate func(bits, accum []int, size int)
	iterate = func(bits, accum []int, size int) {
		if len(accum) == size {
			mask, _ := NewBitMask(accum...)
			callback(mask)
			return
		}
		for i := range bits {
			iterate(bits[i+1:], append(accum, bits[i]), size)
		}
	}

	for i := 1; i <= len(bits); i++ {
		iterate(bits, []int{}, i)
	}
}

```

- 回到外层的callback中，cpusInMask代表取出NUMA节点组合（以bitmask形式表示）中所涉及到的cpu
  - 如果NUMA节点组合中所涉及到的cpu个数比请求的cpu数大(满足容器的申请需求)，
  - 并且这个组合所涉及的NUMA节点个数是目前为止所有组合中最小的，那么就更新它。

```go
		// First, update minAffinitySize for the current request size.
		cpusInMask := p.topology.CPUDetails.CPUsInNUMANodes(mask.GetBits()...).Size()
		if cpusInMask >= request && mask.Count() < minAffinitySize {
			minAffinitySize = mask.Count()
		}

```

- 下面这两个for循环用户统计当前k8s节点可用的cpu中，有哪些是属于当前正在处理的NUMA节点组合

```go
		// Then check to see if we have enough CPUs available on the current
		// numa node bitmask to satisfy the CPU request.
		numMatching := 0
		for _, c := range reusableCPUs.ToSlice() {
			// Disregard this mask if its NUMANode isn't part of it.
			if !mask.IsSet(p.topology.CPUDetails[c].NUMANodeID) {
				return
			}
			numMatching++
		}

		// Finally, check to see if enough available CPUs remain on the current
		// NUMA node combination to satisfy the CPU request.
		for _, c := range availableCPUs.ToSlice() {
			if mask.IsSet(p.topology.CPUDetails[c].NUMANodeID) {
				numMatching++
			}
		}

```

- 如果当前组合中可用的cpu数比请求的cpu小，那么就直接返回，否则就创建一个TopologyHint，并把它加入到hints这个slice中

```go
		// If they don't, then move onto the next combination.
		if numMatching < request {
			return
		}

		// Otherwise, create a new hint from the numa node bitmask and add it to the
		// list of hints.  We set all hint preferences to 'false' on the first
		// pass through.
		hints = append(hints, topologymanager.TopologyHint{
			NUMANodeAffinity: mask,
			Preferred:        false,
		})
```

- 拿到所有的TopologyHint后，开始对TopologyHint标注“Preferred = true”
  - 如果所需的numa节点最少就标记Preferred =true

```go
	// Loop back through all hints and update the 'Preferred' field based on
	// counting the number of bits sets in the affinity mask and comparing it
	// to the minAffinitySize. Only those with an equal number of bits set (and
	// with a minimal set of numa nodes) will be considered preferred.
	for i := range hints {
		if hints[i].NUMANodeAffinity.Count() == minAffinitySize {
			hints[i].Preferred = true
		}
	}

```

> 以一张图来说明一下整个流程
> ![topology_gethit_cpu.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/908/1634434370000/285cf27fbb494c0f8d619a1b245db2d0.png)

- 图中有3个NUMA节点，每个节点有2个cpu
- 假设某个pod请求2个cpu以及已知当前k8s节点上空闲的cpu为 1,3,4,6，寻找TopologyHint过程如图：

### 同理我们可以看到Device Manager的GetTopologyHints实现

- 位置 D:\go_path\src\github.com\kubernetes\kubernetes\pkg\kubelet\cm\devicemanager\topology_hints.go
- 获取初始化minAffinitySize为k8s节点中NUMA节点个数

```go
	// Initialize minAffinitySize to include all NUMA Nodes
	minAffinitySize := len(m.numaNodes)
```

- 获取所有NUMA节点组合

```go
	// Iterate through all combinations of NUMA Nodes and build hints from them.
	hints := []topologymanager.TopologyHint{}
```

- devicesInMask用于统计该NUMA组合涉及到device个数
- 获取某种资源下的所有设备（比如获取gpu资源的所有GPU卡），并检查该device是否在当前NUMA组合中，如果在，devicesInMask值加1

```go
		// First, update minAffinitySize for the current request size.
		devicesInMask := 0
		for _, device := range m.allDevices[resource] {
			if mask.AnySet(m.getNUMANodeIds(device.Topology)) {
				devicesInMask++
			}
		}
```

- 如果当前NUMA组合涉及到的device数量比request大，并且当前NUMA组合中包含的NUMA个数比minAffinitySize还小，那么更新minAffinitySize的值。
  - 意思是满足pod资源申请的最小numa消耗

```go
		if devicesInMask >= request && mask.Count() < minAffinitySize {
			minAffinitySize = mask.Count()
		}

```

- numMatching用于获取当前NUMA组合中空闲的device数
  - 如果reusable中的device的NUMA节点ID不在当前这个NUMA组合中，那么直接返回
  - 不对这个NUMA组合创建TopologyHint，这样做的原因是保证reusable中的device优先被使用完

```go
		numMatching := 0
		for d := range reusable {
			// Skip the device if it doesn't specify any topology info.
			if m.allDevices[resource][d].Topology == nil {
				continue
			}
			// Otherwise disregard this mask if its NUMANode isn't part of it.
			if !mask.AnySet(m.getNUMANodeIds(m.allDevices[resource][d].Topology)) {
				return
			}
			numMatching++
		}

```

- 如果当前NUMA组合中可用的device比请求的device数还少，那么直接返回，往下就创建TopologyHint

```go
		// If they don't, then move onto the next combination.
		if numMatching < request {
			return
		}

		// Otherwise, create a new hint from the NUMA mask and add it to the
		// list of hints.  We set all hint preferences to 'false' on the first
		// pass through.
		hints = append(hints, topologymanager.TopologyHint{
			NUMANodeAffinity: mask,
			Preferred:        false,
		})
```

- 如果某个TopologyHint所涉及的NUMA数最少，那么将该TopologyHint的Preferred设置为true

```go
	// Loop back through all hints and update the 'Preferred' field based on
	// counting the number of bits sets in the affinity mask and comparing it
	// to the minAffinity. Only those with an equal number of bits set will be
	// considered preferred.
	for i := range hints {
		if hints[i].NUMANodeAffinity.Count() == minAffinitySize {
			hints[i].Preferred = true
		}
	}

```

> 稍微总结一下：

- 创建一个存放TopologyHint的数组，名称为hints。
- 根据k8s节点上所有的NUMA节点ID求所有的NUMA节点组合。
- 找出这些组合中涉及NUMA节点个数的最小值，将这个值设置为minAffinitySize。
- 对每个组合，检查当前k8s节点上某种资源（比如GPU）可用的设备数与该组合所涉及的该资源的设备数的交集的个数是否大于容器申请的设备数，如果比容器申请的设备数小，那么就不创建TopologyHint，否则就创建一个TopologyHint，并放入hints中。
- 检查hints中所有的TopologyHint，如果该TopologyHint涉及到的NUMA节点数与minAffinitySize值相同，那么将该TopologyHint的Preferred设置为true。

### TopologyHint的merge操作

- 前面已经说到了CPU Manager和Device Manager会产生多组TopologyHint
- 那么如何合并这些TopologyHint，找到最优的那个TopologyHint呢？来看看是怎样实现的。

> 以下面这幅图做说明
> ![topology_merge.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/908/1634434370000/e8d3315d7a224ee39a44bdeb095f701c.png)

- 在这幅图中总共有3个NUMA节点，对于某个容器而言
- CPU Manager找出了CPU资源的一组TopologyHint
- Device Manager找出了GPU和NIC的TopologyHint。整个merge流程如下：

#### 总结4中分配策略

> 前面提到过Topology Manager的四种策略，现在重点说一下四种策略中的后面三种：

- best-effort: 结合上图来说，如果没有找到最优的TopologyHint（即图中的TH6），k8s节点也会接纳这个Pod。
- restricted：结合上图来说，如果没有找到最优的TopologyHint（即图中的TH6），那么节点会拒绝接纳这个Pod，如果Pod遭到节点拒绝，其状态将变为Terminated。
- single-numa-node：结合上图来说，如果没有找到最优的TopologyHint（即图中的TH6，并且NUMA节点个数为1），那么节点会拒绝接纳这个Pod，如果Pod遭到节点拒绝，其状态将变为Terminated。

# 本节重点总结:

- TopologyManager是创建容器时的5大准入控制器之一
- 通过遍历cpu、device、memory三个资源管理器进行拓扑查询
  - 如果有报错证明 资源不能按照提议的拓扑分配，那么admit失败
- 以cpuManager和deviceManager为例解读了 generateCPUTopologyHints的过程
- cpu generateCPUTopologyHints的过程![topology_gethit_cpu.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/908/1634434370000/dbe13e558883417197a5381caf90e706.png)
- merge的流程![topology_merge.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/908/1634434370000/80d8dab2602f4a86b5d3a229ac28b817.png)