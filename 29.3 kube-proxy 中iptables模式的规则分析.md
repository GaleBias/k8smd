# 本机重点总结

- kube-proxy 组件负责维护 node 节点上的防火墙规则和路由规则，在 iptables 模式下，会根据 service 以及 endpoints 对象的改变来实时刷新规则
- kube-proxy 使用了 iptables 的 filter 表和 nat 表
- 自定义了 KUBE-SERVICES、KUBE-EXTERNAL-SERVICES、KUBE-NODEPORTS、KUBE-POSTROUTING、KUBE-MARK-MASQ、KUBE-MARK-DROP、KUBE-FORWARD 七条链
- 另外还新增了以“KUBE-SVC-xxx”和“KUBE-SEP-xxx”开头的数个链，除了创建自定义的链以外还将自定义链插入到已有链的后面以便劫持数据包。

> 工作示意图
> ![iptables01.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/908/1636076019000/9106160065e6417aa582562ed8d3b2c4.png)

# iptables功能简介

## iptables 的功能：

- 流量转发：DNAT 实现 IP 地址和端口的映射；
- 负载均衡：statistic 模块为每个后端设置权重；
- 会话保持：recent 模块设置会话保持时间；

## iptables 五条链分别对应为：

- PREROUTING 链：数据包进入路由之前，可以在此处进行 DNAT；
- INPUT 链：一般处理本地进程的数据包，目的地址为本机；
- FORWARD 链：一般处理转发到其他机器或者 network namespace 的数据包；
- OUTPUT 链：原地址为本机，向外发送，一般处理本地进程的输出数据包；
- POSTROUTING 链：发送到网卡之前，可以在此处进行 SNAT；

## iptables 五张表分别为：

> 这五张表是对 iptables 所有规则的逻辑集群且是有顺序的，当数据包到达某一条链时会按表的顺序进行处理，表的优先级为：raw、mangle、nat、filter、security

- filter 表：用于控制到达某条链上的数据包是继续放行、直接丢弃(drop)还是拒绝(reject)；
- nat 表：network address translation 网络地址转换，用于修改数据包的源地址和目的地址；
- mangle 表：用于修改数据包的 IP 头信息；
- raw 表：iptables 是有状态的，其对数据包有链接追踪机制，连接追踪信息在 /proc/net/nf_conntrack 中可以看到记录，而 raw 是用来去除链接追踪机制的；
- security 表：最不常用的表，用在 SELinux 上；

## 工作示意图

![iptables01.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/908/1636076019000/37b2c681680a4581bd6b3f8fdd3f46c6.png)

# kube-proxy 的 iptables 模式

- kube-proxy 组件负责维护 node 节点上的防火墙规则和路由规则，在 iptables 模式下，会根据 service 以及 endpoints 对象的改变来实时刷新规则
- kube-proxy 使用了 iptables 的 filter 表和 nat 表
- 自定义了 KUBE-SERVICES、KUBE-EXTERNAL-SERVICES、KUBE-NODEPORTS、KUBE-POSTROUTING、KUBE-MARK-MASQ、KUBE-MARK-DROP、KUBE-FORWARD 七条链
- 另外还新增了以“KUBE-SVC-xxx”和“KUBE-SEP-xxx”开头的数个链，除了创建自定义的链以外还将自定义链插入到已有链的后面以便劫持数据包。

## 示意图

![image](https://cdn.tianfeiyu.com/Center.png)

## 在 nat 表中自定义的链以及追加的链如下所示

```shell
Chain PREROUTING (policy ACCEPT)
target     prot opt source               destination     
cali-PREROUTING  all  --  0.0.0.0/0            0.0.0.0/0            /* cali:6gwbT8clXdHdC1b1 */
KUBE-SERVICES  all  --  0.0.0.0/0            0.0.0.0/0            /* kubernetes service portals */

Chain INPUT (policy ACCEPT)
target     prot opt source               destination     

Chain OUTPUT (policy ACCEPT)
target     prot opt source               destination     
cali-OUTPUT  all  --  0.0.0.0/0            0.0.0.0/0            /* cali:tVnHkvAo15HuiPy0 */
KUBE-SERVICES  all  --  0.0.0.0/0            0.0.0.0/0            /* kubernetes service portals */

Chain POSTROUTING (policy ACCEPT)
target     prot opt source               destination     
cali-POSTROUTING  all  --  0.0.0.0/0            0.0.0.0/0            /* cali:O3lYWMrLQYEMJtB5 */
KUBE-POSTROUTING  all  --  0.0.0.0/0            0.0.0.0/0            /* kubernetes postrouting rules */

Chain KUBE-KUBELET-CANARY (0 references)
target     prot opt source               destination     

Chain KUBE-MARK-DROP (0 references)
target     prot opt source               destination     
MARK       all  --  0.0.0.0/0            0.0.0.0/0            MARK or 0x8000

Chain KUBE-MARK-MASQ (48 references)
target     prot opt source               destination     
MARK       all  --  0.0.0.0/0            0.0.0.0/0            MARK or 0x4000

Chain KUBE-NODEPORTS (1 references)
target     prot opt source               destination     
KUBE-MARK-MASQ  tcp  --  0.0.0.0/0            0.0.0.0/0            /* default/grafana-node-port */ tcp dpt:30000
KUBE-SVC-JK7GGK64VHXRAVK6  tcp  --  0.0.0.0/0            0.0.0.0/0            /* default/grafana-node-port */ tcp dpt:30000
KUBE-MARK-MASQ  tcp  --  0.0.0.0/0            0.0.0.0/0            /* monitoring/grafana:http */ tcp dpt:3003
KUBE-SVC-AWA2CQSXVI7X2GE5  tcp  --  0.0.0.0/0            0.0.0.0/0            /* monitoring/grafana:http */ tcp dpt:3003
KUBE-MARK-MASQ  tcp  --  0.0.0.0/0            0.0.0.0/0            /* monitoring/prometheus-k8s:web */ tcp dpt:6090
KUBE-SVC-IFO32E4YIRUTZPGJ  tcp  --  0.0.0.0/0            0.0.0.0/0            /* monitoring/prometheus-k8s:web */ tcp dpt:6090
KUBE-MARK-MASQ  tcp  --  0.0.0.0/0            0.0.0.0/0            /* default/nginx-svc01 */ tcp dpt:8085
KUBE-SVC-NZ7QO3TOX75DUDMK  tcp  --  0.0.0.0/0            0.0.0.0/0            /* default/nginx-svc01 */ tcp dpt:8085

Chain KUBE-POSTROUTING (1 references)
target     prot opt source               destination     
RETURN     all  --  0.0.0.0/0            0.0.0.0/0            mark match ! 0x4000/0x4000
MARK       all  --  0.0.0.0/0            0.0.0.0/0            MARK xor 0x4000
MASQUERADE  all  --  0.0.0.0/0            0.0.0.0/0            /* kubernetes service traffic requiring SNAT */

Chain KUBE-PROXY-CANARY (0 references)
target     prot opt source               destination     

Chain KUBE-SEP-53JL7QCMGTF4KGQY (1 references)
target     prot opt source               destination     
KUBE-MARK-MASQ  all  --  10.100.85.238        0.0.0.0/0            /* default/nginx-svc01 */
DNAT       tcp  --  0.0.0.0/0            0.0.0.0/0            /* default/nginx-svc01 */ tcp to:10.100.85.238:80

Chain KUBE-SEP-7J3PXXC746AAT6A4 (1 references)
target     prot opt source               destination     
KUBE-MARK-MASQ  all  --  10.100.32.131        0.0.0.0/0            /* kube-system/kube-dns:dns */
DNAT       udp  --  0.0.0.0/0            0.0.0.0/0            /* kube-system/kube-dns:dns */ udp to:10.100.32.131:53

Chain KUBE-SEP-APPOXWYHNPQM3A5Z (1 references)
target     prot opt source               destination     
KUBE-MARK-MASQ  all  --  10.100.32.131        0.0.0.0/0            /* kube-system/kube-dns:dns-tcp */
DNAT       tcp  --  0.0.0.0/0            0.0.0.0/0            /* kube-system/kube-dns:dns-tcp */ tcp to:10.100.32.131:53

Chain KUBE-SEP-CQF75V4LX3XBIYEF (1 references)
target     prot opt source               destination     
KUBE-MARK-MASQ  all  --  10.100.85.207        0.0.0.0/0            /* monitoring/blackbox-exporter:https */
DNAT       tcp  --  0.0.0.0/0            0.0.0.0/0            /* monitoring/blackbox-exporter:https */ tcp to:10.100.85.207:9115
```

## 在 filter 表定义的链以及追加的链如下所示如下所示

```shell
Chain INPUT (policy ACCEPT)
target     prot opt source               destination     
cali-INPUT  all  --  0.0.0.0/0            0.0.0.0/0            /* cali:Cz_u1IQiXIMmKD4c */
KUBE-NODEPORTS  all  --  0.0.0.0/0            0.0.0.0/0            /* kubernetes health check service ports */
KUBE-EXTERNAL-SERVICES  all  --  0.0.0.0/0            0.0.0.0/0            ctstate NEW /* kubernetes externally-visible service portals */
KUBE-FIREWALL  all  --  0.0.0.0/0            0.0.0.0/0       

Chain FORWARD (policy ACCEPT)
target     prot opt source               destination     
cali-FORWARD  all  --  0.0.0.0/0            0.0.0.0/0            /* cali:wUHhoiAYhphO9Mso */
KUBE-FORWARD  all  --  0.0.0.0/0            0.0.0.0/0            /* kubernetes forwarding rules */
KUBE-SERVICES  all  --  0.0.0.0/0            0.0.0.0/0            ctstate NEW /* kubernetes service portals */
KUBE-EXTERNAL-SERVICES  all  --  0.0.0.0/0            0.0.0.0/0            ctstate NEW /* kubernetes externally-visible service portals */
ACCEPT     all  --  0.0.0.0/0            0.0.0.0/0            /* cali:S93hcgKJrXEqnTfs */ /* Policy explicitly accepted packet. */ mark match 0x10000/0x10000

Chain OUTPUT (policy ACCEPT)
target     prot opt source               destination     
cali-OUTPUT  all  --  0.0.0.0/0            0.0.0.0/0            /* cali:tVnHkvAo15HuiPy0 */
KUBE-SERVICES  all  --  0.0.0.0/0            0.0.0.0/0            ctstate NEW /* kubernetes service portals */
KUBE-FIREWALL  all  --  0.0.0.0/0            0.0.0.0/0       

Chain KUBE-EXTERNAL-SERVICES (2 references)
target     prot opt source               destination     

Chain KUBE-FIREWALL (2 references)
target     prot opt source               destination     
DROP       all  --  0.0.0.0/0            0.0.0.0/0            /* kubernetes firewall for dropping marked packets */ mark match 0x8000/0x8000
DROP       all  -- !127.0.0.0/8          127.0.0.0/8          /* block incoming localnet connections */ ! ctstate RELATED,ESTABLISHED,DNAT

Chain KUBE-FORWARD (1 references)
target     prot opt source               destination     
DROP       all  --  0.0.0.0/0            0.0.0.0/0            ctstate INVALID
ACCEPT     all  --  0.0.0.0/0            0.0.0.0/0            /* kubernetes forwarding rules */ mark match 0x4000/0x4000
ACCEPT     all  --  0.0.0.0/0            0.0.0.0/0            /* kubernetes forwarding conntrack pod source rule */ ctstate RELATED,ESTABLISHED
ACCEPT     all  --  0.0.0.0/0            0.0.0.0/0            /* kubernetes forwarding conntrack pod destination rule */ ctstate RELATED,ESTABLISHED

Chain KUBE-KUBELET-CANARY (0 references)
target     prot opt source               destination     

Chain KUBE-NODEPORTS (1 references)
target     prot opt source               destination     

Chain KUBE-PROXY-CANARY (0 references)
target     prot opt source               destination     

Chain KUBE-SERVICES (2 references)
target     prot opt source               destination     

Chain cali-FORWARD (1 references)
target     prot opt source               destination     
MARK       all  --  0.0.0.0/0            0.0.0.0/0            /* cali:vjrMJCRpqwy5oRoX */ MARK and 0xfff1ffff
cali-from-hep-forward  all  --  0.0.0.0/0            0.0.0.0/0            /* cali:A_sPAO0mcxbT9mOV */ mark match 0x0/0x10000
cali-from-wl-dispatch  all  --  0.0.0.0/0            0.0.0.0/0            /* cali:8ZoYfO5HKXWbB3pk */
cali-to-wl-dispatch  all  --  0.0.0.0/0            0.0.0.0/0            /* cali:jdEuaPBe14V2hutn */
cali-to-hep-forward  all  --  0.0.0.0/0            0.0.0.0/0            /* cali:12bc6HljsMKsmfr- */
cali-cidr-block  all  --  0.0.0.0/0            0.0.0.0/0            /* cali:NOSxoaGx8OIstr1z */

Chain cali-INPUT (1 references)
target     prot opt source               destination     
ACCEPT     udp  --  0.0.0.0/0            0.0.0.0/0            /* cali:w7ud0UgQSEi_zKuQ */ /* Allow VXLAN packets from whitelisted hosts */ multiport dports 4789 match-set cali40all-vxlan-net src ADDRTYPE match dst-type LOCAL
DROP       udp  --  0.0.0.0/0            0.0.0.0/0            /* cali:4cgmbdWsLmozYhJh */ /* Drop VXLAN packets from non-whitelisted hosts */ multiport dports 4789 ADDRTYPE match dst-type LOCAL
cali-wl-to-host  all  --  0.0.0.0/0            0.0.0.0/0           [goto]  /* cali:t45BUBhpu3Wsmi1_ */
ACCEPT     all  --  0.0.0.0/0            0.0.0.0/0            /* cali:NOmsycyknYZaGOFf */ mark match 0x10000/0x10000
MARK       all  --  0.0.0.0/0            0.0.0.0/0            /* cali:Or0B7eoenKO2p8Bf */ MARK and 0xfff0ffff
cali-from-host-endpoint  all  --  0.0.0.0/0            0.0.0.0/0            /* cali:AmIfvPGG2lYUK6mj */
ACCEPT     all  --  0.0.0.0/0            0.0.0.0/0            /* cali:79fWWn1SpufdO7SE */ /* Host endpoint policy accepted packet. */ mark match 0x10000/0x10000

```

## clusterIP 访问方式 iptables 规则分析

### 创建一个dep和clusterip类型的svc

- yaml如下

```yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment-svc05
  labels:
    app: nginx-svc05
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nginx-svc05
  template:
    metadata:
      labels:
        app: nginx-svc05
    spec:
      containers:
      - name: nginx
        image: nginx:1.8
        ports:
        - containerPort: 80
--- 
apiVersion: v1
kind: Service
metadata:
  name: nginx-svc05
spec:
  selector:
    app: nginx-svc05 # 选择我们上面创建dep中的nginx pod，
  ports:
    - protocol: TCP
      port: 8085  # service对外暴露的端口
      targetPort: 80 # 代表目标容器的端口是80
```

- 获取svc的详情 ，可以看到nginx-svc05对应的cluster-ip为10.96.112.4

```shell
[root@k8s-master01 k8s_svc]# kubectl get svc nginx-svc05
NAME          TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)    AGE
nginx-svc05   ClusterIP   10.96.112.4   <none>        8085/TCP   23s
```

### 报错iptables规则，在规则中查询

```shell
[root@k8s-node01 ~]# iptables-save  > ipt                
[root@k8s-node01 ~]# wc -l ipt 
918 ipt
```

### 从 pod 中访问 clusterIP 的 iptables 规则流向为

```shell
PREROUTING --> KUBE-SERVICE --> KUBE-SVC-XXX --> KUBE-SEP-XXX

```

- 分析如下
- 01 对于进入 PREROUTING 链的都转到 KUBE-SERVICES 链进行处理；

```shell
[root@k8s-node01 ~]# grep KUBE-SERVICES ipt 
-A PREROUTING -m comment --comment "kubernetes service portals" -j KUBE-SERVICES
```

- 02 在 KUBE-SERVICES 链，对于访问 clusterIP 为10.96.112.4 的转发到 KUBE-SVC-WZ2BDG775GVNWW7K；

```shell
grep 10.96.112.4 ipt
-A KUBE-SERVICES -d 10.96.112.4/32 -p tcp -m comment --comment "default/nginx-svc05 cluster IP" -m tcp --dport 8085 -j KUBE-SVC-WZ2BDG775GVNWW7K
```

- 03 访问 KUBE-SVC-WZ2BDG775GVNWW7K 的使用随机数负载均衡，并转发到 KUBE-SEP-R5AT6OISESZ7ICST 和  KUBE-SEP-7IRIKX5EUKHDJQVS 上；

```shell
[root@k8s-node01 ~]# grep KUBE-SVC-WZ2BDG775GVNWW7K ipt 
:KUBE-SVC-WZ2BDG775GVNWW7K - [0:0]
-A KUBE-SVC-WZ2BDG775GVNWW7K -m comment --comment "default/nginx-svc05" -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-R5AT6OISESZ7ICST
-A KUBE-SVC-WZ2BDG775GVNWW7K -m comment --comment "default/nginx-svc05" -j KUBE-SEP-7IRIKX5EUKHDJQVS
```

- 04 KUBE-SEP-CI5ZO3FTK7KBNRMG 和 KUBE-SEP-OVNLTDWFHTHII4SC 对应 endpoint 中的 pod 10.100.85.241 和 10.100.85.242，设置 mark 标记，进行 DNAT 并转发到具体的 pod 上，如果某个 service 的 endpoints 中没有 pod，那么针对此 service 的请求将会被 drop 掉；

```shell
[root@k8s-node01 ~]# grep  KUBE-SEP-R5AT6OISESZ7ICST ipt 
-A KUBE-SEP-R5AT6OISESZ7ICST -s 10.100.85.241/32 -m comment --comment "default/nginx-svc05" -j KUBE-MARK-MASQ
-A KUBE-SEP-R5AT6OISESZ7ICST -p tcp -m comment --comment "default/nginx-svc05" -m tcp -j DNAT --to-destination 10.100.85.241:80


[root@k8s-node01 ~]# grep  KUBE-SEP-7IRIKX5EUKHDJQVS ipt 
-A KUBE-SEP-7IRIKX5EUKHDJQVS -s 10.100.85.242/32 -m comment --comment "default/nginx-svc05" -j KUBE-MARK-MASQ
-A KUBE-SEP-7IRIKX5EUKHDJQVS -p tcp -m comment --comment "default/nginx-svc05" -m tcp -j DNAT --to-destination 10.100.85.242:80

# 获取 ep 和pod
[root@k8s-master01 k8s_svc]# kubectl get pod -o wide 
NAME                                       READY   STATUS    RESTARTS   AGE     IP              NODE         NOMINATED NODE   READINESS GATES
nginx-deployment-svc05-68684b78bd-dk4wt    1/1     Running   0          15m     10.100.85.242   k8s-node01   <none>           <none>
nginx-deployment-svc05-68684b78bd-k2s8b    1/1     Running   0          15m     10.100.85.241   k8s-node01   <none>           <none>
[root@k8s-master01 k8s_svc]# kubectl get ep
NAME                                          ENDPOINTS                                            AGE
nginx-svc05                                   10.100.85.241:80,10.100.85.242:80                    15m
[root@k8s-master01 k8s_svc]# 
```

## nodePort 方式 iptables 规则分析

- 在 nodePort 方式下，会用到 KUBE-NODEPORTS 规则链
- 通过 iptables -t nat -L -n 可以看到 KUBE-NODEPORTS 位于 KUBE-SERVICE 链的最后一个
- iptables 在处理报文时会优先处理目的 IP 为clusterIP 的报文，在前面的 KUBE-SVC-XXX 都匹配失败之后再去使用 nodePort 方式进行匹配。

### 创建一个dep和clusterip类型的svc

- yaml如下

```yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment-svc06
  labels:
    app: nginx-svc06
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nginx-svc06
  template:
    metadata:
      labels:
        app: nginx-svc06
    spec:
      containers:
      - name: nginx
        image: nginx:1.8
        ports:
        - containerPort: 80
--- 
apiVersion: v1
kind: Service
metadata:
  name: nginx-svc06
spec:
  selector:
    app: nginx-svc06 # 选择我们上面创建dep中的nginx pod，
  type: NodePort
  ports:
    - protocol: TCP
      port: 8085  # service对外暴露的端口
      targetPort: 80 # 代表目标容器的端口是80
      nodePort: 8066
```

- 获取svc的详情 ，可以看到nginx-svc06对应的cluster-ip为10.96.214.73

```shell
[root@k8s-master01 k8s_svc]# kubectl get svc -o wide 
NAME                TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)         AGE    SELECTOR
nginx-svc06         NodePort    10.96.214.73    <none>        8085:8066/TCP   65s    app=nginx-svc06
```

### 非本机访问 的链路

```shell
PREROUTING --> KUBE-SERVICE --> KUBE-NODEPORTS --> KUBE-SVC-XXX --> KUBE-SEP-XXX

```

### 本机访问

```shell
OUTPUT --> KUBE-SERVICE --> KUBE-NODEPORTS --> KUBE-SVC-XXX --> KUBE-SEP-XXX

```

### 具体分析

- 01 经过 PREROUTING 转到 KUBE-SERVICES

```shell
-A PREROUTING -m comment --comment "kubernetes service portals" -j KUBE-SERVICES
```

- 02 经过 KUBE-SERVICES 转到 KUBE-NODEPORTS

```shell
-A KUBE-SERVICES -m comment --comment "kubernetes service nodeports; NOTE: this must be the last rule in this chain" -m addrtype --dst-type LOCAL -j KUBE-NODEPORTS
```

- 03 经过 KUBE-NODEPORTS 转到 KUBE-SVC-XUMDHN64OUAEQNNK

```shell
[root@k8s-node01 ~]# grep svc06  ipt  |grep -i node
-A KUBE-NODEPORTS -p tcp -m comment --comment "default/nginx-svc06" -m tcp --dport 8066 -j KUBE-MARK-MASQ
-A KUBE-NODEPORTS -p tcp -m comment --comment "default/nginx-svc06" -m tcp --dport 8066 -j KUBE-SVC-XUMDHN64OUAEQNNK
```

- 04 经过 KUBE-SVC-XUMDHN64OUAEQNNK 转到 KUBE-SEP-5TEVKMH5AQRGQEGF 和 KUBE-SEP-PR3EE4KNPHRMIAIE

```shell
[root@k8s-node01 ~]# grep KUBE-SVC-XUMDHN64OUAEQNNK  ipt  
-A KUBE-SVC-XUMDHN64OUAEQNNK -m comment --comment "default/nginx-svc06" -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-5TEVKMH5AQRGQEGF
-A KUBE-SVC-XUMDHN64OUAEQNNK -m comment --comment "default/nginx-svc06" -j KUBE-SEP-PR3EE4KNPHRMIAIE
```

- 05 经过 KUBE-SEP-PR3EE4KNPHRMIAIE 和 KUBE-SEP-5TEVKMH5AQRGQEGF 分别转到 10.100.85.243:80 和 10.100.85.244:80

```shell
[root@k8s-node01 ~]# grep KUBE-SEP-5TEVKMH5AQRGQEGF ipt 

-A KUBE-SEP-5TEVKMH5AQRGQEGF -p tcp -m comment --comment "default/nginx-svc06" -m tcp -j DNAT --to-destination 10.100.85.243:80
-A KUBE-SEP-PR3EE4KNPHRMIAIE -p tcp -m comment --comment "default/nginx-svc06" -m tcp -j DNAT --to-destination 10.100.85.244:80

[root@k8s-master01 k8s_svc]# kubectl get pod -o wide   |grep  svc06  
nginx-deployment-svc06-6bd95799cf-4hx5h    1/1     Running   0          9m50s   10.100.85.244   k8s-node01   <none>           <none>
nginx-deployment-svc06-6bd95799cf-ljp64    1/1     Running   0          9m50s   10.100.85.243   k8s-node01   <none>           <none>

```

# 本机重点总结

- kube-proxy 组件负责维护 node 节点上的防火墙规则和路由规则，在 iptables 模式下，会根据 service 以及 endpoints 对象的改变来实时刷新规则
- kube-proxy 使用了 iptables 的 filter 表和 nat 表
- 自定义了 KUBE-SERVICES、KUBE-EXTERNAL-SERVICES、KUBE-NODEPORTS、KUBE-POSTROUTING、KUBE-MARK-MASQ、KUBE-MARK-DROP、KUBE-FORWARD 七条链
- 另外还新增了以“KUBE-SVC-xxx”和“KUBE-SEP-xxx”开头的数个链，除了创建自定义的链以外还将自定义链插入到已有链的后面以便劫持数据包。