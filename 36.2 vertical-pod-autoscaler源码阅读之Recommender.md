# 本节重点总结

> 初始化阶段

- recommender启动的时候会根据不同的存储后端 把checkpoints数据加载到 clusterState中
  - useCheckpoints代表从etcd获取 遍历ns 调用feeder 通过restapi获取VerticalPodAutoscalerCheckpoints，然后通过feeder把checkpoint塞入clusterState中
  - 如果是prometheus型的 从prometheus查询8天的历史数据，追踪后发现就是调用的prometheus 的QueryRange接口，拼接的查询promql 如下
    - cpu 型`rate(container_cpu_usage_seconds_total{job="kubernetes-nodes-cadvisor", pod!="POD", pod!="",container="xxx"}[30s])`
    - mem 型`container_memory_working_set_bytes{job="kubernetes-nodes-cadvisor", pod!="POD", pod!="",container="xxx"}`

> 运行依赖于 recommender.RunOnce

- 01 首先更新vpa的状态到clusterState中
- 02 LoadPods 更新pod状态
- 03  LoadRealTimeMetrics 获取容器当前的资源监控情况更新 clusterState
  - 这里看到的就是从 apiserver的聚合插件请求后端 metrics-server的数据
- 04 UpdateVPAs 利用上面计算好的cpu内存值 给出推荐值
  - 整体的步骤就是 遍历要更新的vpa，调用GetRecommendedPodResources生成 cpu和mem的推荐值，然后更新vpa的Recommended字段，交给后面的updater组件处理即可
- 05  如果使用的是 checkpoint的模式，代表历史数据的存储依赖etcd，那么在计算后调用StoreCheckpoints将结果存入etcd中

# Recommender：

> 架构图
> ![vpaarchitecture20200901140235706.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/908/1636076380000/614ba47c497a49a8add2a1dfef34da66.png)

- 用于根据监控指标结合内置机制给出资源建议值

> Recommender 具体做什么

- 监听vpa和对应的pod，获取pod的资源，计算并更新vpa的recommend值

# 源码入口

- 位置 D:\go_path\src\github.com\kubernetes\autoscaler\vertical-pod-autoscaler\pkg\recommender\main.go

## 解读

- 首先初始化kube-config

```go
func main() {
	klog.InitFlags(nil)
	kube_flag.InitFlags()
	klog.V(1).Infof("Vertical Pod Autoscaler %s Recommender", common.VerticalPodAutoscalerVersion)

	config := common.CreateKubeConfigOrDie(*kubeconfig, float32(*kubeApiQps), int(*kubeApiBurst))

```

- 设定监控指标聚合的配置

```go
	model.InitializeAggregationsConfig(model.NewAggregationsConfig(*memoryAggregationInterval, *memoryAggregationIntervalCount, *memoryHistogramDecayHalfLife, *cpuHistogramDecayHalfLife))
```

- 添加监控检测的 handler和注册相关的metrics

```go

	healthCheck := metrics.NewHealthCheck(*metricsFetcherInterval*5, true)
	metrics.Initialize(*address, healthCheck)
	metrics_recommender.Register()
	metrics_quality.Register()
```

- 监控数据的提供方，prometheus或者Checkpoint，Checkpoint这里代表recommend本地内存的数据，不断的读取metrics-server的指标缓存到本地

```go
	useCheckpoints := *storage != "prometheus"
```

#### 初始化recommender对象，

- 这里要对传参进行解读

  - config代表 kubeconfig 的配置
  - checkpointsGCInterval 代表Checkpoint存储metrics 执行gc的间隔
  - useCheckpoints上面解释过了
  - vpaObjectNamespace代表监控的vpa的ns，默认空也就是所有ns的vpa
- 初始化代码如下 ，可以看到其中初始化了很多对象，我们来分析一下

```go
	recommender := routines.NewRecommender(config, *checkpointsGCInterval, useCheckpoints, *vpaObjectNamespace)

// NewRecommender creates a new recommender instance.
// Dependencies are created automatically.
// Deprecated; use RecommenderFactory instead.
func NewRecommender(config *rest.Config, checkpointsGCInterval time.Duration, useCheckpoints bool, namespace string) Recommender {
	clusterState := model.NewClusterState(AggregateContainerStateGCInterval)
	return RecommenderFactory{
		ClusterState:           clusterState,
		ClusterStateFeeder:     input.NewClusterStateFeeder(config, clusterState, *memorySaver, namespace),
		CheckpointWriter:       checkpoint.NewCheckpointWriter(clusterState, vpa_clientset.NewForConfigOrDie(config).AutoscalingV1()),
		VpaClient:              vpa_clientset.NewForConfigOrDie(config).AutoscalingV1(),
		PodResourceRecommender: logic.CreatePodResourceRecommender(),
		CheckpointsGCInterval:  checkpointsGCInterval,
		UseCheckpoints:         useCheckpoints,
	}.Make()
}

```

##### 01 clusterState 是一个持有 vpa对象的核心资源，具体字段解释如下

```go
clusterState := model.NewClusterState(AggregateContainerStateGCInterval)
// NewClusterState returns a new ClusterState with no pods.
func NewClusterState(gcInterval time.Duration) *ClusterState {
	return &ClusterState{
		Pods:                          make(map[PodID]*PodState),
		Vpas:                          make(map[VpaID]*Vpa),
		EmptyVPAs:                     make(map[VpaID]time.Time),
		aggregateStateMap:             make(aggregateContainerStatesMap),
		labelSetMap:                   make(labelSetMap),
		lastAggregateContainerStateGC: time.Unix(0, 0),
		gcInterval:                    gcInterval,
	}
}
type ClusterState struct {
	// Pods in the cluster.
	Pods map[PodID]*PodState // 对应pod的 运行信息，包括对应的容器和状态
	// VPA objects in the cluster.
	Vpas map[VpaID]*Vpa  // 代表vpa的map
	// VPA objects in the cluster that have no recommendation mapped to the first
	// time we've noticed the recommendation missing or last time we logged
	// a warning about it.
	EmptyVPAs map[VpaID]time.Time // 代表没有 recommendation的vpa，记录他们的创建时间
	// Observed VPAs. Used to check if there are updates needed.
	ObservedVpas []*vpa_types.VerticalPodAutoscaler  // 后续要更新的vpa

	// All container aggregations where the usage samples are stored.
	aggregateStateMap aggregateContainerStatesMap  // 指标情况
	// Map with all label sets used by the aggregations. It serves as a cache
	// that allows to quickly access labels.Set corresponding to a labelSetKey.
	labelSetMap labelSetMap  // 标签

	lastAggregateContainerStateGC time.Time
	gcInterval                    time.Duration
}

```

##### 02 ClusterStateFeeder 对象使用来更新ClusterState的，new代码如下

```go
ClusterStateFeeder:     input.NewClusterStateFeeder(config, clusterState, *memorySaver, namespace),
// NewClusterStateFeeder creates new ClusterStateFeeder with internal data providers, based on kube client config.
// Deprecated; Use ClusterStateFeederFactory instead.
func NewClusterStateFeeder(config *rest.Config, clusterState *model.ClusterState, memorySave bool, namespace string) ClusterStateFeeder {
	kubeClient := kube_client.NewForConfigOrDie(config)
	podLister, oomObserver := NewPodListerAndOOMObserver(kubeClient, namespace)
	factory := informers.NewSharedInformerFactoryWithOptions(kubeClient, defaultResyncPeriod, informers.WithNamespace(namespace))
	controllerFetcher := controllerfetcher.NewControllerFetcher(config, kubeClient, factory, scaleCacheEntryFreshnessTime, scaleCacheEntryLifetime, scaleCacheEntryJitterFactor)
	controllerFetcher.Start(context.TODO(), scaleCacheLoopPeriod)
	return ClusterStateFeederFactory{
		PodLister:           podLister,
		OOMObserver:         oomObserver,
		KubeClient:          kubeClient,
		MetricsClient:       newMetricsClient(config, namespace),
		VpaCheckpointClient: vpa_clientset.NewForConfigOrDie(config).AutoscalingV1(),
		VpaLister:           vpa_api_util.NewVpasLister(vpa_clientset.NewForConfigOrDie(config), make(chan struct{}), namespace),
		ClusterState:        clusterState,
		SelectorFetcher:     target.NewVpaTargetSelectorFetcher(config, kubeClient, factory),
		MemorySaveMode:      memorySave,
		ControllerFetcher:   controllerFetcher,
	}.Make()
}
```

- 同时在NewClusterStateFeeder中又会创建并启动vpa所监控的 scale对象的informer，在这里我们可以看到的有
  - DaemonSets
  - deployment
  - replicaSet
  - statefulSet
  - replicationController
  - job
  - cronJob
- controllerFetcher 从它的接口代码可以看到是返回一个获取最近的可以scale的控制器，比如dep对应的rs ，代码如下，

```go
type ControllerFetcher interface {
	// FindTopMostWellKnownOrScalable returns topmost well-known or scalable controller. Error is returned if controller cannot be found.
	FindTopMostWellKnownOrScalable(controller *ControllerKeyWithAPIVersion) (*ControllerKeyWithAPIVersion, error)
}



controllerFetcher := controllerfetcher.NewControllerFetcher(config, kubeClient, factory, scaleCacheEntryFreshnessTime, scaleCacheEntryLifetime, scaleCacheEntryJitterFactor)

// NewControllerFetcher returns a new instance of controllerFetcher
func NewControllerFetcher(config *rest.Config, kubeClient kube_client.Interface, factory informers.SharedInformerFactory, betweenRefreshes, lifeTime time.Duration, jitterFactor float64) *controllerFetcher {
	discoveryClient, err := discovery.NewDiscoveryClientForConfig(config)
	if err != nil {
		klog.Fatalf("Could not create discoveryClient: %v", err)
	}
	resolver := scale.NewDiscoveryScaleKindResolver(discoveryClient)
	restClient := kubeClient.CoreV1().RESTClient()
	cachedDiscoveryClient := cacheddiscovery.NewMemCacheClient(discoveryClient)
	mapper := restmapper.NewDeferredDiscoveryRESTMapper(cachedDiscoveryClient)
	go wait.Until(func() {
		mapper.Reset()
	}, discoveryResetPeriod, make(chan struct{}))

	informersMap := map[wellKnownController]cache.SharedIndexInformer{
		daemonSet:             factory.Apps().V1().DaemonSets().Informer(),
		deployment:            factory.Apps().V1().Deployments().Informer(),
		replicaSet:            factory.Apps().V1().ReplicaSets().Informer(),
		statefulSet:           factory.Apps().V1().StatefulSets().Informer(),
		replicationController: factory.Core().V1().ReplicationControllers().Informer(),
		job:                   factory.Batch().V1().Jobs().Informer(),
		cronJob:               factory.Batch().V1beta1().CronJobs().Informer(),
	}

	for kind, informer := range informersMap {
		stopCh := make(chan struct{})
		go informer.Run(stopCh)
		synced := cache.WaitForCacheSync(stopCh, informer.HasSynced)
		if !synced {
			klog.Warningf("Could not sync cache for %s: %v", kind, err)
		} else {
			klog.Infof("Initial sync of %s completed", kind)
		}
	}

	scaleNamespacer := scale.New(restClient, mapper, dynamic.LegacyAPIPathResolverFunc, resolver)
	return &controllerFetcher{
		scaleNamespacer:              scaleNamespacer,
		mapper:                       mapper,
		informersMap:                 informersMap,
		scaleSubresourceCacheStorage: newControllerCacheStorage(betweenRefreshes, lifeTime, jitterFactor),
	}
}

```

- 然后开启对这些informer的同步，在periodicallyRefreshCache可以看到主要流程就是遍历这些 对象，拿到他们的scale 对象，调用Refresh更新缓存，最终会更新到controllerCacheStorage的cache map中

```go
	controllerFetcher.Start(context.TODO(), scaleCacheLoopPeriod)
func (f *controllerFetcher) Start(ctx context.Context, loopPeriod time.Duration) {
	go f.periodicallyRefreshCache(ctx, loopPeriod)
}
func (f *controllerFetcher) periodicallyRefreshCache(ctx context.Context, period time.Duration) {
	for {
		select {
		case <-ctx.Done():
			return
		case <-time.After(period):
			keysToRefresh := f.scaleSubresourceCacheStorage.GetKeysToRefresh()
			klog.V(5).Info("Starting to refresh entries in controllerFetchers scaleSubresourceCacheStorage")
			for _, item := range keysToRefresh {
				scale, err := f.scaleNamespacer.Scales(item.namespace).Get(context.TODO(), item.groupResource, item.name, metav1.GetOptions{})
				f.scaleSubresourceCacheStorage.Refresh(item.namespace, item.groupResource, item.name, scale, err)
			}
			klog.V(5).Infof("Finished refreshing %d entries in controllerFetchers scaleSubresourceCacheStorage", len(keysToRefresh))
			f.scaleSubresourceCacheStorage.RemoveExpired()
		}
	}
}

func (cc *controllerCacheStorage) Refresh(namespace string, groupResource schema.GroupResource, name string, controller *autoscalingapi.Scale, err error) {
	key := scaleCacheKey{namespace: namespace, groupResource: groupResource, name: name}
	cc.mux.Lock()
	defer cc.mux.Unlock()
	old, ok := cc.cache[key]
	if !ok {
		return
	}
	// We refresh entries that are waiting to be removed. So when we refresh an
	// entry we mustn't change entries deleteAfter time (otherwise we risk never
	// removing entries that are not being read).
	cc.cache[key] = scaleCacheEntry{
		refreshAfter: now().Add(wait.Jitter(cc.validityTime, cc.jitterFactor)),
		deleteAfter:  old.deleteAfter,
		resource:     controller,
		err:          err,
	}
}
```

- 然后是初始化vpa 的informer并启动，最后设置一个vpaLister对象，我们知道 xxLister代表的都是informer的本地缓存

```go
VpaLister:           vpa_api_util.NewVpasLister(vpa_clientset.NewForConfigOrDie(config), make(chan struct{}), namespace),
func NewVpasLister(vpaClient *vpa_clientset.Clientset, stopChannel <-chan struct{}, namespace string) vpa_lister.VerticalPodAutoscalerLister {
	vpaListWatch := cache.NewListWatchFromClient(vpaClient.AutoscalingV1().RESTClient(), "verticalpodautoscalers", namespace, fields.Everything())
	indexer, controller := cache.NewIndexerInformer(vpaListWatch,
		&vpa_types.VerticalPodAutoscaler{},
		1*time.Hour,
		&cache.ResourceEventHandlerFuncs{},
		cache.Indexers{cache.NamespaceIndex: cache.MetaNamespaceIndexFunc})
	vpaLister := vpa_lister.NewVerticalPodAutoscalerLister(indexer)
	go controller.Run(stopChannel)
	if !cache.WaitForCacheSync(make(chan struct{}), controller.HasSynced) {
		klog.Fatalf("Failed to sync VPA cache during initialization")
	} else {
		klog.Info("Initial VPA synced successfully")
	}
	return vpaLister
}

```

- NewClusterStateFeeder中还会初始化一个SelectorFetcher对象
- 从它的接口可以看到一个Fetch方法，用来获取 vpa管理的pod的标签
- 它内部的初始化和上面的controllerFetcher很像

```go
SelectorFetcher:     target.NewVpaTargetSelectorFetcher(config, kubeClient, factory),
// VpaTargetSelectorFetcher gets a labelSelector used to gather Pods controlled by the given VPA.
type VpaTargetSelectorFetcher interface {
	// Fetch returns a labelSelector used to gather Pods controlled by the given VPA.
	// If error is nil, the returned labelSelector is not nil.
	Fetch(vpa *vpa_types.VerticalPodAutoscaler) (labels.Selector, error)
}
// NewVpaTargetSelectorFetcher returns new instance of VpaTargetSelectorFetcher
func NewVpaTargetSelectorFetcher(config *rest.Config, kubeClient kube_client.Interface, factory informers.SharedInformerFactory) VpaTargetSelectorFetcher {
	discoveryClient, err := discovery.NewDiscoveryClientForConfig(config)
	if err != nil {
		klog.Fatalf("Could not create discoveryClient: %v", err)
	}
	resolver := scale.NewDiscoveryScaleKindResolver(discoveryClient)
	restClient := kubeClient.CoreV1().RESTClient()
	cachedDiscoveryClient := cacheddiscovery.NewMemCacheClient(discoveryClient)
	mapper := restmapper.NewDeferredDiscoveryRESTMapper(cachedDiscoveryClient)
	go wait.Until(func() {
		mapper.Reset()
	}, discoveryResetPeriod, make(chan struct{}))

	informersMap := map[wellKnownController]cache.SharedIndexInformer{
		daemonSet:             factory.Apps().V1().DaemonSets().Informer(),
		deployment:            factory.Apps().V1().Deployments().Informer(),
		replicaSet:            factory.Apps().V1().ReplicaSets().Informer(),
		statefulSet:           factory.Apps().V1().StatefulSets().Informer(),
		replicationController: factory.Core().V1().ReplicationControllers().Informer(),
		job:                   factory.Batch().V1().Jobs().Informer(),
		cronJob:               factory.Batch().V1beta1().CronJobs().Informer(),
	}

	for kind, informer := range informersMap {
		stopCh := make(chan struct{})
		go informer.Run(stopCh)
		synced := cache.WaitForCacheSync(stopCh, informer.HasSynced)
		if !synced {
			klog.Fatalf("Could not sync cache for %s: %v", kind, err)
		} else {
			klog.Infof("Initial sync of %s completed", kind)
		}
	}

	scaleNamespacer := scale.New(restClient, mapper, dynamic.LegacyAPIPathResolverFunc, resolver)
	return &vpaTargetSelectorFetcher{
		scaleNamespacer: scaleNamespacer,
		mapper:          mapper,
		informersMap:    informersMap,
	}
}

```

##### 03 PodResourceRecommender 对象用来计算 pod的 recommend值

- 代码如下，有一个GetRecommendedPodResources方法用来计算值的，这个我们后面看

```go
PodResourceRecommender: logic.CreatePodResourceRecommender(),
// PodResourceRecommender computes resource recommendation for a Vpa object.
type PodResourceRecommender interface {
	GetRecommendedPodResources(containerNameToAggregateStateMap model.ContainerNameToAggregateStateMap) RecommendedPodResources
}

// CreatePodResourceRecommender returns the primary recommender.
func CreatePodResourceRecommender() PodResourceRecommender {
	targetCPUPercentile := 0.9
	lowerBoundCPUPercentile := 0.5
	upperBoundCPUPercentile := 0.95

	targetMemoryPeaksPercentile := 0.9
	lowerBoundMemoryPeaksPercentile := 0.5
	upperBoundMemoryPeaksPercentile := 0.95

	targetEstimator := NewPercentileEstimator(targetCPUPercentile, targetMemoryPeaksPercentile)
	lowerBoundEstimator := NewPercentileEstimator(lowerBoundCPUPercentile, lowerBoundMemoryPeaksPercentile)
	upperBoundEstimator := NewPercentileEstimator(upperBoundCPUPercentile, upperBoundMemoryPeaksPercentile)

	targetEstimator = WithMargin(*safetyMarginFraction, targetEstimator)
	lowerBoundEstimator = WithMargin(*safetyMarginFraction, lowerBoundEstimator)
	upperBoundEstimator = WithMargin(*safetyMarginFraction, upperBoundEstimator)

	// Apply confidence multiplier to the upper bound estimator. This means
	// that the updater will be less eager to evict pods with short history
	// in order to reclaim unused resources.
	// Using the confidence multiplier 1 with exponent +1 means that
	// the upper bound is multiplied by (1 + 1/history-length-in-days).
	// See estimator.go to see how the history length and the confidence
	// multiplier are determined. The formula yields the following multipliers:
	// No history     : *INF  (do not force pod eviction)
	// 12h history    : *3    (force pod eviction if the request is > 3 * upper bound)
	// 24h history    : *2
	// 1 week history : *1.14
	upperBoundEstimator = WithConfidenceMultiplier(1.0, 1.0, upperBoundEstimator)

	// Apply confidence multiplier to the lower bound estimator. This means
	// that the updater will be less eager to evict pods with short history
	// in order to provision them with more resources.
	// Using the confidence multiplier 0.001 with exponent -2 means that
	// the lower bound is multiplied by the factor (1 + 0.001/history-length-in-days)^-2
	// (which is very rapidly converging to 1.0).
	// See estimator.go to see how the history length and the confidence
	// multiplier are determined. The formula yields the following multipliers:
	// No history   : *0   (do not force pod eviction)
	// 5m history   : *0.6 (force pod eviction if the request is < 0.6 * lower bound)
	// 30m history  : *0.9
	// 60m history  : *0.95
	lowerBoundEstimator = WithConfidenceMultiplier(0.001, -2.0, lowerBoundEstimator)

	return &podResourceRecommender{
		targetEstimator,
		lowerBoundEstimator,
		upperBoundEstimator}
}

```

#### 把checkpoints数据加载到 clusterState中

- 解析prometheus 查询的超时时间

```go
	promQueryTimeout, err := time.ParseDuration(*queryTimeout)
	if err != nil {
		klog.Fatalf("Could not parse --prometheus-query-timeout as a time.Duration: %v", err)
	}
```

- 然后是把checkpoints数据加载到 clusterState中，这里可以看到根据不同的后端有不同的加载方式

```go
	if useCheckpoints {
		recommender.GetClusterStateFeeder().InitFromCheckpoints()
	} else {
		config := history.PrometheusHistoryProviderConfig{
			Address:                *prometheusAddress,
			QueryTimeout:           promQueryTimeout,
			HistoryLength:          *historyLength,
			HistoryResolution:      *historyResolution,
			PodLabelPrefix:         *podLabelPrefix,
			PodLabelsMetricName:    *podLabelsMetricName,
			PodNamespaceLabel:      *podNamespaceLabel,
			PodNameLabel:           *podNameLabel,
			CtrNamespaceLabel:      *ctrNamespaceLabel,
			CtrPodNameLabel:        *ctrPodNameLabel,
			CtrNameLabel:           *ctrNameLabel,
			CadvisorMetricsJobName: *prometheusJobName,
			Namespace:              *vpaObjectNamespace,
		}
		provider, err := history.NewPrometheusHistoryProvider(config)
		if err != nil {
			klog.Fatalf("Could not initialize history provider: %v", err)
		}
		recommender.GetClusterStateFeeder().InitFromHistoryProvider(provider)
	}
```

##### useCheckpoints代表从本地加载

- 首先会遍历clusterState.Vpa 对象获取 所有配置了vpa的ns
- 然后遍历ns 调用feeder 通过restapi获取VerticalPodAutoscalerCheckpoints，然后通过feeder把checkpoint塞入clusterState中

```go
func (feeder *clusterStateFeeder) InitFromCheckpoints() {
	klog.V(3).Info("Initializing VPA from checkpoints")
	feeder.LoadVPAs()

	namespaces := make(map[string]bool)
	for _, v := range feeder.clusterState.Vpas {
		namespaces[v.ID.Namespace] = true
	}

	for namespace := range namespaces {
		klog.V(3).Infof("Fetching checkpoints from namespace %s", namespace)
		checkpointList, err := feeder.vpaCheckpointClient.VerticalPodAutoscalerCheckpoints(namespace).List(context.TODO(), metav1.ListOptions{})
		if err != nil {
			klog.Errorf("Cannot list VPA checkpoints from namespace %v. Reason: %+v", namespace, err)
		}
		for _, checkpoint := range checkpointList.Items {

			klog.V(3).Infof("Loading VPA %s/%s checkpoint for %s", checkpoint.ObjectMeta.Namespace, checkpoint.Spec.VPAObjectName, checkpoint.Spec.ContainerName)
			err = feeder.setVpaCheckpoint(&checkpoint)
			if err != nil {
				klog.Errorf("Error while loading checkpoint. Reason: %+v", err)
			}

		}
	}
}
```

- 上面提到的VerticalPodAutoscalerCheckpoints是一种crd会存储在 etcd中，代表的是vpa对应 的历史监控数据

```shell
[root@k8s-master01 ~]# kubectl get VerticalPodAutoscalerCheckpoints -n vpa
NAME                    AGE
vpa-nginx-off01-nginx   4h30m
[root@k8s-master01 ~]# kubectl get VerticalPodAutoscalerCheckpoints -n vpa -o yaml
apiVersion: v1
items:
- apiVersion: autoscaling.k8s.io/v1
  kind: VerticalPodAutoscalerCheckpoint
  metadata:
    creationTimestamp: "2021-11-04T07:16:31Z"
    generation: 274
    name: vpa-nginx-off01-nginx
    namespace: vpa
    resourceVersion: "5931730"
    uid: bba20180-f5c7-474a-aeea-166c4d86a474
  spec:
    containerName: nginx
    vpaObjectName: vpa-nginx-off01
  status:
    cpuHistogram:
      bucketWeights:
        "0": 10000
        "13": 18
        "14": 20
        "25": 35
        "26": 20
        "27": 20
      referenceTimestamp: "2021-11-04T00:00:00Z"
      totalWeight: 71.22690875829201
    firstSampleStart: "2021-11-04T07:17:22Z"
    lastSampleStart: "2021-11-04T11:49:22Z"
    lastUpdateTime: null
    memoryHistogram:
      bucketWeights:
        "0": 10000
      referenceTimestamp: "2021-11-05T00:00:00Z"
      totalWeight: 10.256443489574725
    totalSamplesCount: 540
    version: v3
kind: List
metadata:
  resourceVersion: ""
  selfLink: "
```

##### 如果是prometheus型的 从prometheus查询8天的历史数据

- 这里看到构造了查询prometheus 的client等对象，

```go
		config := history.PrometheusHistoryProviderConfig{
			Address:                *prometheusAddress,
			QueryTimeout:           promQueryTimeout,
			HistoryLength:          *historyLength,
			HistoryResolution:      *historyResolution,
			PodLabelPrefix:         *podLabelPrefix,
			PodLabelsMetricName:    *podLabelsMetricName,
			PodNamespaceLabel:      *podNamespaceLabel,
			PodNameLabel:           *podNameLabel,
			CtrNamespaceLabel:      *ctrNamespaceLabel,
			CtrPodNameLabel:        *ctrPodNameLabel,
			CtrNameLabel:           *ctrNameLabel,
			CadvisorMetricsJobName: *prometheusJobName,
			Namespace:              *vpaObjectNamespace,
		}
		provider, err := history.NewPrometheusHistoryProvider(config)
		if err != nil {
			klog.Fatalf("Could not initialize history provider: %v", err)
		}
		recommender.GetClusterStateFeeder().InitFromHistoryProvider(provider)
```

- 在InitFromHistoryProvider中 首先调用GetClusterHistory获取
- 追踪后发现就是调用的prometheus 的QueryRange接口，拼接的查询promql 如下
  - cpu 型`rate(container_cpu_usage_seconds_total{job="kubernetes-nodes-cadvisor", pod!="POD", pod!="",container="xxx"}[30s])`
  - mem 型`container_memory_working_set_bytes{job="kubernetes-nodes-cadvisor", pod!="POD", pod!="",container="xxx"}`
- 同时查询的时间范围来自配置项history-length 默认是8d
- prometheus类型的获取历史数据的代码追踪如下

```go
func (feeder *clusterStateFeeder) InitFromHistoryProvider(historyProvider history.HistoryProvider) {
	klog.V(3).Info("Initializing VPA from history provider")
	clusterHistory, err := historyProvider.GetClusterHistory()

func (p *prometheusHistoryProvider) GetClusterHistory() (map[model.PodID]*PodHistory, error) {
	res := make(map[model.PodID]*PodHistory)
	var podSelector string
	if p.config.CadvisorMetricsJobName != "" {
		podSelector = fmt.Sprintf("job=\"%s\", ", p.config.CadvisorMetricsJobName)
	}
	podSelector = podSelector + fmt.Sprintf("%s=~\".+\", %s!=\"POD\", %s!=\"\"",
		p.config.CtrPodNameLabel, p.config.CtrNameLabel, p.config.CtrNameLabel)

	if p.config.Namespace != "" {
		podSelector = fmt.Sprintf("%s, %s=\"%s\"", podSelector, p.config.CtrNamespaceLabel, p.config.Namespace)
	}
	historicalCpuQuery := fmt.Sprintf("rate(container_cpu_usage_seconds_total{%s}[%s])", podSelector, p.config.HistoryResolution)
	klog.V(4).Infof("Historical CPU usage query used: %s", historicalCpuQuery)
	err := p.readResourceHistory(res, historicalCpuQuery, model.ResourceCPU)
	if err != nil {
		return nil, fmt.Errorf("cannot get usage history: %v", err)
	}

	historicalMemoryQuery := fmt.Sprintf("container_memory_working_set_bytes{%s}", podSelector)
	klog.V(4).Infof("Historical memory usage query used: %s", historicalMemoryQuery)
	err = p.readResourceHistory(res, historicalMemoryQuery, model.ResourceMemory)
	if err != nil {
		return nil, fmt.Errorf("cannot get usage history: %v", err)
	}
	for _, podHistory := range res {
		for _, samples := range podHistory.Samples {
			sort.Slice(samples, func(i, j int) bool { return samples[i].MeasureStart.Before(samples[j].MeasureStart) })
		}
	}
	p.readLastLabels(res, p.config.PodLabelsMetricName)
	return res, nil
}

func (p *prometheusHistoryProvider) readResourceHistory(res map[model.PodID]*PodHistory, query string, resource model.ResourceName) error {
	end := time.Now()
	start := end.Add(-time.Duration(p.historyDuration))

	ctx, cancel := context.WithTimeout(context.Background(), p.queryTimeout)
	defer cancel()

	result, _, err := p.prometheusClient.QueryRange(ctx, query, prometheusv1.Range{
		Start: start,
		End:   end,
		Step:  time.Duration(p.historyResolution),
	})
```

#### 最终会定时运行recommender.RunOnce

```go
	ticker := time.Tick(*metricsFetcherInterval)
	for range ticker {
		recommender.RunOnce()
		healthCheck.UpdateLastActivity()
	}
```

##### 01 RunOnce又分很多步骤，首先更新vpa的状态到clusterState中

```go
func (r *recommender) RunOnce() {
	timer := metrics_recommender.NewExecutionTimer()
	defer timer.ObserveTotal()

	ctx := context.Background()
	ctx, cancelFunc := context.WithDeadline(ctx, time.Now().Add(*checkpointsWriteTimeout))
	defer cancelFunc()

	klog.V(3).Infof("Recommender Run")

	r.clusterStateFeeder.LoadVPAs()
	timer.ObserveStep("LoadVPAs")
```

- 首先通过 vpa的lister获取所有vpa

```go
// Fetch VPA objects and load them into the cluster state.
func (feeder *clusterStateFeeder) LoadVPAs() {
	// List VPA API objects.
	vpaCRDs, err := feeder.vpaLister.List(labels.Everything())
	if err != nil {
		klog.Errorf("Cannot list VPAs. Reason: %+v", err)
		return
	}
	klog.V(3).Infof("Fetched %d VPAs.", len(vpaCRDs))
	// Add or update existing VPAs in the model.
	vpaKeys := make(map[model.VpaID]bool)
```

- 遍历这些vpa更新到clusterState中，没有就创建，有就更新

```go
	// Add or update existing VPAs in the model.
	vpaKeys := make(map[model.VpaID]bool)
	for _, vpaCRD := range vpaCRDs {
		vpaID := model.VpaID{
			Namespace: vpaCRD.Namespace,
			VpaName:   vpaCRD.Name,
		}

		selector, conditions := feeder.getSelector(vpaCRD)
		klog.Infof("Using selector %s for VPA %s/%s", selector.String(), vpaCRD.Namespace, vpaCRD.Name)

		if feeder.clusterState.AddOrUpdateVpa(vpaCRD, selector) == nil {
			// Successfully added VPA to the model.
			vpaKeys[vpaID] = true

			for _, condition := range conditions {
				if condition.delete {
					delete(feeder.clusterState.Vpas[vpaID].Conditions, condition.conditionType)
				} else {
					feeder.clusterState.Vpas[vpaID].Conditions.Set(condition.conditionType, true, "", condition.message)
				}
			}
		}
	}
```

- 在clusterState缓存中删除已经不存在的vpa

```go
	// Delete non-existent VPAs from the model.
	for vpaID := range feeder.clusterState.Vpas {
		if _, exists := vpaKeys[vpaID]; !exists {
			klog.V(3).Infof("Deleting VPA %v", vpaID)
			feeder.clusterState.DeleteVpa(vpaID)
		}
	}
	feeder.clusterState.ObservedVpas = vpaCRDs
```

##### 02 LoadPods 更新pod状态

```go
	r.clusterStateFeeder.LoadPods()
	timer.ObserveStep("LoadPods")
```

- 获取最新pod状态，准备一个以podspec为key 的map，对比clusterState删除不存在的

```go
func (feeder *clusterStateFeeder) LoadPods() {
	podSpecs, err := feeder.specClient.GetPodSpecs()
	if err != nil {
		klog.Errorf("Cannot get SimplePodSpecs. Reason: %+v", err)
	}
	pods := make(map[model.PodID]*spec.BasicPodSpec)
	for _, spec := range podSpecs {
		pods[spec.ID] = spec
	}
	for key := range feeder.clusterState.Pods {
		if _, exists := pods[key]; !exists {
			klog.V(3).Infof("Deleting Pod %v", key)
			feeder.clusterState.DeletePod(key)
		}
```

- 遍历pod更新 clusterState中的状态，不存在就创建，存在就更新

```go
	for _, pod := range pods {
		if feeder.memorySaveMode && !feeder.matchesVPA(pod) {
			continue
		}
		feeder.clusterState.AddOrUpdatePod(pod.ID, pod.PodLabels, pod.Phase)
		for _, container := range pod.Containers {
			if err = feeder.clusterState.AddOrUpdateContainer(container.ID, container.Request); err != nil {
				klog.Warningf("Failed to add container %+v. Reason: %+v", container.ID, err)
			}
		}
	}
```

##### 03 LoadRealTimeMetrics 获取容器当前的资源监控情况更新 clusterState

```go
	r.clusterStateFeeder.LoadRealTimeMetrics()
	timer.ObserveStep("LoadMetrics")
	klog.V(3).Infof("ClusterState is tracking %v PodStates and %v VPAs", len(r.clusterState.Pods), len(r.clusterState.Vpas))

```

- 首先会调用metricsClient获取容器的监控指标

```go

func (feeder *clusterStateFeeder) LoadRealTimeMetrics() {
	containersMetrics, err := feeder.metricsClient.GetContainersMetrics()
	if err != nil {
		klog.Errorf("Cannot get ContainerMetricsSnapshot from MetricsClient. Reason: %+v", err)
	}

```

- 这里获取的手段是通过restClient 获取podMetrics这种类型的对象

```go
func (c *metricsClient) GetContainersMetrics() ([]*ContainerMetricsSnapshot, error) {
	var metricsSnapshots []*ContainerMetricsSnapshot

	podMetricsInterface := c.metricsGetter.PodMetricses(c.namespace)
	podMetricsList, err := podMetricsInterface.List(context.TODO(), metav1.ListOptions{})
	if err != nil {
		return nil, err
	}
	klog.V(3).Infof("%v podMetrics retrieved for all namespaces", len(podMetricsList.Items))
	for _, podMetrics := range podMetricsList.Items {
		metricsSnapshotsForPod := createContainerMetricsSnapshots(podMetrics)
		metricsSnapshots = append(metricsSnapshots, metricsSnapshotsForPod...)
	}

	return metricsSnapshots, nil
}

```

- 这里我们执行get PodMetrics获取的结果和上面一样，都是由metrics-server提供的每个pod的cpu mem当前点监控数据

```go
[root@k8s-master01 ~]# kubectl get PodMetrics -A
NAMESPACE          NAME                                            CPU          MEMORY     WINDOW
calico-system      calico-kube-controllers-7578849648-4zshs        1420100n     39220Ki    15s
calico-system      calico-node-4bcrs                               50535428n    112276Ki   15s
calico-system      calico-node-rgbcp                               31908430n    127544Ki   15s
calico-system      calico-typha-846d7f69df-gnbl8                   975616n      36696Ki    15s
calico-system      calico-typha-846d7f69df-s6bmt                   1752875n     27392Ki    15s
custom-metrics     custom-metrics-apiserver-77886b4f7b-tcfcw       4790963n     20060Ki    15s
default            grafana-756fb84d84-25vgx                        1346564n     26824Ki    15s
default            hpa-demo01-5f65595656-j6g9h                     0            1368Ki     15s
default            hpa-mem-demo01-cf66bf74d-7kvwj                  0            6636Ki     15s
default            login-pod-deployment-65f574799c-vd55p           0            11828Ki    15s
default            myconfigmap01-pod                               0            1348Ki     15s
default            mysa-pod01                                      0            1544Ki     15s
default            nfs-client-provisioner-78577dfbf6-5vc8k         1417133n     9404Ki     15s
default            nginx-dep08-584b5f5458-cbksj                    0            1360Ki     15s
default            nginx-dep08-584b5f5458-m7pz8                    0            1360Ki     15s
default            nginx-deployment-curl01-5dfbf76b79-wknqh        0            32Ki       15s
default            nginx-deployment-svc01-5cc6946cb4-4sdq7         0            1488Ki     15s
default            nginx-deployment-svc01-5cc6946cb4-64z5c         0            1348Ki     15s
default            nginx-deployment-svc01-5cc6946cb4-mqb7v         0            1336Ki     15s
default            nginx-deployment-svc05-68684b78bd-dk4wt         0            1348Ki     15s
default            nginx-deployment-svc05-68684b78bd-k2s8b         0            1348Ki     15s
default            nginx-deployment-svc06-6bd95799cf-4hx5h         0            1340Ki     15s
default            nginx-deployment-svc06-6bd95799cf-ljp64         0            1364Ki     15s
default            nginx-svc09-5dfcb54f88-rbhjf                    0            1372Ki     15s
default            nginx-svc09-5dfcb54f88-s79cg                    0            1360Ki     15s
default            onepod-multicontiner02                          0            1392Ki     15s
default            pod-client                                      0            36Ki       15s
default            prometheus-single-0                             23907277n    413972Ki   15s
default            web-0                                           0            1332Ki     15s
default            web-1                                           0            1332Ki     15s
guestbook-system   guestbook-controller-manager-59fc5d658c-jnztl   3804345n     22876Ki    15s
kong               ingress-kong-66d575669c-vhchh                   1916532n     345564Ki   15s
kube-system        coredns-7d75679df-9vzm2                         5149311n     25712Ki    15s
kube-system        coredns-7d75679df-wf8wl                         4295470n     24716Ki    15s
kube-system        etcd-k8s-master01                               46623597n    313152Ki   15s
kube-system        kube-apiserver-k8s-master01                     115710599n   658548Ki   15s
kube-system        kube-controller-manager-k8s-master01            23197783n    80488Ki    15s
kube-system        kube-proxy-6czbx                                378078n      24424Ki    15s
kube-system        kube-proxy-kq4m2                                700411n      23300Ki    15s
kube-system        kube-scheduler-k8s-master01                     5232800n     24936Ki    15s
kube-system        kube-state-metrics-647444dd74-rwwqk             1424096n     29700Ki    15s
kube-system        metrics-server-c44f75469-sf27q                  3301844n     22372Ki    15s
kube-system        nginx-svc09-5dfcb54f88-f94qq                    0            1360Ki     15s
kube-system        nginx-svc09-5dfcb54f88-nnrqj                    0            1368Ki     15s
kube-system        prometheus-0                                    21490286n    679628Ki   15s
kube-system        traefik-ingress-controller-54d6b8df8d-r858j     5712229n     19304Ki    15s
kube-system        vpa-admission-controller-6bbd694ccb-spt47       866348n      12936Ki    15s
kube-system        vpa-recommender-666f49cfcf-qqpmk                777488n      16392Ki    15s
kube-system        vpa-updater-5886d4796d-dbvd4                    386832n      15120Ki    15s
monitoring         alertmanager-main-0                             3156620n     37196Ki    15s
monitoring         alertmanager-main-1                             3795451n     41352Ki    15s
monitoring         alertmanager-main-2                             7393834n     33064Ki    15s
monitoring         blackbox-exporter-55c457d5fb-kslj5              489715n      44280Ki    15s
monitoring         grafana-6dd5b5f65-prp9x                         13150436n    34856Ki    15s
monitoring         kube-state-metrics-76f6cb7996-xsq4j             814736n      67844Ki    15s
monitoring         node-exporter-jzfqx                             22334647n    36788Ki    15s
monitoring         node-exporter-nv2gf                             13971271n    35856Ki    15s
monitoring         prometheus-adapter-59df95d9f5-68djm             5852026n     38160Ki    15s
monitoring         prometheus-adapter-59df95d9f5-chhlp             3275855n     36464Ki    15s
monitoring         prometheus-k8s-0                                21800347n    521812Ki   15s
monitoring         prometheus-k8s-1                                77848664n    509936Ki   15s
monitoring         prometheus-operator-7775c66ccf-zfvkp            1660753n     64432Ki    15s
tigera-operator    tigera-operator-cf6b69777-nrfng                 3267137n     29632Ki    15s
vpa                vpa-nginx-off01-79f78f6797-pvhlv                0            1500Ki     15s
vpa                vpa-nginx-off01-79f78f6797-q8v4v                0            1504Ki     15s
[root@k8s-master01 ~]# 
```

- 然后遍历拿到的metrics数据计算并更新clusterState中的数据

```go
	sampleCount := 0
	droppedSampleCount := 0
	for _, containerMetrics := range containersMetrics {
		for _, sample := range newContainerUsageSamplesWithKey(containerMetrics) {
			if err := feeder.clusterState.AddSample(sample); err != nil {
				// Not all pod states are tracked in memory saver mode
				if _, isKeyError := err.(model.KeyError); isKeyError && feeder.memorySaveMode {
					continue
				}
				klog.Warningf("Error adding metric sample for container %v: %v", sample.Container, err)
				droppedSampleCount++
			} else {
				sampleCount++
			}
		}
	}
```

- 同时处理容器oom的case

```go
Loop:
	for {
		select {
		case oomInfo := <-feeder.oomChan:
			klog.V(3).Infof("OOM detected %+v", oomInfo)
			if err = feeder.clusterState.RecordOOM(oomInfo.ContainerID, oomInfo.Timestamp, oomInfo.Memory); err != nil {
				klog.Warningf("Failed to record OOM %+v. Reason: %+v", oomInfo, err)
			}
		default:
			break Loop
		}
	}
```

- 根据RecordOOM粗看，像是在容器oom之后 计算内存的峰值再加上100M后乘以1.2倍，总之就是oom了之后多给他点内存

```go
// RecordOOM adds info regarding OOM event in the model as an artificial memory sample.
func (container *ContainerState) RecordOOM(timestamp time.Time, requestedMemory ResourceAmount) error {
	// Discard old OOM
	if timestamp.Before(container.WindowEnd.Add(-1 * GetAggregationsConfig().MemoryAggregationInterval)) {
		return fmt.Errorf("OOM event will be discarded - it is too old (%v)", timestamp)
	}
	// Get max of the request and the recent usage-based memory peak.
	// Omitting oomPeak here to protect against recommendation running too high on subsequent OOMs.
	memoryUsed := ResourceAmountMax(requestedMemory, container.memoryPeak)
	memoryNeeded := ResourceAmountMax(memoryUsed+MemoryAmountFromBytes(OOMMinBumpUp),
		ScaleResource(memoryUsed, OOMBumpUpRatio))

	oomMemorySample := ContainerUsageSample{
		MeasureStart: timestamp,
		Usage:        memoryNeeded,
		Resource:     ResourceMemory,
	}
	if !container.addMemorySample(&oomMemorySample, true) {
		return fmt.Errorf("adding OOM sample failed")
	}
	return nil
}
```

##### 04 UpdateVPAs 利用上面计算好的cpu内存值 给出推荐值

```go
	r.UpdateVPAs()
	timer.ObserveStep("UpdateVPAs")

```

- 整体的步骤就是 遍历要更新的vpa，调用GetRecommendedPodResources生成 cpu和mem的推荐值，然后更新vpa的Recommended字段，交给后面的updater组件处理即可

```go
// Updates VPA CRD objects' statuses.
func (r *recommender) UpdateVPAs() {
	cnt := metrics_recommender.NewObjectCounter()
	defer cnt.Observe()

	for _, observedVpa := range r.clusterState.ObservedVpas {
		key := model.VpaID{
			Namespace: observedVpa.Namespace,
			VpaName:   observedVpa.Name,
		}

		vpa, found := r.clusterState.Vpas[key]
		if !found {
			continue
		}
		resources := r.podResourceRecommender.GetRecommendedPodResources(GetContainerNameToAggregateStateMap(vpa))
		had := vpa.HasRecommendation()
		vpa.UpdateRecommendation(getCappedRecommendation(vpa.ID, resources, observedVpa.Spec.ResourcePolicy))
		if vpa.HasRecommendation() && !had {
			metrics_recommender.ObserveRecommendationLatency(vpa.Created)
		}
		hasMatchingPods := vpa.PodCount > 0
		vpa.UpdateConditions(hasMatchingPods)
		if err := r.clusterState.RecordRecommendation(vpa, time.Now()); err != nil {
			klog.Warningf("%v", err)
			klog.V(4).Infof("VPA dump")
			klog.V(4).Infof("%+v", vpa)
			klog.V(4).Infof("HasMatchingPods: %v", hasMatchingPods)
			klog.V(4).Infof("PodCount: %v", vpa.PodCount)
			pods := r.clusterState.GetMatchingPods(vpa)
			klog.V(4).Infof("MatchingPods: %+v", pods)
			if len(pods) != vpa.PodCount {
				klog.Errorf("ClusterState pod count and matching pods disagree for vpa %v/%v", vpa.ID.Namespace, vpa.ID.VpaName)
			}
		}
		cnt.Add(vpa)

		_, err := vpa_utils.UpdateVpaStatusIfNeeded(
			r.vpaClient.VerticalPodAutoscalers(vpa.ID.Namespace), vpa.ID.VpaName, vpa.AsStatus(), &observedVpa.Status)
		if err != nil {
			klog.Errorf(
				"Cannot update VPA %v object. Reason: %+v", vpa.ID.VpaName, err)
		}
	}
}
```

##### 05 MaintainCheckpoints 更新checkpoint到 etcd中

```go

	r.MaintainCheckpoints(ctx, *minCheckpointsPerRun)
	timer.ObserveStep("MaintainCheckpoints")
```

- 如果使用的是 checkpoint的模式，代表历史数据的存储依赖etcd，那么在计算后调用StoreCheckpoints将结果存入etcd中

```go

func (r *recommender) MaintainCheckpoints(ctx context.Context, minCheckpointsPerRun int) {
	now := time.Now()
	if r.useCheckpoints {
		if err := r.checkpointWriter.StoreCheckpoints(ctx, now, minCheckpointsPerRun); err != nil {
			klog.Warningf("Failed to store checkpoints. Reason: %+v", err)
		}
		if time.Now().Sub(r.lastCheckpointGC) > r.checkpointsGCInterval {
			r.lastCheckpointGC = now
			r.clusterStateFeeder.GarbageCollectCheckpoints()
		}
	}
}

```

# 本节重点总结

> 初始化阶段

- recommender启动的时候会根据不同的存储后端 把checkpoints数据加载到 clusterState中
  - useCheckpoints代表从etcd获取 遍历ns 调用feeder 通过restapi获取VerticalPodAutoscalerCheckpoints，然后通过feeder把checkpoint塞入clusterState中
  - 如果是prometheus型的 从prometheus查询8天的历史数据，追踪后发现就是调用的prometheus 的QueryRange接口，拼接的查询promql 如下
    - cpu 型`rate(container_cpu_usage_seconds_total{job="kubernetes-nodes-cadvisor", pod!="POD", pod!="",container="xxx"}[30s])`
    - mem 型`container_memory_working_set_bytes{job="kubernetes-nodes-cadvisor", pod!="POD", pod!="",container="xxx"}`

> 运行依赖于 recommender.RunOnce

- 01 首先更新vpa的状态到clusterState中
- 02 LoadPods 更新pod状态
- 03  LoadRealTimeMetrics 获取容器当前的资源监控情况更新 clusterState
  - 这里看到的就是从 apiserver的聚合插件请求后端 metrics-server的数据
- 04 UpdateVPAs 利用上面计算好的cpu内存值 给出推荐值
  - 整体的步骤就是 遍历要更新的vpa，调用GetRecommendedPodResources生成 cpu和mem的推荐值，然后更新vpa的Recommended字段，交给后面的updater组件处理即可
- 05  如果使用的是 checkpoint的模式，代表历史数据的存储依赖etcd，那么在计算后调用StoreCheckpoints将结果存入etcd中