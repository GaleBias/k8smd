# 本节重点总结
- manageReplicas 是最核心的方法，它会计算 replicaSet 需要创建或者删除多少个 pod 并调用 apiserver 的接口进行操作
- 在此阶段仅仅是调用 apiserver 的接口进行创建，并不保证 pod 成功运行
- 如果在某一轮，未能成功创建的所有 Pod 对象，则不再创建剩余的 pod
- 一个周期内最多只能创建或删除 500 个 pod，若超过上限值未创建完成的 pod 数会在下一个 syncLoop 继续进行处理。

> manageReplicas创建的过程
- 计算当前pod数量和rs副本数的差距diff，如果diff<0 说明 rs 实际的 pod 数未达到期望值需要继续创建 pod
    - 使用慢启动扩容函数 slowStartBatch 创建pod，目的是为了防止大量创建pod出现相同的错误
    - 比如任务总数是10个，那么分批的任务数为 1,2,4,3

> manageReplicas删除的过程
- 如果 diff > 0 说明缩容操作需要删除多余的 pod
    - 通过 getPodsToDelete 方法筛选出需要删除的 pod，
    -  对 pod 进行筛选操作，此阶段会选出最劣质的 pod，下面是用到的 6 种筛选方法：
        - 判断是否绑定了 node：Unassigned < assigned；
        - 判断 pod phase：PodPending < PodUnknown < PodRunning；
        - 判断 pod 状态：Not ready < ready；
        - 若 pod 都为 ready，则按运行时间排序，运行时间最短会被删除：empty time < less time < more time；
        - 根据 pod 重启次数排序：higher restart counts < lower restart counts；
        - 按 pod 创建时间进行排序：Empty creation time pods < newer pods < older pods；
    - 然后调用apiserver删除
    



# 接上回
- rs控制器两步走的第一步 ：初始化相关对象已分析完，下面来看下Run运行
- 我们已经非常熟悉了，老配方
- 首先需要等待这个控制器依赖的informer本地数据至少同步过一次
- 然后启动多个worker，共同消费执行同步
```go
// Run begins watching and syncing.
func (rsc *ReplicaSetController) Run(workers int, stopCh <-chan struct{}) {
	defer utilruntime.HandleCrash()
	defer rsc.queue.ShutDown()

	controllerName := strings.ToLower(rsc.Kind)
	klog.Infof("Starting %v controller", controllerName)
	defer klog.Infof("Shutting down %v controller", controllerName)

	if !cache.WaitForNamedCacheSync(rsc.Kind, stopCh, rsc.podListerSynced, rsc.rsListerSynced) {
		return
	}

	for i := 0; i < workers; i++ {
		go wait.Until(rsc.worker, time.Second, stopCh)
	}

	<-stopCh
}
```

## worker的内容
- 熟悉的配方：不断的消费队列中的数据然后调用 syncHandler处理
- syncHandler对应的就是syncReplicaSet
```go
func (rsc *ReplicaSetController) worker() {
	for rsc.processNextWorkItem() {
	}
}

func (rsc *ReplicaSetController) processNextWorkItem() bool {
	key, quit := rsc.queue.Get()
	if quit {
		return false
	}
	defer rsc.queue.Done(key)

	err := rsc.syncHandler(key.(string))
	if err == nil {
		rsc.queue.Forget(key)
		return true
	}

	utilruntime.HandleError(fmt.Errorf("sync %q failed with %v", key, err))
	rsc.queue.AddRateLimited(key)

	return true
}
```


### syncReplicaSet 解读
#### 前期准备工作
- 计时的操作
```go
func (rsc *ReplicaSetController) syncReplicaSet(key string) error {
	startTime := time.Now()
	defer func() {
		klog.V(4).Infof("Finished syncing %v %q (%v)", rsc.Kind, key, time.Since(startTime))
	}()

```
- 从key中分隔namespace和name，然后调用rsLister就是informer的本地存储中获取这个ns的这个rs对象
- 如果没找到就删除这个key在expectations的缓存
```go
	namespace, name, err := cache.SplitMetaNamespaceKey(key)
	if err != nil {
		return err
	}
	rs, err := rsc.rsLister.ReplicaSets(namespace).Get(name)
	if apierrors.IsNotFound(err) {
		klog.V(4).Infof("%v %v has been deleted", rsc.Kind, key)
		rsc.expectations.DeleteExpectations(key)
		return nil
	}
```

#### 这里我们回顾一下expectations 机制

- 其实，控制器除了有 informer 的缓存外，还有一个本地缓存就是 expectations
- expectations 会记录 控制器 所有对象需要 add/del 的 pod 数量
- 若两者都为 0 则说明该 控制器所期望创建的 pod 或者删除的 pod 数已经被满足
- 若不满足则说明某次在 syncLoop 中创建或者删除 pod 时有失败的操作
- 则需要等待 expectations 过期后再次同步该控制器
> 总结 expectations 机制的目的就是减少不必要的 sync 操作。

#####  expectations 中用到的几个方法
```go
type ControllerExpectationsInterface interface {
	GetExpectations(controllerKey string) (*ControlleeExpectations, bool, error)
    // 检查expectations 机制是否会满足
	SatisfiedExpectations(controllerKey string) bool
     // 删除该 key
	DeleteExpectations(controllerKey string)
	SetExpectations(controllerKey string, add, del int) error
    // 写入 key 需要 add 的 pod 数量
	ExpectCreations(controllerKey string, adds int) error
    // 写入 key 需要 del 的 pod 数量
	ExpectDeletions(controllerKey string, dels int) error
    // 创建了一个 pod 说明 expectations 中对应的 key add 期望值需要减少一个 pod， add -1
	CreationObserved(controllerKey string)
    // 删除了一个 pod 说明 expectations 中对应的 key del 期望值需要减少一个 pod， del - 1
	DeletionObserved(controllerKey string)
	RaiseExpectations(controllerKey string, add, del int)
	LowerExpectations(controllerKey string, add, del int)
}
```


##### 首先看下判断 jobNeedsSync用到的SatisfiedExpectations方法
- 我们观察下SatisfiedExpectations返回true的地方其实有三个
    - 该 key 在 ControllerExpectations 中不存在，即该对象是新创建的；
    - 满足exp.Fulfilled() ，该 key 在 ControllerExpectations 中的 adds 和 dels 都 <= 0，即调用 apiserver 的创建和删除接口没有失败过
    - 满足 exp.isExpired() ，该 key 在 ControllerExpectations 中已经超过 5min 没有更新了
- SatisfiedExpectations代码如下
```go
func (r *ControllerExpectations) SatisfiedExpectations(controllerKey string) bool {
	if exp, exists, err := r.GetExpectations(controllerKey); exists {
		if exp.Fulfilled() {
			klog.V(4).Infof("Controller expectations fulfilled %#v", exp)
			return true
		} else if exp.isExpired() {
			klog.V(4).Infof("Controller expectations expired %#v", exp)
			return true
		} else {
			klog.V(4).Infof("Controller still waiting on expectations %#v", exp)
			return false
		}
	} else if err != nil {
		klog.V(2).Infof("Error encountered while checking expectations %#v, forcing sync", err)
	} else {
		// When a new controller is created, it doesn't have expectations.
		// When it doesn't see expected watch events for > TTL, the expectations expire.
		//	- In this case it wakes up, creates/deletes controllees, and sets expectations again.
		// When it has satisfied expectations and no controllees need to be created/destroyed > TTL, the expectations expire.
		//	- In this case it continues without setting expectations till it needs to create/delete controllees.
		klog.V(4).Infof("Controller %v either never recorded expectations, or the ttl expired.", controllerKey)
	}
	// Trigger a sync if we either encountered and error (which shouldn't happen since we're
	// getting from local store) or this controller hasn't established expectations.
	return true
}

```

> Fulfilled 代表 add和del都为小于等于0
```go
// Fulfilled returns true if this expectation has been fulfilled.
func (e *ControlleeExpectations) Fulfilled() bool {
	// TODO: think about why this line being atomic doesn't matter
	return atomic.LoadInt64(&e.add) <= 0 && atomic.LoadInt64(&e.del) <= 0
}
```
> isExpired代表上次更新时间超过5分钟
```go
func (exp *ControlleeExpectations) isExpired() bool {
	return clock.RealClock{}.Since(exp.timestamp) > ExpectationsTimeout
}
```

#### 过滤pod
- 根据标签选择去本地informer缓存中过滤pod，然后在pod中过滤active的
```go
	selector, err := metav1.LabelSelectorAsSelector(rs.Spec.Selector)
	if err != nil {
		utilruntime.HandleError(fmt.Errorf("error converting pod selector to selector: %v", err))
		return nil
	}

	// list all pods to include the pods that don't match the rs`s selector
	// anymore but has the stale controller ref.
	// TODO: Do the List and Filter in a single pass, or use an index.
	allPods, err := rsc.podLister.Pods(rs.Namespace).List(labels.Everything())
	if err != nil {
		return err
	}
	// Ignore inactive pods.
	filteredPods := controller.FilterActivePods(allPods)


```
- 然后调用claimPods 从activepod中用标签匹配pod、认领pod等操作之后再过滤一遍得到filteredPods
```go
	// NOTE: filteredPods are pointing to objects from cache - if you need to
	// modify them, you need to copy it first.
	filteredPods, err = rsc.claimPods(rs, selector, filteredPods)
	if err != nil {
		return err
	}
```

#### 调用manageReplicas 处理同步
- 调用manageReplicas 处理同步
- 然后计算newStatus，更新rs到apiserver中，并判断错误，
- 所以重点在manageReplicas中
```go
	var manageReplicasErr error
	if rsNeedsSync && rs.DeletionTimestamp == nil {
		manageReplicasErr = rsc.manageReplicas(filteredPods, rs)
	}
	rs = rs.DeepCopy()
	newStatus := calculateStatus(rs, filteredPods, manageReplicasErr)

	// Always updates status as pods come up or die.
	updatedRS, err := updateReplicaSetStatus(rsc.kubeClient.AppsV1().ReplicaSets(rs.Namespace), rs, newStatus)
	if err != nil {
		// Multiple things could lead to this update failing. Requeuing the replica set ensures
		// Returning an error causes a requeue without forcing a hotloop
		return err
	}
	// Resync the ReplicaSet after MinReadySeconds as a last line of defense to guard against clock-skew.
	if manageReplicasErr == nil && updatedRS.Spec.MinReadySeconds > 0 &&
		updatedRS.Status.ReadyReplicas == *(updatedRS.Spec.Replicas) &&
		updatedRS.Status.AvailableReplicas != *(updatedRS.Spec.Replicas) {
		rsc.queue.AddAfter(key, time.Duration(updatedRS.Spec.MinReadySeconds)*time.Second)
	}
	return manageReplicasErr
```


### manageReplicas 真正的同步动作
- 计算当前pod数量和rs副本数的差距
```go
	diff := len(filteredPods) - int(*(rs.Spec.Replicas))
	rsKey, err := controller.KeyFunc(rs)
	if err != nil {
		utilruntime.HandleError(fmt.Errorf("couldn't get key for %v %#v: %v", rsc.Kind, rs, err))
		return nil
	}
```
#### 如果 diff < 0
- 说明 rs 实际的 pod 数未达到期望值需要继续创建 pod
- 首先保证diff不要超过500，然后更新expectations缓存，将add 减去diff个
- 打印replicaSet  need creating的日志
```go
	if diff < 0 {
		diff *= -1
		if diff > rsc.burstReplicas {
			diff = rsc.burstReplicas
		}

		rsc.expectations.ExpectCreations(rsKey, diff)
		klog.V(2).InfoS("Too few replicas", "replicaSet", klog.KObj(rs), "need", *(rs.Spec.Replicas), "creating", diff)
		
```
- 然后调用slowStartBatch 慢启动创建所需的pod
```go
		successfulCreations, err := slowStartBatch(diff, controller.SlowStartInitialBatchSize, func() error {
			err := rsc.podControl.CreatePods(rs.Namespace, &rs.Spec.Template, rs, metav1.NewControllerRef(rs, rsc.GroupVersionKind))
			if err != nil {
				if apierrors.HasStatusCause(err, v1.NamespaceTerminatingCause) {
					// if the namespace is being terminated, we don't have to do
					// anything because any creation will fail
					return nil
				}
			}
			return err
		})
```
- 这个slowStartBatch在7.2已经讲解过了，这里复习一下

##### 慢启动扩容函数 slowStartBatch
- 目的是为了防止大量创建pod出现相同的错误
```go
func slowStartBatch(count int, initialBatchSize int, fn func() error) (int, error) {
	remaining := count
	successes := 0
	for batchSize := integer.IntMin(remaining, initialBatchSize); batchSize > 0; batchSize = integer.IntMin(2*batchSize, remaining) {
		errCh := make(chan error, batchSize)
		var wg sync.WaitGroup
		wg.Add(batchSize)
		for i := 0; i < batchSize; i++ {
			go func() {
				defer wg.Done()
				if err := fn(); err != nil {
					errCh <- err
				}
			}()
		}
		wg.Wait()
		curSuccesses := batchSize - len(errCh)
		successes += curSuccesses
		if len(errCh) > 0 {
			return successes, <-errCh
		}
		remaining -= batchSize
	}
	return successes, nil
}
```
- 代码逻辑解读，假设我们要扩容10个pod，即count=10，那么过程如下
    - 第1轮，batchSize是remaining和initialBatchSize的最小值就是1，启动1个任务
    - 第2轮，batchSize是 2和剩余数量9的最小值为2，启动2个任务
    - 第3轮，batchSize是 4和剩余数量7的最小值为4，启动2个任务
    - 第4轮，batchSize是 8和剩余数量4的最小值为3，启动3个任务，任务结束
    - 整体的任务数为 1,2,4,3
    
- 真正的动作函数是slowStartBatch的入参 rsc.podControl.CreatePods代表 rs驱动创建pod

##### 回到diff<0中
- 对慢启动创建pod 的结果做判断，如果有创建失败的，那么更新expectations缓存，将这个rsKey的 add减去skippedPods
```go
		if skippedPods := diff - successfulCreations; skippedPods > 0 {
			klog.V(2).Infof("Slow-start failure. Skipping creation of %d pods, decrementing expectations for %v %v/%v", skippedPods, rsc.Kind, rs.Namespace, rs.Name)
			for i := 0; i < skippedPods; i++ {
				// Decrement the expected number of creates because the informer won't observe this pod
				rsc.expectations.CreationObserved(rsKey)
			}
		}
		return err
```

###  如果 diff > 0 
- 如果 diff > 0 说明缩容操作需要删除多余的 pod
- 那么通过getPodsToDelete 过滤要删除的pod
```go
 else if diff > 0 {
		if diff > rsc.burstReplicas {
			diff = rsc.burstReplicas
		}
		klog.V(2).InfoS("Too many replicas", "replicaSet", klog.KObj(rs), "need", *(rs.Spec.Replicas), "deleting", diff)

		relatedPods, err := rsc.getIndirectlyRelatedPods(rs)
		utilruntime.HandleError(err)

		// Choose which Pods to delete, preferring those in earlier phases of startup.
		podsToDelete := getPodsToDelete(filteredPods, relatedPods, diff)
```
- 然后更新expectations缓存
```go
rsc.expectations.ExpectDeletions(rsKey, getPodKeys(podsToDelete))

```
- 然后就是并发调apiserver删除pod
```go
		errCh := make(chan error, diff)
		var wg sync.WaitGroup
		wg.Add(diff)
		for _, pod := range podsToDelete {
			go func(targetPod *v1.Pod) {
				defer wg.Done()
				if err := rsc.podControl.DeletePod(rs.Namespace, targetPod.Name, rs); err != nil {
					// Decrement the expected number of deletes because the informer won't observe this deletion
					podKey := controller.PodKey(targetPod)
					rsc.expectations.DeletionObserved(rsKey, podKey)
					if !apierrors.IsNotFound(err) {
						klog.V(2).Infof("Failed to delete %v, decremented expectations for %v %s/%s", podKey, rsc.Kind, rs.Namespace, rs.Name)
						errCh <- err
					}
				}
			}(pod)
		}
		wg.Wait()

		select {
		case err := <-errCh:
			// all errors have been reported before and they're likely to be the same, so we'll only return the first one we hit.
			if err != nil {
				return err
			}
		default:
		}
```

#### getPodsToDelete过滤函数解析
```go
func getPodsToDelete(filteredPods, relatedPods []*v1.Pod, diff int) []*v1.Pod {
	// No need to sort pods if we are about to delete all of them.
	// diff will always be <= len(filteredPods), so not need to handle > case.
	if diff < len(filteredPods) {
		podsWithRanks := getPodsRankedByRelatedPodsOnSameNode(filteredPods, relatedPods)
		sort.Sort(podsWithRanks)
		reportSortingDeletionAgeRatioMetric(filteredPods, diff)
	}
	return filteredPods[:diff]
}
```
-  对 pod 进行筛选操作，此阶段会选出最劣质的 pod，下面是用到的 6 种筛选方法：
    - 判断是否绑定了 node：Unassigned < assigned；
    - 判断 pod phase：PodPending < PodUnknown < PodRunning；
    - 判断 pod 状态：Not ready < ready；
    - 若 pod 都为 ready，则按运行时间排序，运行时间最短会被删除：empty time < less time < more time；
    - 根据 pod 重启次数排序：higher restart counts < lower restart counts；
    - 按 pod 创建时间进行排序：Empty creation time pods < newer pods < older pods；
    
- 筛选方法就是的ActivePodsWithRanks的Less
```go
// Less compares two pods with corresponding ranks and returns true if the first
// one should be preferred for deletion.
func (s ActivePodsWithRanks) Less(i, j int) bool {
	// 1. Unassigned < assigned
	// If only one of the pods is unassigned, the unassigned one is smaller
	if s.Pods[i].Spec.NodeName != s.Pods[j].Spec.NodeName && (len(s.Pods[i].Spec.NodeName) == 0 || len(s.Pods[j].Spec.NodeName) == 0) {
		return len(s.Pods[i].Spec.NodeName) == 0
	}
	// 2. PodPending < PodUnknown < PodRunning
	if podPhaseToOrdinal[s.Pods[i].Status.Phase] != podPhaseToOrdinal[s.Pods[j].Status.Phase] {
		return podPhaseToOrdinal[s.Pods[i].Status.Phase] < podPhaseToOrdinal[s.Pods[j].Status.Phase]
	}
	// 3. Not ready < ready
	// If only one of the pods is not ready, the not ready one is smaller
	if podutil.IsPodReady(s.Pods[i]) != podutil.IsPodReady(s.Pods[j]) {
		return !podutil.IsPodReady(s.Pods[i])
	}

	// 4. lower pod-deletion-cost < higher pod-deletion cost
	if utilfeature.DefaultFeatureGate.Enabled(features.PodDeletionCost) {
		pi, _ := helper.GetDeletionCostFromPodAnnotations(s.Pods[i].Annotations)
		pj, _ := helper.GetDeletionCostFromPodAnnotations(s.Pods[j].Annotations)
		if pi != pj {
			return pi < pj
		}
	}

	// 5. Doubled up < not doubled up
	// If one of the two pods is on the same node as one or more additional
	// ready pods that belong to the same replicaset, whichever pod has more
	// colocated ready pods is less
	if s.Rank[i] != s.Rank[j] {
		return s.Rank[i] > s.Rank[j]
	}
	// TODO: take availability into account when we push minReadySeconds information from deployment into pods,
	//       see https://github.com/kubernetes/kubernetes/issues/22065
	// 6. Been ready for empty time < less time < more time
	// If both pods are ready, the latest ready one is smaller
	if podutil.IsPodReady(s.Pods[i]) && podutil.IsPodReady(s.Pods[j]) {
		readyTime1 := podReadyTime(s.Pods[i])
		readyTime2 := podReadyTime(s.Pods[j])
		if !readyTime1.Equal(readyTime2) {
			if !utilfeature.DefaultFeatureGate.Enabled(features.LogarithmicScaleDown) {
				return afterOrZero(readyTime1, readyTime2)
			} else {
				if s.Now.IsZero() || readyTime1.IsZero() || readyTime2.IsZero() {
					return afterOrZero(readyTime1, readyTime2)
				}
				rankDiff := logarithmicRankDiff(*readyTime1, *readyTime2, s.Now)
				if rankDiff == 0 {
					return s.Pods[i].UID < s.Pods[j].UID
				}
				return rankDiff < 0
			}
		}
	}
	// 7. Pods with containers with higher restart counts < lower restart counts
	if maxContainerRestarts(s.Pods[i]) != maxContainerRestarts(s.Pods[j]) {
		return maxContainerRestarts(s.Pods[i]) > maxContainerRestarts(s.Pods[j])
	}
	// 8. Empty creation time pods < newer pods < older pods
	if !s.Pods[i].CreationTimestamp.Equal(&s.Pods[j].CreationTimestamp) {
		if !utilfeature.DefaultFeatureGate.Enabled(features.LogarithmicScaleDown) {
			return afterOrZero(&s.Pods[i].CreationTimestamp, &s.Pods[j].CreationTimestamp)
		} else {
			if s.Now.IsZero() || s.Pods[i].CreationTimestamp.IsZero() || s.Pods[j].CreationTimestamp.IsZero() {
				return afterOrZero(&s.Pods[i].CreationTimestamp, &s.Pods[j].CreationTimestamp)
			}
			rankDiff := logarithmicRankDiff(s.Pods[i].CreationTimestamp, s.Pods[j].CreationTimestamp, s.Now)
			if rankDiff == 0 {
				return s.Pods[i].UID < s.Pods[j].UID
			}
			return rankDiff < 0
		}
	}
	return false
}

```

# 本节重点总结
- manageReplicas 是最核心的方法，它会计算 replicaSet 需要创建或者删除多少个 pod 并调用 apiserver 的接口进行操作
- 在此阶段仅仅是调用 apiserver 的接口进行创建，并不保证 pod 成功运行
- 如果在某一轮，未能成功创建的所有 Pod 对象，则不再创建剩余的 pod
- 一个周期内最多只能创建或删除 500 个 pod，若超过上限值未创建完成的 pod 数会在下一个 syncLoop 继续进行处理。

> manageReplicas创建的过程
- 计算当前pod数量和rs副本数的差距diff，如果diff<0 说明 rs 实际的 pod 数未达到期望值需要继续创建 pod
    - 使用慢启动扩容函数 slowStartBatch 创建pod，目的是为了防止大量创建pod出现相同的错误
    - 比如任务总数是10个，那么分批的任务数为 1,2,4,3

> manageReplicas删除的过程
- 如果 diff > 0 说明缩容操作需要删除多余的 pod
    - 通过 getPodsToDelete 方法筛选出需要删除的 pod，
    -  对 pod 进行筛选操作，此阶段会选出最劣质的 pod，下面是用到的 6 种筛选方法：
        - 判断是否绑定了 node：Unassigned < assigned；
        - 判断 pod phase：PodPending < PodUnknown < PodRunning；
        - 判断 pod 状态：Not ready < ready；
        - 若 pod 都为 ready，则按运行时间排序，运行时间最短会被删除：empty time < less time < more time；
        - 根据 pod 重启次数排序：higher restart counts < lower restart counts；
        - 按 pod 创建时间进行排序：Empty creation time pods < newer pods < older pods；
    - 然后调用apiserver删除
    



