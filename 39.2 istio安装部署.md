# 下载 Istio

- 主要是下载istioctl 和对应的yaml

```shell
wget https://github.com/istio/istio/releases/download/1.11.4/istio-1.11.4-linux-amd64.tar.gz
```

- 解压并且把 istioctl放到 /usr/bin/下面

```shell
tar xf istio-1.11.4-linux-amd64.tar.gz 
/bin/cp -f bin/istioctl /usr/bin/
```

# 安装 Istio

- 可以参考官网的安装手册进行，[地址](https://istio.io/latest/zh/docs/setup/getting-started/)
- 对于本次安装，我们采用 demo 配置组合。 选择它是因为它包含了一组专为测试准备的功能集合，另外还有用于生产或性能测试的配置组合。

```shell
istioctl install --set profile=demo -y
```

- 输出为

```shell
✔ Istio core installed
✔ Istiod installed
✔ Egress gateways installed
✔ Ingress gateways installed
✔ Installation complete
```

- 给命名空间添加标签，指示 Istio 在部署应用的时候，自动注入 Envoy 边车代理：

```shell
kubectl label namespace default istio-injection=enabled
```

# 部署示例应用

- 命令就是apply 一个bookinfo.yaml

```shell
kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml
service/details created
serviceaccount/bookinfo-details created
deployment.apps/details-v1 created
service/ratings created
serviceaccount/bookinfo-ratings created
deployment.apps/ratings-v1 created
service/reviews created
serviceaccount/bookinfo-reviews created
deployment.apps/reviews-v1 created
deployment.apps/reviews-v2 created
deployment.apps/reviews-v3 created
service/productpage created
serviceaccount/bookinfo-productpage created
deployment.apps/productpage-v1 created
```

- [bookinfo 服务的简介](https://istio.io/latest/zh/docs/examples/bookinfo/)
- 检查部署的pod ，可以看到detail、productpage、ratings都是1个，3个版本的reviews服务

```shell
[root@k8s-master01 ~]# kubectl get pods
NAME                                      READY   STATUS    RESTARTS   AGE
details-v1-79f774bdb9-gzvhx               2/2     Running   0          53m
productpage-v1-6b746f74dc-6ll9g           2/2     Running   0          53m
ratings-v1-b6994bb9-mtz4t                 2/2     Running   0          53m
reviews-v1-545db77b95-rglb7               2/2     Running   0          53m
reviews-v2-7bf8c9648f-m7dxd               2/2     Running   0          53m
reviews-v3-84779c7bbc-9pv4j               2/2     Running   0          53m
```

- 通过在rating服务中请求 productpage 检查返回的页面标题，来验证应用是否已在集群中运行，并已提供网页服务：

```shell
kubectl exec "$(kubectl get pod -l app=ratings -o jsonpath='{.items[0].metadata.name}')" -c ratings -- curl -s productpage:9080/productpage | grep -o "<title>.*</title>"
<title>Simple Bookstore App</title>

```

## 对外暴露booking服务应用程序

- 创建[Istio 入站网关（Ingress Gateway）](https://istio.io/latest/zh/docs/concepts/traffic-management/#gateways), 它会在网格边缘把一个路径映射到路由。
- ```
  kubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml

  ```
- 我们这里直接选择 nodeport 的svc暴露就可以，在 istio-ingressgateway 查看对应的nodeport端口

```shell
export INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="http2")].nodePort}')
echo $INGRESS_PORT
```

- 然后通过nodeIp+上面的nodePort来访问，我这里的$INGRESS_PORT输出的是19257
- 访问的url 为 nodeIp:19257/productpage![istio_bookinfo01.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/908/1636786574000/7ca5944fd8eb465cb4cba44333bab432.png)
- 如果能访问到说明正常的，然后多刷新几次可以看到页面上的 Book Reviews的星星会有三种情况
  - 空的
  - 5颗红色的
  - 5颗黑色的![istio_bookinfo02.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/908/1636786574000/9b03d4c766fb4e06a2c335837cc5a94e.png)

# 部署并查看 Kiali 仪表板

- Istio 和几个遥测应用做了集成。 遥测能帮你了解服务网格的结构、展示网络的拓扑结构、分析网格的健康状态。
- 使用下面说明部署 Kiali 仪表板、 以及 Prometheus、 Grafana、 还有 Jaeger，安装命令如下

```shell
kubectl apply -f samples/addons
```

- 最终会在istio-system 部署 Prometheus、 Grafana、 还有 Jaeger等

```shell
[root@k8s-master01 ~]# kubectl get pod -n istio-system
NAME                                    READY   STATUS    RESTARTS   AGE
grafana-68cc7d6d78-k6wr9                1/1     Running   0          40m
istio-egressgateway-756d4db566-c62rf    1/1     Running   0          69m
istio-ingressgateway-8577c57fb6-hz6d4   1/1     Running   0          69m
istiod-5847c59c69-w5qrd                 1/1     Running   0          71m
jaeger-5d44bc5c5d-c8rq6                 1/1     Running   0          40m
kiali-fd9f88575-6kxbl                   1/1     Running   0          40m
prometheus-77b49cb997-xq227             2/2     Running   0          40m
[root@k8s-master01 ~]# 
```

- 应用部署完之后我们还是通过NodePort访问Kiali 仪表板，编辑 kiali的svc，修改类型为nodePort

```shell
kubectl edit svc kiali -n istio-system 
spec:
  clusterIP: 10.96.93.7
  clusterIPs:
  - 10.96.93.7
  externalTrafficPolicy: Cluster
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - name: http
    nodePort: 10002
    port: 20001
    protocol: TCP
    targetPort: 20001
  - name: http-metrics
    port: 9090
    protocol: TCP
    targetPort: 9090
  selector:
    app.kubernetes.io/instance: kiali
    app.kubernetes.io/name: kiali
  sessionAffinity: None
  type: NodePort
```

- 然后打开浏览器访问  http://nodeIp:10002/kiali/console/overview，就可以看到kiali页面了
- 同时在后台启动一个不断请求productpage的curl来模拟流量

```shell
while true;do curl -s http://172.20.70.215:19257/productpage >/dev/null ;done 
```

## Kiali 仪表板功能介绍

> graph 标签

- kiali 仪表板 的graph功能介绍![istio_bookinfo03.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/908/1636786574000/c1d02de25f594445847dff28f2039e84.png)
- 还可以切换视角![istio_bookinfo04.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/908/1636786574000/89c2c1dd155f4c6b97d23c63c909aa5c.png)
- display 勾选展示什么内容，比如延迟![istio_bookinfo05.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/908/1636786574000/00d2e1adf8a648658bc87bddb5359280.png)

> workload标签

- workload的详情![istio_bookinfo06.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/908/1636786574000/67860c691cc84b07adb457580bf53f68.png)
- ![istio_bookinfo07.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/908/1636786574000/d9e55e75283942d0b28664c8cd439ac3.png)
- log 可以看到 istio-proxy 也就是envoy的日志和app容器的日志![istio_bookinfo08.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/908/1636786574000/e1946542da3540fcbba3c8710f7cda31.png)
- 查看入口流量的相关指标，如qps、延迟、http请求/响应大小![istio_bookinfo09.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/908/1636786574000/389039fc8ed5469a8a764796d24065eb.png)

# prometheus 页面

- 同时我们观察在上面同时也部署了prometheus，可以同样的改变svc的类型为 nodePort ，然后请求http://172.20.70.215:10003/targets访问prometheus

```shell
kubectl edit svc prometheus -n istio-system
spec:
  clusterIP: 10.96.164.205
  clusterIPs:
  - 10.96.164.205
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - name: http
    port: 9090
    protocol: TCP
    targetPort: 9090
    nodePort: 10003
  selector:
    app: prometheus
    component: server
    release: prometheus
  sessionAffinity: None
  type: NodePort
status:
  loadBalancer: {}
```

## istio默认的prometheus 部署形式说明

- 经过观察发现prometheus在istio是以deployment部署的

```shell
[root@k8s-master01 istio-1.11.4]# kubectl get deployment   -n istio-system     
NAME                   READY   UP-TO-DATE   AVAILABLE   AGE
grafana                1/1     1            1           153m
istio-egressgateway    1/1     1            1           3h2m
istio-ingressgateway   1/1     1            1           3h2m
istiod                 1/1     1            1           3h3m
jaeger                 1/1     1            1           153m
kiali                  1/1     1            1           153m
prometheus             1/1     1            1           153m

```

### 01 prometheus dep分析

- 然后我们看下这个deployment中就可以发现如下特点

```shell
[root@k8s-master01 istio-1.11.4]# kubectl get deployment prometheus   -n istio-system -o yaml
```

- 01 同样也是部署了configmap-reload 作为prometheus也加载配置的sidecar

```yaml

    spec:
      containers:
      - args:
        - --volume-dir=/etc/config
        - --webhook-url=http://127.0.0.1:9090/-/reload
        image: jimmidyson/configmap-reload:v0.5.0
        imagePullPolicy: IfNotPresent
        name: prometheus-server-configmap-reload
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /etc/config
          name: config-volume
          readOnly: true
```

- 然后从prometheus的容器发现 :
  - 存储目录为 /data 对应的volumes使用的是emptyDir等于没有持久化的方案，这也可以理解，毕竟要轻量级的部署
  - 探测/-/healthy path 作为存活探针
  - 探测/-/ready path 作为就绪探针
- prometheus容器的yaml如下

```yaml
      - args:
        - --storage.tsdb.retention.time=15d
        - --config.file=/etc/config/prometheus.yml
        - --storage.tsdb.path=/data
        - --web.console.libraries=/etc/prometheus/console_libraries
        - --web.console.templates=/etc/prometheus/consoles
        - --web.enable-lifecycle
        image: prom/prometheus:v2.26.0
        imagePullPolicy: IfNotPresent
        livenessProbe:
          failureThreshold: 3
          httpGet:
            path: /-/healthy
            port: 9090
            scheme: HTTP
          initialDelaySeconds: 30
          periodSeconds: 15
          successThreshold: 1
          timeoutSeconds: 10
        name: prometheus-server
        ports:
        - containerPort: 9090
          protocol: TCP
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /-/ready
            port: 9090
            scheme: HTTP
          periodSeconds: 5
          successThreshold: 1
          timeoutSeconds: 4
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /etc/config
          name: config-volume
        - mountPath: /data
          name: storage-volume
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext:
        fsGroup: 65534
        runAsGroup: 65534
        runAsNonRoot: true
        runAsUser: 65534
      serviceAccount: prometheus
      serviceAccountName: prometheus
      terminationGracePeriodSeconds: 300
      volumes:
      - configMap:
          defaultMode: 420
          name: prometheus
        name: config-volume
      - emptyDir: {}
        name: storage-volume
```

### 02 prometheus采集任务分析

- 访问nodeIp+nodePort ，我这里是http://172.20.70.215:10003/targets![istio_prome01.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/908/1636786574000/a93b3320e29e4c69a99c105429939e67.png)
- 初步看target页面是 采集了apiserver、pod的业务指标、node上的cadvisor提供的容器基础资源指标
- 然后通过config 配置分析

#### 01 apiserver的指标

- 对应的配置  job_name: kubernetes-apiservers

```yaml
- job_name: kubernetes-apiservers
  honor_timestamps: true
  scrape_interval: 15s
  scrape_timeout: 10s
  metrics_path: /metrics
  scheme: https
  authorization:
    type: Bearer
    credentials_file: /var/run/secrets/kubernetes.io/serviceaccount/token
  tls_config:
    ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
    insecure_skip_verify: true
  follow_redirects: true
  relabel_configs:
  - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
    separator: ;
    regex: default;kubernetes;https
    replacement: $1
    action: keep
  kubernetes_sd_configs:
  - role: endpoints
    follow_redirects: true
```

- 可有看到就是通过k8s的endpoint sd请求default中的 kubernetes svc的endpoint

```shell
[root@k8s-master01 istio-1.11.4]# kubectl describe  svc kubernetes  
Name:              kubernetes
Namespace:         default
Labels:            component=apiserver
                   provider=kubernetes
Annotations:       <none>
Selector:          <none>
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                10.96.0.1
IPs:               10.96.0.1
Port:              https  443/TCP
TargetPort:        6443/TCP
Endpoints:         172.20.70.205:6443
Session Affinity:  None
Events:            <none>
```

#### 02 容器基础资源指标

- 对应的配置  job_name: kubernetes-nodes-cadvisor

```yaml
- job_name: kubernetes-nodes-cadvisor
  honor_timestamps: true
  scrape_interval: 15s
  scrape_timeout: 10s
  metrics_path: /metrics
  scheme: https
  authorization:
    type: Bearer
    credentials_file: /var/run/secrets/kubernetes.io/serviceaccount/token
  tls_config:
    ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
    insecure_skip_verify: true
  follow_redirects: true
  relabel_configs:
  - separator: ;
    regex: __meta_kubernetes_node_label_(.+)
    replacement: $1
    action: labelmap
  - separator: ;
    regex: (.*)
    target_label: __address__
    replacement: kubernetes.default.svc:443
    action: replace
  - source_labels: [__meta_kubernetes_node_name]
    separator: ;
    regex: (.+)
    target_label: __metrics_path__
    replacement: /api/v1/nodes/$1/proxy/metrics/cadvisor
    action: replace
  kubernetes_sd_configs:
  - role: node
    follow_redirects: true
```

- 这里要解读一下，首先使用 k8s的node sd发现所有的node

```shell
  kubernetes_sd_configs:
  - role: node
    follow_redirects: true
```

- 然后请求的host地址为  kubernetes.default.svc:443也就是，apiserver

```shell
  - separator: ;
    regex: (.*)
    target_label: __address__
    replacement: kubernetes.default.svc:443
    action: replace
```

- 然后构造请求的path ，将服务发现到的__meta_kubernetes_node_name标签的值作为$1拼接到 /api/v1/nodes/$1/proxy/metrics/cadvisor中

```shell
  - source_labels: [__meta_kubernetes_node_name]
    separator: ;
    regex: (.+)
    target_label: __metrics_path__
    replacement: /api/v1/nodes/$1/proxy/metrics/cadvisor
    action: replace
```

- 也就是最后请求的是apiserver 的 /api/v1/nodes/<node_name>/proxy/metrics/cadvisor这个path
- 和我们下面的 path类似

```shell
[root@k8s-master01 istio-1.11.4]# kubectl get --raw /api/v1/nodes/k8s-node01/proxy/metrics/cadvisor |head 
# HELP cadvisor_version_info A metric with a constant '1' value labeled by kernel version, OS version, docker version, cadvisor version & cadvisor revision.
# TYPE cadvisor_version_info gauge
cadvisor_version_info{cadvisorRevision="",cadvisorVersion="",dockerVersion="Unknown",kernelVersion="3.10.0-957.1.3.el7.x86_64",osVersion="CentOS Linux 7 (Core)"} 1
# HELP container_blkio_device_usage_total Blkio Device bytes usage
# TYPE container_blkio_device_usage_total counter
container_blkio_device_usage_total{container="",device="",id="/",image="",major="11",minor="0",name="",namespace="",operation="Async",pod=""} 0 1636546333267
container_blkio_device_usage_total{container="",device="",id="/",image="",major="11",minor="0",name="",namespace="",operation="Read",pod=""} 0 1636546333267
container_blkio_device_usage_total{container="",device="",id="/",image="",major="11",minor="0",name="",namespace="",operation="Sync",pod=""} 0 1636546333267
container_blkio_device_usage_total{container="",device="",id="/",image="",major="11",minor="0",name="",namespace="",operation="Total",pod=""} 0 1636546333267
container_blkio_device_usage_total{container="",device="",id="/",image="",major="11",minor="0",name="",namespace="",operation="Write",pod=""} 0 1636546333267
[root@k8s-master01 istio-1.11.4]# 
```

#### 03 pod内自暴露的指标

- 对应的配置  job_name: kubernetes-pods

```yaml
- job_name: kubernetes-pods
  honor_timestamps: true
  scrape_interval: 15s
  scrape_timeout: 10s
  metrics_path: /metrics
  scheme: http
  follow_redirects: true
  relabel_configs:
  - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
    separator: ;
    regex: "true"
    replacement: $1
    action: keep
  - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scheme]
    separator: ;
    regex: (https?)
    target_label: __scheme__
    replacement: $1
    action: replace
  - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
    separator: ;
    regex: (.+)
    target_label: __metrics_path__
    replacement: $1
    action: replace
  - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
    separator: ;
    regex: ([^:]+)(?::\d+)?;(\d+)
    target_label: __address__
    replacement: $1:$2
    action: replace
  - separator: ;
    regex: __meta_kubernetes_pod_label_(.+)
    replacement: $1
    action: labelmap
  - source_labels: [__meta_kubernetes_namespace]
    separator: ;
    regex: (.*)
    target_label: kubernetes_namespace
    replacement: $1
    action: replace
  - source_labels: [__meta_kubernetes_pod_name]
    separator: ;
    regex: (.*)
    target_label: kubernetes_pod_name
    replacement: $1
    action: replace
  - source_labels: [__meta_kubernetes_pod_phase]
    separator: ;
    regex: Pending|Succeeded|Failed
    replacement: $1
    action: drop
  kubernetes_sd_configs:
  - role: pod
    follow_redirects: true
```

> 应用： 采集pod自定义指标中replace 和 keep的应用

- 我们在使用pod自定义指标时在pod yaml 的spec.template.metadata.annotations中需要定义三个以`prometheus.io`开头的配置
- 分别代表是否需要prometheus采集、metrics暴露的端口、metrics的http path信息
- 我们之前写的go代码 ink8s_pod_metrics项目中的deployment配置如下

```yaml
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ink8s-pod-metrics
  template:
    metadata:
      labels:
        app: ink8s-pod-metrics
      annotations:
        prometheus.io/scrape: 'true'
        prometheus.io/port: '8080'
        prometheus.io/path: 'metrics'
```

- 在采集pod自定义指标时采用如下(job=kubernetes-pods)

```yaml
  relabel_configs:
  - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
    separator: ;
    regex: "true"
    replacement: $1
    action: keep
  - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
    separator: ;
    regex: (.+)
    target_label: __metrics_path__
    replacement: $1
    action: replace
  - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
    separator: ;
    regex: ([^:]+)(?::\d+)?;(\d+)
    target_label: __address__
    replacement: $1:$2
    action: replace
```

> 其中keep的应用

- 配置如下

```yaml
  - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
    separator: ;
    regex: "true"
    replacement: $1
    action: keep
```

- 含义是保留 __meta_kubernetes_pod_annotation_prometheus_io_scrape这个标签为true的pod
- 也就是pod的yaml中 spec.template.metadata.annotations 配置`prometheus.io/scrape: 'true'`的才需要采集
- 其余的pod不采集

# grafana 查看

- 同样我们也把grafana的 svc 类型改为NodePort

```shell
kubectl edit svc grafana -n istio-system
spec:
  clusterIP: 10.96.122.71
  clusterIPs:
  - 10.96.122.71
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - name: service
    port: 3000
    protocol: TCP
    targetPort: 3000
    nodePort: 10004
  selector:
    app.kubernetes.io/instance: grafana
    app.kubernetes.io/name: grafana
  sessionAffinity: None
  type: NodePort
status:
  loadBalancer: {}
~               
```

## istio grafana 相关技术说明

### 首先访问grafana 页面查看数据源

- 浏览器访问http://172.20.70.215:10004/datasources![istio_prome02.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/908/1636786574000/f96ca9be9237448ba784bc2410bbb0ca.png)
- 发现数据源就是 http://prometheus:9090 因为prometheus和 grafana在统一ns下，且有同名的 svc所以dns能直接解析出svc的ip

```shell
[root@k8s-master01 istio-1.11.4]# kubectl get svc -n istio-system
NAME                   TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                                                                    AGE
grafana                NodePort       10.96.122.71    <none>        3000:10004/TCP                                                             3h8m
istio-egressgateway    ClusterIP      10.96.156.251   <none>        80/TCP,443/TCP                                                             3h37m
istio-ingressgateway   LoadBalancer   10.96.129.161   <pending>     15021:32987/TCP,80:19257/TCP,443:7541/TCP,31400:30207/TCP,15443:6591/TCP   3h37m
istiod                 ClusterIP      10.96.188.40    <none>        15010/TCP,15012/TCP,443/TCP,15014/TCP                                      3h39m
jaeger-collector       ClusterIP      10.96.77.183    <none>        14268/TCP,14250/TCP,9411/TCP                                               3h8m
kiali                  NodePort       10.96.93.7      <none>        20001:10002/TCP,9090:20385/TCP                                             3h8m
prometheus             NodePort       10.96.164.205   <none>        9090:10003/TCP                                                             3h8m
tracing                ClusterIP      10.96.148.116   <none>        80/TCP,16685/TCP                                                           3h8m
zipkin                 ClusterIP      10.96.228.46    <none>        9411/TCP                                                                   3h8m
[root@k8s-master01 istio-1.11.4]# 
```

- 我们可以exec到grafana容器中模拟一下

```shell
[root@k8s-master01 istio-1.11.4]# kubectl get pod -n istio-system                                  
NAME                                    READY   STATUS    RESTARTS   AGE
grafana-68cc7d6d78-k6wr9                1/1     Running   0          3h9m
istio-egressgateway-756d4db566-c62rf    1/1     Running   0          3h38m
istio-ingressgateway-8577c57fb6-hz6d4   1/1     Running   0          3h38m
istiod-5847c59c69-w5qrd                 1/1     Running   0          3h40m
jaeger-5d44bc5c5d-c8rq6                 1/1     Running   0          3h9m
kiali-fd9f88575-6kxbl                   1/1     Running   0          3h9m
prometheus-77b49cb997-xq227             2/2     Running   0          3h9m
[root@k8s-master01 istio-1.11.4]# kubectl exec grafana-68cc7d6d78-k6wr9    -n istio-system -ti -- /bin/sh
/usr/share/grafana $ cat /etc/resolv.conf 
search istio-system.svc.cluster.local svc.cluster.local cluster.local
nameserver 10.96.0.10
options ndots:5
/usr/share/grafana $ ping prometheus
PING prometheus (10.96.164.205): 56 data bytes
ping: permission denied (are you root?)
/usr/share/grafana $ 
```

- 对应数据源是grafana pod挂载了configmap提供的

```shell
[root@k8s-master01 istio-1.11.4]# kubectl get cm grafana  -n istio-system -o yaml                 
apiVersion: v1
data:
  dashboardproviders.yaml: |
    apiVersion: 1
    providers:
    - disableDeletion: false
      folder: istio
      name: istio
      options:
        path: /var/lib/grafana/dashboards/istio
      orgId: 1
      type: file
    - disableDeletion: false
      folder: istio
      name: istio-services
      options:
        path: /var/lib/grafana/dashboards/istio-services
      orgId: 1
      type: file
  datasources.yaml: |
    apiVersion: 1
    datasources:
    - access: proxy
      editable: true
      isDefault: true
      jsonData:
        timeInterval: 5s
      name: Prometheus
      orgId: 1
      type: prometheus
      url: http://prometheus:9090
```

### 然后是grafana 提供的istio的dashboard

- 可以在grafana 的manage页面找到![istio_grafana01.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/908/1636786574000/58225737a8aa45a6b477a48ec02e77da.png)

#### 首先来看下 控制平面 istio-control-plane-dashboard

- istio-pilot的版本  ，我这里看到是1.11.4![istio_grafana02.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/908/1636786574000/12397e14d7a44421b9f6e5ffc377df76.png)
- 在prometheus中查看全量标签后得知，说明是这个pod istiod-5847c59c69-w5qrd 提供的指标

```shell

istio_build{app="istiod", component="pilot", install_operator_istio_io_owning_resource="unknown", instance="10.100.85.249:15014", istio="pilot", istio_io_rev="default", job="kubernetes-pods", kubernetes_namespace="istio-system", kubernetes_pod_name="istiod-5847c59c69-w5qrd", operator_istio_io_component="Pilot", pod_template_hash="5847c59c69", sidecar_istio_io_inject="false", tag="1.11.4"}

[root@k8s-master01 istio-1.11.4]# kubectl get pod -n istio-system

NAME                                    READY   STATUS    RESTARTS   AGE
istiod-5847c59c69-w5qrd                 1/1     Running   0          3h50m
```

- 对应的deployment 为istiod

```shell
[root@k8s-master01 istio-1.11.4]# kubectl get  deployment  -n istio-system                       
NAME                   READY   UP-TO-DATE   AVAILABLE   AGE
istiod                 1/1     1            1           3h56m
```

- 对应还有一些其他的指标![istio_grafana03.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/908/1636786574000/fec5c231dcb54a1e8dec81ff476c6bc8.png)

#### 其他dashboard

- istio mesh  workload 请求情况![istio_grafana04.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/908/1636786574000/be0ce37af71d4786af4c0126a282d85c.png)
- istio performance  组件性能情况![istio_grafana05.png](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/908/1636786574000/3e311c3639d74997932da35d603e4091.png)