
# 本节重点总结 :
- 各种资源的控制器对比期望数量和当前运行数量
    - 如果运行的不足就扩容
    - 如果运行的多了就缩容
- 获取当前运行数量通过syncHandler调用informer获取指定ns中的对象，再根据标签过滤
- 扩容采用渐进式扩容机制，从1开始2倍增加数量，指导扩容完毕
    - 目的是为了防止全量扩容时，同一个pod出现大量相同的错误，并且对服务组件压力较大
- 缩容有对应的筛选机制，对运行的pod进行排序：目的是尽量删除哪些最近创建的，状态不稳定的pod
    - Pending的
    - 同一个节点上多开的
    - 创建时间早的，ready时间短的
    - 重启次数多的
    


# 什么是replicaset
- 文档地址 https://kubernetes.io/zh/docs/concepts/workloads/controllers/replicaset/
- ReplicaSet 的目的是维护一组在任何时候都处于运行状态的 Pod 副本的稳定集合
- 因此，它通常用来保证给定数量的、完全相同的 Pod 的可用性

# ReplicaSet 的工作原理
- 它对比了定义声明的Pod数和当前集群中满足条件的Pod数，进行相对应的扩缩容

# replicaset控制器源码解读

- 控制器map入口 ，位置 D:\go_path\src\github.com\kubernetes\kubernetes\cmd\kube-controller-manager\app\controllermanager.go
```go
func NewControllerInitializers(loopMode ControllerLoopMode) map[string]InitFunc {
	controllers := map[string]InitFunc{}
    ...
    controllers["replicaset"] = startReplicaSetController
}
```

## startReplicaSetController的准备工作
- 位置 D:\go_path\src\github.com\kubernetes\kubernetes\cmd\kube-controller-manager\app\apps.go
- 使用Pods和 ReplicaSets两个informer初始化 rs控制器，因为需要同时观察这两种资源的数量
```go

func startReplicaSetController(ctx ControllerContext) (controller.Interface, bool, error) {
	go replicaset.NewReplicaSetController(
		ctx.InformerFactory.Apps().V1().ReplicaSets(),
		ctx.InformerFactory.Core().V1().Pods(),
		ctx.ClientBuilder.ClientOrDie("replicaset-controller"),
		replicaset.BurstReplicas,
	).Run(int(ctx.ComponentConfig.ReplicaSetController.ConcurrentRSSyncs), ctx.Stop)
	return nil, true, nil
}
```

### NewReplicaSetController  使用event_recorder初始化控制器
- 位置 D:\go_path\src\github.com\kubernetes\kubernetes\pkg\controller\replicaset\replica_set.go
```go
// NewReplicaSetController configures a replica set controller with the specified event recorder
func NewReplicaSetController(rsInformer appsinformers.ReplicaSetInformer, podInformer coreinformers.PodInformer, kubeClient clientset.Interface, burstReplicas int) *ReplicaSetController {
	eventBroadcaster := record.NewBroadcaster()
	eventBroadcaster.StartStructuredLogging(0)
	eventBroadcaster.StartRecordingToSink(&v1core.EventSinkImpl{Interface: kubeClient.CoreV1().Events("")})
	if err := metrics.Register(legacyregistry.Register); err != nil {
		klog.ErrorS(err, "unable to register metrics")
	}
	return NewBaseController(rsInformer, podInformer, kubeClient, burstReplicas,
		apps.SchemeGroupVersion.WithKind("ReplicaSet"),
		"replicaset_controller",
		"replicaset",
		controller.RealPodControl{
			KubeClient: kubeClient,
			Recorder:   eventBroadcaster.NewRecorder(scheme.Scheme, v1.EventSource{Component: "replicaset-controller"}),
		},
	)
}
```


### 底层调用NewBaseController
- 添加rsInformer的回调，对应rs对象新增、更新、删除
```go
	rsInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
		AddFunc:    rsc.addRS,
		UpdateFunc: rsc.updateRS,
		DeleteFunc: rsc.deleteRS,
	})
```
- 添加podInformer的回调，对应pod的新增、更新、删除
```go

	podInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
		AddFunc: rsc.addPod,
		// This invokes the ReplicaSet for every pod change, eg: host assignment. Though this might seem like
		// overkill the most frequent pod update is status, and the associated ReplicaSet will only list from
		// local storage, so it should be ok.
		UpdateFunc: rsc.updatePod,
		DeleteFunc: rsc.deletePod,
	})
```
- 举个rsc.addRS的例子，底层就是往队列中添加一条
```go
func (rsc *ReplicaSetController) enqueueRS(rs *apps.ReplicaSet) {
	key, err := controller.KeyFunc(rs)
	if err != nil {
		utilruntime.HandleError(fmt.Errorf("couldn't get key for object %#v: %v", rs, err))
		return
	}

	rsc.queue.Add(key)
}
```
- 设置syncHandler为syncReplicaSet
```go
rsc.syncHandler = rsc.syncReplicaSet
```

## startReplicaSetController的Run方法

### 并发同步的限制个数
- 位置D:\go_path\src\github.com\kubernetes\kubernetes\pkg\controller\replicaset\config\types.go
- 传入的ctx.ComponentConfig.ReplicaSetController.ConcurrentRSSync代表 并发同步的限制个数
- 数量越高说明需要的cpu和网络等资源越高
```go
// ReplicaSetControllerConfiguration contains elements describing ReplicaSetController.
type ReplicaSetControllerConfiguration struct {
	// concurrentRSSyncs is the number of replica sets that are  allowed to sync
	// concurrently. Larger number = more responsive replica  management, but more
	// CPU (and network) load.
	ConcurrentRSSyncs int32
}

```
- 对应的参数为 --concurrent-replicaset-syncs
- 从configz可以看出默认值是5
```shell script
        "ReplicaSetController": {
            "ConcurrentRSSyncs": 5
        },
```

### 通过rsc.podListerSynced判断informer数据已经同步过了
- 目的是在进行主流程钱，本地informer缓存中已经有数据了
```go
	if !cache.WaitForNamedCacheSync(rsc.Kind, stopCh, rsc.podListerSynced, rsc.rsListerSynced) {
		return
	}

```
- 对应的判断方法在NewBaseController中赋值
```go
rsc.podListerSynced = podInformer.Informer().HasSynced
```


### worker函数
- 启动等于并发限制个数的worker
```go
	for i := 0; i < workers; i++ {
		go wait.Until(rsc.worker, time.Second, stopCh)
	}
```
- 死循环处理
```go
// worker runs a worker thread that just dequeues items, processes them, and marks them done.
// It enforces that the syncHandler is never invoked concurrently with the same key.
func (rsc *ReplicaSetController) worker() {
	for rsc.processNextWorkItem() {
	}
}
```

- 处理函数processNextWorkItem
- 代码如下
```go
func (rsc *ReplicaSetController) processNextWorkItem() bool {
    
	key, quit := rsc.queue.Get()
	if quit {
		return false
	}
	defer rsc.queue.Done(key)

	err := rsc.syncHandler(key.(string))
	if err == nil {
		rsc.queue.Forget(key)
		return true
	}

	utilruntime.HandleError(fmt.Errorf("sync %q failed with %v", key, err))
	rsc.queue.AddRateLimited(key)

	return true
}
```
- 解读一下
    - 先从队列中获取一个元素 rsc.queue.Get
    - 然后调用syncHandler处理
        - 如果没出错，调用rsc.queue.Forget删除，意思是不再retry了
        - 否则，其他的worker还要拿到这个key进行重试
        
## syncHandler就是 syncReplicaSet
- 位置 D:\go_path\src\github.com\kubernetes\kubernetes\pkg\controller\replicaset\replica_set.go
- 首先从key中分隔出rs的namespace和 name
```go
	namespace, name, err := cache.SplitMetaNamespaceKey(key)
	if err != nil {
		return err
	}
```
- 通过rs informer获取rs对象
```go
	rs, err := rsc.rsLister.ReplicaSets(namespace).Get(name)
	if apierrors.IsNotFound(err) {
		klog.V(4).Infof("%v %v has been deleted", rsc.Kind, key)
		rsc.expectations.DeleteExpectations(key)
		return nil
	}
```
- 获取rs的selector，就是标签选择器
```go
	selector, err := metav1.LabelSelectorAsSelector(rs.Spec.Selector)
	if err != nil {
		utilruntime.HandleError(fmt.Errorf("error converting pod selector to selector: %v", err))
		return nil
	}
```

- 筛选出rs所在ns的所有pod
```go
	allPods, err := rsc.podLister.Pods(rs.Namespace).List(labels.Everything())
	if err != nil {
		return err
	}
```

- 去掉inactive的pod
```go
	// Ignore inactive pods.
	filteredPods := controller.FilterActivePods(allPods)

```

- 根据标签选择器过滤pod
```go
	// NOTE: filteredPods are pointing to objects from cache - if you need to
	// modify them, you need to copy it first.
	filteredPods, err = rsc.claimPods(rs, selector, filteredPods)
	if err != nil {
		return err
	}
```

- 管理的方法，下面详细讲
```go
	var manageReplicasErr error
	if rsNeedsSync && rs.DeletionTimestamp == nil {
		manageReplicasErr = rsc.manageReplicas(filteredPods, rs)
	}
```
- 计算更新后的状态
```go
	newStatus := calculateStatus(rs, filteredPods, manageReplicasErr)

```


### 核心函数 manageReplicas解析
- 根据函数注释可以知道，主要用来检查和更新rs，期间并不会修改filteredPods，如果出错就将rs重新入队做retry
```go
// manageReplicas checks and updates replicas for the given ReplicaSet.
// Does NOT modify <filteredPods>.
// It will requeue the replica set in case of an error while creating/deleting pods.
```
- 用当前运行的pod个数减去rs中配置的副本数得到diff
```go
diff := len(filteredPods) - int(*(rs.Spec.Replicas))
```
- 如果diff<0 说明当前运行的小于期望的，需要扩容
- 如果diff>0 说明当前运行的多于期望的，需要缩容

#### 扩容过程
##### 慢启动扩容函数 slowStartBatch
- 目的是为了防止大量创建pod出现相同的错误
```go
func slowStartBatch(count int, initialBatchSize int, fn func() error) (int, error) {
	remaining := count
	successes := 0
	for batchSize := integer.IntMin(remaining, initialBatchSize); batchSize > 0; batchSize = integer.IntMin(2*batchSize, remaining) {
		errCh := make(chan error, batchSize)
		var wg sync.WaitGroup
		wg.Add(batchSize)
		for i := 0; i < batchSize; i++ {
			go func() {
				defer wg.Done()
				if err := fn(); err != nil {
					errCh <- err
				}
			}()
		}
		wg.Wait()
		curSuccesses := batchSize - len(errCh)
		successes += curSuccesses
		if len(errCh) > 0 {
			return successes, <-errCh
		}
		remaining -= batchSize
	}
	return successes, nil
}
```
- 代码逻辑解读，假设我们要扩容10个pod，即count=10，那么过程如下
    - 第1轮，batchSize是remaining和initialBatchSize的最小值就是1，启动1个任务
    - 第2轮，batchSize是 2和剩余数量9的最小值为2，启动2个任务
    - 第3轮，batchSize是 4和剩余数量7的最小值为4，启动2个任务
    - 第4轮，batchSize是 8和剩余数量4的最小值为3，启动3个任务，任务结束
    - 整体的任务数为 1,2,4,3
    
    
    
    
##### 真正的create函数为CreatePods
- 底层调用client 的create pod，通过apiserver写入etcd，等待scheduler的调度
```go
			err := rsc.podControl.CreatePods(rs.Namespace, &rs.Spec.Template, rs, metav1.NewControllerRef(rs, rsc.GroupVersionKind))
			if err != nil {
				if apierrors.HasStatusCause(err, v1.NamespaceTerminatingCause) {
					// if the namespace is being terminated, we don't have to do
					// anything because any creation will fail
					return nil
				}
			}
```


#### 缩容过程
- 通过rs获取相关的pods
```go
relatedPods, err := rsc.getIndirectlyRelatedPods(rs)
```
- 根据diff获取要删除的pod
```go
podsToDelete := getPodsToDelete(filteredPods, relatedPods, diff)
```
##### 获取要删除pod的策略
- getPodsToDelete将运行的pod 排序，获取前diff个删除
```go
func getPodsToDelete(filteredPods, relatedPods []*v1.Pod, diff int) []*v1.Pod {
	// No need to sort pods if we are about to delete all of them.
	// diff will always be <= len(filteredPods), so not need to handle > case.
	if diff < len(filteredPods) {
		podsWithRanks := getPodsRankedByRelatedPodsOnSameNode(filteredPods, relatedPods)
		sort.Sort(podsWithRanks)
		reportSortingDeletionAgeRatioMetric(filteredPods, diff)
	}
	return filteredPods[:diff]
}
```
- 排序的依据在ActivePodsWithRanks的Less中，位置 D:\go_path\src\github.com\kubernetes\kubernetes\pkg\controller\controller_utils.go
> 策略1 未分配到node的小于分配到node的
```go
	if s.Pods[i].Spec.NodeName != s.Pods[j].Spec.NodeName && (len(s.Pods[i].Spec.NodeName) == 0 || len(s.Pods[j].Spec.NodeName) == 0) {
		return len(s.Pods[i].Spec.NodeName) == 0
	}
```

> 策略2 PodPending < PodUnknown < PodRunning
- 三种状态对应的数字为0,1,2
```go
	if podPhaseToOrdinal[s.Pods[i].Status.Phase] != podPhaseToOrdinal[s.Pods[j].Status.Phase] {
		return podPhaseToOrdinal[s.Pods[i].Status.Phase] < podPhaseToOrdinal[s.Pods[j].Status.Phase]
	}
```

> 策略3 Not ready < ready
```go
	if podutil.IsPodReady(s.Pods[i]) != podutil.IsPodReady(s.Pods[j]) {
		return !podutil.IsPodReady(s.Pods[i])
	}
```

> 策略4 pod删除代价小的排在前面
- 删除代价 pod-deletion-cost 是[1.21引入的功能](https://kubernetes.io/zh/docs/concepts/workloads/controllers/replicaset/#pod-deletion-cost)
- 代表一个int32数字，设置在Annotations中，数字越小删除的代价越小，删除代价小代表pod的利用率较低
- 同一应用的不同 Pods 可能其利用率是不同的。在对应用执行缩容操作时，可能 希望移除利用率较低的 Pods。为了避免频繁更新 Pods
```go
	if utilfeature.DefaultFeatureGate.Enabled(features.PodDeletionCost) {
		pi, _ := helper.GetDeletionCostFromPodAnnotations(s.Pods[i].Annotations)
		pj, _ := helper.GetDeletionCostFromPodAnnotations(s.Pods[j].Annotations)
		if pi != pj {
			return pi < pj
		}
	}
```

> 策略5 节点上多开的排前面 
```go
	if s.Rank[i] != s.Rank[j] {
		return s.Rank[i] > s.Rank[j]
	}
```


> 策略6 ready时间短的排前面
```go
	// 6. Been ready for empty time < less time < more time
	// If both pods are ready, the latest ready one is smaller
	if podutil.IsPodReady(s.Pods[i]) && podutil.IsPodReady(s.Pods[j]) {
		readyTime1 := podReadyTime(s.Pods[i])
		readyTime2 := podReadyTime(s.Pods[j])
		if !readyTime1.Equal(readyTime2) {
			if !utilfeature.DefaultFeatureGate.Enabled(features.LogarithmicScaleDown) {
				return afterOrZero(readyTime1, readyTime2)
			} else {
				if s.Now.IsZero() || readyTime1.IsZero() || readyTime2.IsZero() {
					return afterOrZero(readyTime1, readyTime2)
				}
				rankDiff := logarithmicRankDiff(*readyTime1, *readyTime2, s.Now)
				if rankDiff == 0 {
					return s.Pods[i].UID < s.Pods[j].UID
				}
				return rankDiff < 0
			}
		}
	}
```

> 策略7 重启次数多的排前面
```go
	// 7. Pods with containers with higher restart counts < lower restart counts
	if maxContainerRestarts(s.Pods[i]) != maxContainerRestarts(s.Pods[j]) {
		return maxContainerRestarts(s.Pods[i]) > maxContainerRestarts(s.Pods[j])
	}
```



> 策略8 创建时间短的排前面
```go
	// 8. Empty creation time pods < newer pods < older pods
	if !s.Pods[i].CreationTimestamp.Equal(&s.Pods[j].CreationTimestamp) {
		if !utilfeature.DefaultFeatureGate.Enabled(features.LogarithmicScaleDown) {
			return afterOrZero(&s.Pods[i].CreationTimestamp, &s.Pods[j].CreationTimestamp)
		} else {
			if s.Now.IsZero() || s.Pods[i].CreationTimestamp.IsZero() || s.Pods[j].CreationTimestamp.IsZero() {
				return afterOrZero(&s.Pods[i].CreationTimestamp, &s.Pods[j].CreationTimestamp)
			}
			rankDiff := logarithmicRankDiff(s.Pods[i].CreationTimestamp, s.Pods[j].CreationTimestamp, s.Now)
			if rankDiff == 0 {
				return s.Pods[i].UID < s.Pods[j].UID
			}
			return rankDiff < 0
		}
	}
```


##### 缩容动作
- 调用 DeletePod执行
```go
		errCh := make(chan error, diff)
		var wg sync.WaitGroup
		wg.Add(diff)
		for _, pod := range podsToDelete {
			go func(targetPod *v1.Pod) {
				defer wg.Done()
				if err := rsc.podControl.DeletePod(rs.Namespace, targetPod.Name, rs); err != nil {
					// Decrement the expected number of deletes because the informer won't observe this deletion
					podKey := controller.PodKey(targetPod)
					rsc.expectations.DeletionObserved(rsKey, podKey)
					if !apierrors.IsNotFound(err) {
						klog.V(2).Infof("Failed to delete %v, decremented expectations for %v %s/%s", podKey, rsc.Kind, rs.Namespace, rs.Name)
						errCh <- err
					}
				}
			}(pod)
		}
		wg.Wait()
```

# 本节重点总结 :
- 各种资源的控制器对比期望数量和当前运行数量
    - 如果运行的不足就扩容
    - 如果运行的多了就缩容
- 获取当前运行数量通过syncHandler调用informer获取指定ns中的对象，再根据标签过滤
- 扩容采用渐进式扩容机制，从1开始2倍增加数量，指导扩容完毕
    - 目的是为了防止全量扩容时，同一个pod出现大量相同的错误，并且对服务组件压力较大
- 缩容有对应的筛选机制，对运行的pod进行排序：目的是尽量删除哪些最近创建的，状态不稳定的pod
    - Pending的
    - 同一个节点上多开的
    - 创建时间早的，ready时间短的
    - 重启次数多的